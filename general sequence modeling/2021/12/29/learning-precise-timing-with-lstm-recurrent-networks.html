<!DOCTYPE html>
<html lang="zh-TW"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Learning Precise Timing with LSTM Recurrent Networks | ML Notes</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Learning Precise Timing with LSTM Recurrent Networks" />
<meta property="og:locale" content="zh_TW" />
<meta name="description" content="目標 在 LSTM 上加入 peephole connections 作者 Felix A. Gers, Nicol N. Schraudolph, Jürgen Schmidhuber 隸屬單位 IDSIA 期刊/會議名稱 JMLR, Volume 3 發表時間 2002 論文連結 https://www.jmlr.org/papers/v3/gers02a.html" />
<meta property="og:description" content="目標 在 LSTM 上加入 peephole connections 作者 Felix A. Gers, Nicol N. Schraudolph, Jürgen Schmidhuber 隸屬單位 IDSIA 期刊/會議名稱 JMLR, Volume 3 發表時間 2002 論文連結 https://www.jmlr.org/papers/v3/gers02a.html" />
<link rel="canonical" href="/general%20sequence%20modeling/2021/12/29/learning-precise-timing-with-lstm-recurrent-networks.html" />
<meta property="og:url" content="/general%20sequence%20modeling/2021/12/29/learning-precise-timing-with-lstm-recurrent-networks.html" />
<meta property="og:site_name" content="ML Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-12-29T16:28:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Learning Precise Timing with LSTM Recurrent Networks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-12-29T16:28:00+08:00","datePublished":"2021-12-29T16:28:00+08:00","description":"目標 在 LSTM 上加入 peephole connections 作者 Felix A. Gers, Nicol N. Schraudolph, Jürgen Schmidhuber 隸屬單位 IDSIA 期刊/會議名稱 JMLR, Volume 3 發表時間 2002 論文連結 https://www.jmlr.org/papers/v3/gers02a.html","headline":"Learning Precise Timing with LSTM Recurrent Networks","mainEntityOfPage":{"@type":"WebPage","@id":"/general%20sequence%20modeling/2021/12/29/learning-precise-timing-with-lstm-recurrent-networks.html"},"url":"/general%20sequence%20modeling/2021/12/29/learning-precise-timing-with-lstm-recurrent-networks.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <script src="/assets/js/main.js"></script><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="ML Notes" /><link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8"
        src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js"></script>




  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] #fff -->
  






  
      <!-- [function][get_value] #ff4e00 -->
  






  
      <!-- [function][get_value] uppercase -->
  

<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<!-- favicon. -->
<link rel='icon' type='image/png' sizes='16x16' href='/assets/images/favicon.png' />

<!-- Noto sans CJK source code: https://dotblogs.com.tw/shadow/2019/10/27/153832 -->
<style>
  @font-face {
    font-family: 'Noto Sans TC';
    font-style: normal;
    font-weight: 100;
    src: url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Thin.woff2) format('woff2'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Thin.woff) format('woff'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Thin.otf) format('opentype');
  }

  /*預設font-weight*/
  @font-face {
    font-family: 'Noto Sans TC';
    font-style: normal;
    font-weight: 400;
    src: url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Regular.woff2) format('woff2'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Regular.woff) format('woff'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Regular.otf) format('opentype');
  }

  @font-face {
    font-family: 'Noto Sans TC';
    font-style: normal;
    font-weight: 700;
    src: url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Bold.woff2) format('woff2'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Bold.woff) format('woff'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Bold.otf) format('opentype');
  }

  @font-face {
    font-family: 'Noto Sans TC';
    font-style: normal;
    font-weight: 900;
    src: url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Black.woff2) format('woff2'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Black.woff) format('woff'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Black.otf) format('opentype');
  }

  html,
  body {
    font-family: 'Noto Sans TC';
  }
</style></head>
<body>




  
      <!-- [function][get_value] uppercase -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] 0 -->
  










  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] true -->
  

<header class="site-header " role="banner">

  <div class="wrapper">
    <div class="site-header-inner"><span class="site-brand"><a class="site-brand-inner" rel="author" href="/">
  <img class="site-favicon" title="ML Notes" src="" onerror="this.style.display='none'">
  ML Notes
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>

          <div class="trigger"><a class="page-link" href="/about.html">關於</a><a class="page-link" href="/archives.html">時間表</a><a class="page-link" href="/categories.html">筆記類別</a><a class="page-link" href="/tags.html">標籤</a>




  
      <!-- [function][get_value]  -->
  

</div>
        </nav></div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>





  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value] post-header.html -->
  






  
      <!-- [function][get_value] post-header.html -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] 0 -->
  





<script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




  
      <!-- [function][get_value] off -->
  

<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>




  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value] [] -->
  

<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">Learning Precise Timing with LSTM Recurrent Networks</h1>
  <h2 class="post-subtitle"></h2>

  <p class="post-meta">
    <time class="dt-published" datetime="2021-12-29T16:28:00+08:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Dec 29, 2021
    </time>

    
    

















  
      <!-- [function][get_article_words] 14229 -->
  
















    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 1 hour 28 mins</span>
  </p><div class="post-tags"><a class="post-tag" href="/tags.html#RNN">#RNN</a><a class="post-tag" href="/tags.html#LSTM">#LSTM</a><a class="post-tag" href="/tags.html#model architecture">#model architecture</a><a class="post-tag" href="/tags.html#neural network">#neural network</a></div></header>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <!-- Setup mathjax auto rendering. -->
<!--
  Load MathJax v3.
  See
  https://docs.mathjax.org/en/latest/index.html
  and
  https://docs.mathjax.org/en/latest/web/configuration.html
  for more information.
-->
<script>
  MathJax = {
    loader: {
      load: [
        '[tex]/ams',       // Equivalent to `\usepackage{amsmath}`.
        '[tex]/cancel',    // Equivalent to `\usepackage{cancel}`.
        '[tex]/mathtools', // Equivalent to `\usepackage{mathtools}`.
        '[tex]/unicode',   // Equivalent to `\usepackage{unicode}`.
      ]
    },
    tex: {
      // Extensions to use.
      packages: { '[+]': ['ams', 'cancel', 'mathtools', 'unicode'] },
      // Start/end delimiter pairs for in-line math.
      inlineMath: [
        ['$', '$'],
        ['\\(', '\\)'],
      ],
      // Start/end delimiter pairs for display math.
      displayMath: [
        ['$$', '$$'],
        ['\\[', '\\]']
      ],
      // Use \$ to produce a literal dollar sign.
      processEscapes: true,
      // Process \begin{xxx}...\end{xxx} outside math mode.
      processEnvironments: true,
      // Process \ref{...} outside of math mode.
      processRefs: true,
      // Pattern for recognizing numbers.
      digits: /^(?:[0-9]+(?:\{,\}[0-9]{3})*(?:\.[0-9]*)?|\.[0-9]+)/,
      tags: 'ams',         // Or 'ams' or 'all'.
      useLabelIds: false,  // Use label name rather than tag for ids.
      maxMacros: 1000,     // Maximum number of macro substitutions per expression.
      maxBuffer: 5 * 1024, // Maximum size for the internal TeX string (5K).
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<!--
  Define common LaTeX commands.

  Each command must be wrapped with $ signs.
  We use "display: none;" to avoid redudant whitespaces.
 -->

<p style="display: none;">
  <!-- Fields. -->
  $\newcommand{\field}[1]{\mathbb{#1}}$
  <!-- Natural number set. -->
  $\providecommand{\N}{}$
  $\renewcommand{\N}{\field{N}}$
  <!-- Rational field. -->
  $\providecommand{\Q}{}$
  $\renewcommand{\Q}{\field{Q}}$
  <!-- Real field. -->
  $\providecommand{\R}{}$
  $\renewcommand{\R}{\field{R}}$
  <!-- Integer set. -->
  $\providecommand{\Z}{}$
  $\renewcommand{\Z}{\field{Z}}$
  <!-- Parenthese. -->
  $\providecommand{\pa}{}$
  $\renewcommand{\pa}[1]{\left\lparen #1 \right\rparen}$
  <!-- Bracket. -->
  $\providecommand{\br}{}$
  $\renewcommand{\br}[1]{\left\lbrack #1 \right\rbrack}$
  <!-- Set. -->
  $\providecommand{\set}{}$
  $\renewcommand{\set}[1]{\left\lbrace #1 \right\rbrace}$
  <!-- Absolute value. -->
  $\providecommand{\abs}{}$
  $\renewcommand{\abs}[1]{\left\lvert #1 \right\rvert}$
  <!-- Norm. -->
  $\providecommand{\norm}{}$
  $\renewcommand{\norm}[1]{\left\lVert #1 \right\rVert}$
  <!-- Floor. -->
  $\providecommand{\floor}{}$
  $\renewcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}$
  <!-- Ceiling. -->
  $\providecommand{\ceil}{}$
  $\renewcommand{\ceil}[1]{\left\lceil #1 \right\rceil}$
  <!-- Evaluate. -->
  $\providecommand{\eval}{}$
  $\renewcommand{\eval}[1]{\left. #1 \right\rvert}$
  <!-- Partial derivative. -->
  $\providecommand{\pd}{}$
  $\renewcommand{\pd}[2]{\dfrac{\partial #1}{\partial #2}}$
  <!-- Sign function. -->
  $\DeclareMathOperator{\sign}{sign}$
  <!-- Diagonal function. -->
  $\DeclareMathOperator{\diag}{diag}$
  <!-- Argmax. -->
  $\DeclareMathOperator*{\argmax}{argmax}$
  <!-- Argmin. -->
  $\DeclareMathOperator*{\argmin}{argmin}$
  <!-- Limit in display mode.  This should only be used in inline mode. -->
  $\providecommand{\Lim}{}$
  $\renewcommand{\Lim}{\lim\limits}$
  <!-- Product in display mode.  This should only be used in inline mode. -->
  $\providecommand{\Prod}{}$
  $\renewcommand{\Prod}{\prod\limits}$
  <!-- Sum in display mode.  This should only be used in inline mode. -->
  $\providecommand{\Sum}{}$
  $\renewcommand{\Sum}{\sum\limits}$

  <!-- NN related operations. -->
  <!-- Softmax. -->
  $\DeclareMathOperator{\softmax}{softmax}$
  <!-- Concatenate. -->
  $\DeclareMathOperator{\cat}{concatenate}$

  <!-- Algorithm related tools. -->
  <!-- Procedure statement. -->
  $\providecommand{\algoProc}{}$
  $\renewcommand{\algoProc}[1]{\textbf{procedure}\text{ #1}}$
  $\providecommand{\algoEndProc}{}$
  $\renewcommand{\algoEndProc}{\textbf{end procedure}}$
  <!-- If statement. -->
  $\providecommand{\algoIf}{}$
  $\renewcommand{\algoIf}[1]{\textbf{if } #1 \textbf{ do}}$
  $\providecommand{\algoEndIf}{}$
  $\renewcommand{\algoEndIf}{\textbf{end if}}$
  <!-- Assignment -->
  $\providecommand{\algoEq}{}$
  $\renewcommand{\algoEq}{\leftarrow}$
  <!-- For statement. -->
  $\providecommand{\algoFor}{}$
  $\renewcommand{\algoFor}[1]{\textbf{for } #1 \textbf{ do}}$
  $\providecommand{\algoEndFor}{}$
  $\renewcommand{\algoEndFor}{\textbf{end for}}$
  <!-- While statement. -->
  $\providecommand{\algoWhile}{}$
  $\renewcommand{\algoWhile}[1]{\textbf{while } #1 \textbf{ do}}$
  $\providecommand{\algoEndWhile}{}$
  $\renewcommand{\algoEndWhile}{\textbf{end while}}$
  <!-- Return statement. -->
  $\providecommand{\algoReturn}{}$
  $\renewcommand{\algoReturn}{\textbf{return }}$

  <!-- Some wierd symbols cannot be showed correctly. -->
  $\providecommand{\hash}{}$
  $\renewcommand{\hash}{\unicode{35}}$

</p>

<table>
  <tbody>
    <tr>
      <td>目標</td>
      <td>在 LSTM 上加入 peephole connections</td>
    </tr>
    <tr>
      <td>作者</td>
      <td>Felix A. Gers, Nicol N. Schraudolph, Jürgen Schmidhuber</td>
    </tr>
    <tr>
      <td>隸屬單位</td>
      <td>IDSIA</td>
    </tr>
    <tr>
      <td>期刊/會議名稱</td>
      <td>JMLR, Volume 3</td>
    </tr>
    <tr>
      <td>發表時間</td>
      <td>2002</td>
    </tr>
    <tr>
      <td>論文連結</td>
      <td><a href="https://www.jmlr.org/papers/v3/gers02a.html">https://www.jmlr.org/papers/v3/gers02a.html</a></td>
    </tr>
  </tbody>
</table>

<!--
  Define LaTeX command which will be used through out the writing.

  Each command must be wrapped with $ signs.
  We use "display: none;" to avoid redudant whitespaces.
 -->

<p style="display: none;">

  <!-- Operator in. -->
  $\providecommand{\opnet}{}$
  $\renewcommand{\opnet}{\operatorname{net}}$
  <!-- Operator in. -->
  $\providecommand{\opin}{}$
  $\renewcommand{\opin}{\operatorname{in}}$
  <!-- Operator out. -->
  $\providecommand{\opout}{}$
  $\renewcommand{\opout}{\operatorname{out}}$
  <!-- Operator cell block. -->
  $\providecommand{\opblk}{}$
  $\renewcommand{\opblk}{\operatorname{block}}$
  <!-- Operator cell multiplicative forget gate. -->
  $\providecommand{\opfg}{}$
  $\renewcommand{\opfg}{\operatorname{fg}}$
  <!-- Operator cell multiplicative input gate. -->
  $\providecommand{\opig}{}$
  $\renewcommand{\opig}{\operatorname{ig}}$
  <!-- Operator cell multiplicative output gate. -->
  $\providecommand{\opog}{}$
  $\renewcommand{\opog}{\operatorname{og}}$
  <!-- Operator sequence. -->
  $\providecommand{\opseq}{}$
  $\renewcommand{\opseq}{\operatorname{seq}}$
  <!-- Operator loss. -->
  $\providecommand{\oploss}{}$
  $\renewcommand{\oploss}{\operatorname{loss}}$
  <!-- Operator tri. -->
  $\providecommand{\optri}{}$
  $\renewcommand{\optri}{\operatorname{tri}}$
  <!-- Operator rect. -->
  $\providecommand{\oprect}{}$
  $\renewcommand{\oprect}{\operatorname{rect}}$
  <!-- Operator mod. -->
  $\providecommand{\opmod}{}$
  $\renewcommand{\opmod}{\operatorname{mod}}$

  <!-- Net input. -->
  $\providecommand{\net}{}$
  $\renewcommand{\net}[2]{\opnet_{#1}(#2)}$
  <!-- Net input with activatiton f. -->
  $\providecommand{\fnet}{}$
  $\renewcommand{\fnet}[2]{f_{#1}\big(\net{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input. -->
  $\providecommand{\dfnet}{}$
  $\renewcommand{\dfnet}[2]{f_{#1}'\big(\net{#1}{#2}\big)}$

  <!-- Input dimension. -->
  $\providecommand{\din}{}$
  $\renewcommand{\din}{d_{\opin}}$
  <!-- Output dimension. -->
  $\providecommand{\dout}{}$
  $\renewcommand{\dout}{d_{\opout}}$
  <!-- Cell block dimension. -->
  $\providecommand{\dblk}{}$
  $\renewcommand{\dblk}{d_{\opblk}}$

  <!-- Number of cell blocks. -->
  $\providecommand{\nblk}{}$
  $\renewcommand{\nblk}{n_{\opblk}}$

  <!-- Cell block k. -->
  $\providecommand{\blk}{}$
  $\renewcommand{\blk}[1]{\opblk^{#1}}$

  <!-- Weight of multiplicative forget gate. -->
  $\providecommand{\wfg}{}$
  $\renewcommand{\wfg}{w^{\opfg}}$
  $\providecommand{\ufg}{}$
  $\renewcommand{\ufg}{u^{\opfg}}$
  <!-- Weight of multiplicative input gate. -->
  $\providecommand{\wig}{}$
  $\renewcommand{\wig}{w^{\opig}}$
  $\providecommand{\uig}{}$
  $\renewcommand{\uig}{u^{\opig}}$
  <!-- Weight of multiplicative output gate. -->
  $\providecommand{\wog}{}$
  $\renewcommand{\wog}{w^{\opog}}$
  $\providecommand{\uog}{}$
  $\renewcommand{\uog}{u^{\opog}}$
  <!-- Weight of cell units. -->
  $\providecommand{\wblk}{}$
  $\renewcommand{\wblk}[1]{w^{\blk{#1}}}$
  <!-- Weight of output units. -->
  $\providecommand{\wout}{}$
  $\renewcommand{\wout}{w^{\opout}}$

  <!-- Net input of multiplicative forget gate. -->
  $\providecommand{\netfg}{}$
  $\renewcommand{\netfg}[2]{\opnet_{#1}^{\opfg}(#2)}$
  <!-- Net input of multiplicative forget gate with activatiton f. -->
  $\providecommand{\fnetfg}{}$
  $\renewcommand{\fnetfg}[2]{f_{#1}^{\opfg}\big(\netfg{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input of forget gate. -->
  $\providecommand{\dfnetfg}{}$
  $\renewcommand{\dfnetfg}[2]{f_{#1}^{\opfg}{'}\big(\netfg{#1}{#2}\big)}$
  <!-- Net input of multiplicative input gate. -->
  $\providecommand{\netig}{}$
  $\renewcommand{\netig}[2]{\opnet_{#1}^{\opig}(#2)}$
  <!-- Net input of multiplicative input gate with activatiton f. -->
  $\providecommand{\fnetig}{}$
  $\renewcommand{\fnetig}[2]{f_{#1}^{\opig}\big(\netig{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input of input gate. -->
  $\providecommand{\dfnetig}{}$
  $\renewcommand{\dfnetig}[2]{f_{#1}^{\opig}{'}\big(\netig{#1}{#2}\big)}$
  <!-- Net input of multiplicative output gate. -->
  $\providecommand{\netog}{}$
  $\renewcommand{\netog}[2]{\opnet_{#1}^{\opog}(#2)}$
  <!-- Net input of multiplicative output gate with activatiton f. -->
  $\providecommand{\fnetog}{}$
  $\renewcommand{\fnetog}[2]{f_{#1}^{\opog}\big(\netog{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input of output gate. -->
  $\providecommand{\dfnetog}{}$
  $\renewcommand{\dfnetog}[2]{f_{#1}^{\opog}{'}\big(\netog{#1}{#2}\big)}$
  <!-- Net input of output units. -->
  $\providecommand{\netout}{}$
  $\renewcommand{\netout}[2]{\opnet_{#1}^{\opout}(#2)}$
  <!-- Net input of output units with activatiton f. -->
  $\providecommand{\fnetout}{}$
  $\renewcommand{\fnetout}[2]{f_{#1}^{\opout}\big(\netout{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input of output units. -->
  $\providecommand{\dfnetout}{}$
  $\renewcommand{\dfnetout}[2]{f_{#1}^{\opout}{'}\big(\netout{#1}{#2}\big)}$

  <!-- Net input of cell unit. -->
  $\providecommand{\netblk}{}$
  $\renewcommand{\netblk}[3]{\opnet_{#1}^{\blk{#2}}(#3)}$
  <!-- Net input of cell unit with activatiton g. -->
  $\providecommand{\gnetblk}{}$
  $\renewcommand{\gnetblk}[3]{g_{#1}\big(\netblk{#1}{#2}{#3}\big)}$
  <!-- Derivative of g with respect to net input of cell unit. -->
  $\providecommand{\dgnetblk}{}$
  $\renewcommand{\dgnetblk}[3]{g_{#1}'\big(\netblk{#1}{#2}{#3}\big)}$
  <!-- Cell unit with activatiton h. -->
  $\providecommand{\hblk}{}$
  $\renewcommand{\hblk}[3]{h_{#1}\big(s_{#1}^{\blk{#2}}(#3)\big)}$

  <!-- Gradient approximation by truncating gradient. -->
  $\providecommand{\aptr}{}$
  $\renewcommand{\aptr}{\approx_{\operatorname{tr}}}$
</p>

<!-- End LaTeX command define section. -->

<h2 id="section">重點</h2>

<ul>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 與 <a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 都沒有 peephole connections
    <ul>
      <li>論文提議的 peephole connections 是只連接到相同的記憶單元</li>
      <li>現今常用的 LSTM 使用 peephole connections 的方法是全連接，例如 <a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html">PyTorch 實作的 LSTM</a> 就是一個例子</li>
      <li><a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 細節可以看<a href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html">我的筆記</a></li>
      <li><a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 細節可以看<a href="/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html">我的筆記</a></li>
    </ul>
  </li>
  <li>這篇論文終於把過去兩篇論文寫錯的數學式改對了</li>
  <li>作者認為在不給予 LSTM 模型任何的輸入時， LSTM 必須要能夠觀察記憶單元內部狀態的變化才能模擬週期函數
    <ul>
      <li>例如音樂節奏辨識</li>
      <li>LSTM + peephole connections 在實驗中能夠成功解決模擬週期函數的任務</li>
    </ul>
  </li>
  <li>從<a href="#paper-fig-15">圖 15</a> 可以發現模型的初始計算狀態為 $0$，但開始計算後模型計算狀態再也不為 $0$
    <ul>
      <li>這表示模型<strong>初始計算狀態</strong>應該也被當成<strong>參數</strong>一起訓練</li>
    </ul>
  </li>
  <li>作者認為 RNN 模型在記憶上仍然有問題，即使使用 LSTM 記憶的容量仍然被記憶單元的個數限制，並且無法只靠簡單增加記憶單元個數解決
    <ul>
      <li>與現今的 transformers 想法不同，大家都在搞大型 pre-trained model</li>
      <li>作者認為有效解決記憶容量問題的模型架構仍然未被發現</li>
    </ul>
  </li>
  <li>LSTM 所採用的 truncated BPTT 最佳化演算法會導致模型沒辦法有效的學習遞迴的資訊
    <ul>
      <li>根據作者實驗，當序列資料有大量雜訊時不做特殊的前處理就無法進行訓練</li>
    </ul>
  </li>
</ul>

<h2 id="lstm-">原始 LSTM 架構</h2>

<h3 id="section-1">模型架構</h3>

<p>根據 <a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 提出的架構如下（這篇論文不使用額外的<strong>隱藏單元</strong>，因此我們也完全不列出隱藏單元相關的公式）（細節可以參考<a href="/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html">我的筆記</a>）</p>

<table>
  <thead>
    <tr>
      <th>符號</th>
      <th>意義</th>
      <th>備註</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\din$</td>
      <td><strong>輸入層</strong>的維度</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
    <tr>
      <td>$\dblk$</td>
      <td><strong>記憶單元</strong>的維度</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
    <tr>
      <td>$\nblk$</td>
      <td><strong>記憶單元</strong>的個數</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
    <tr>
      <td>$\dout$</td>
      <td><strong>輸出層</strong>的維度</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
    <tr>
      <td>$T$</td>
      <td>輸入序列的長度</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
  </tbody>
</table>

<p>以下所有符號的時間 $t$ 範圍為 $t \in \set{1, \dots, T}$</p>

<table>
  <thead>
    <tr>
      <th>符號</th>
      <th>意義</th>
      <th>維度</th>
      <th>備註</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$x(t)$</td>
      <td>第 $t$ 個時間點的<strong>輸入</strong></td>
      <td>$\din$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$y^{\opfg}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點的<strong>遺忘閘門</strong></td>
      <td>$\nblk$</td>
      <td>$y^{\opfg}(0) = 0$，同一個記憶單元<strong>共享遺忘閘門</strong></td>
    </tr>
    <tr>
      <td>$y^{\opig}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點的<strong>輸入閘門</strong></td>
      <td>$\nblk$</td>
      <td>$y^{\opig}(0) = 0$，同一個記憶單元<strong>共享輸入閘門</strong></td>
    </tr>
    <tr>
      <td>$y^{\opog}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點的<strong>輸出閘門</strong></td>
      <td>$\nblk$</td>
      <td>$y^{\opog}(0) = 0$，同一個記憶單元<strong>共享輸出閘門</strong></td>
    </tr>
    <tr>
      <td>$s^{\blk{k}}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點的第 $k$ 個<strong>記憶單元區塊內部狀態</strong></td>
      <td>$\dblk$</td>
      <td>$s^{\blk{k}}(0) = 0$ 且 $k \in \set{1, \dots, \nblk}$</td>
    </tr>
    <tr>
      <td>$y^{\blk{k}}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點的第 $k$ 個<strong>記憶單元區塊輸出</strong></td>
      <td>$\dblk$</td>
      <td>$y^{\blk{k}}(0) = 0$ 且 $k \in \set{1, \dots, \nblk}$</td>
    </tr>
    <tr>
      <td>$y(t)$</td>
      <td>第 $t$ 個時間點的<strong>輸出</strong></td>
      <td>$\dout$</td>
      <td>由 $t$ 時間點的<strong>輸入</strong>與<strong>記憶單元輸出</strong>透過<strong>全連接</strong>產生，因此沒有 $y(0)$</td>
    </tr>
    <tr>
      <td>$\hat{y}(t)$</td>
      <td>第 $t$ 個時間點的<strong>預測目標</strong></td>
      <td>$\dout$</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>符號</th>
      <th>意義</th>
      <th>下標範圍</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$x_j(t)$</td>
      <td>第 $t$ 個時間點的第 $j$ 個<strong>輸入</strong></td>
      <td>$j \in \set{1, \dots, \din}$</td>
    </tr>
    <tr>
      <td>$y_k^{\opfg}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點第 $k$ 個記憶單元區塊的<strong>遺忘閘門</strong></td>
      <td>$k \in \set{1, \dots, \nblk}$</td>
    </tr>
    <tr>
      <td>$y_k^{\opig}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點第 $k$ 個記憶單元區塊的<strong>輸入閘門</strong></td>
      <td>$k \in \set{1, \dots, \nblk}$</td>
    </tr>
    <tr>
      <td>$y_k^{\opog}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點第 $k$ 個記憶單元區塊的<strong>輸出閘門</strong></td>
      <td>$k \in \set{1, \dots, \nblk}$</td>
    </tr>
    <tr>
      <td>$s_i^{\blk{k}}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點的第 $k$ 個<strong>記憶單元區塊</strong>的第 $i$ 個<strong>記憶單元內部狀態</strong></td>
      <td>$i \in \set{1, \dots, \dblk}$</td>
    </tr>
    <tr>
      <td>$y_i^{\blk{k}}(t - 1)$</td>
      <td>第 $t - 1$ 個時間點的第 $k$ 個<strong>記憶單元區塊</strong>的第 $i$ 個<strong>記憶單元輸出</strong></td>
      <td>$i \in \set{1, \dots, \dblk}$</td>
    </tr>
    <tr>
      <td>$y_i(t)$</td>
      <td>第 $t$ 個時間點的第 $i$ 個<strong>輸出</strong></td>
      <td>$i \in \set{1, \dots, \dout}$</td>
    </tr>
    <tr>
      <td>$\hat{y}_i(t)$</td>
      <td>第 $t$ 個時間點的第 $i$ 個<strong>預測目標</strong></td>
      <td>$i \in \set{1, \dots, \dout}$</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>參數</th>
      <th>意義</th>
      <th>輸出維度</th>
      <th>輸入維度</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\wfg$</td>
      <td>產生<strong>遺忘閘門</strong>的全連接參數</td>
      <td>$\nblk$</td>
      <td>$\din + \nblk \cdot (3 + \dblk)$</td>
    </tr>
    <tr>
      <td>$\wig$</td>
      <td>產生<strong>輸入閘門</strong>的全連接參數</td>
      <td>$\nblk$</td>
      <td>$\din + \nblk \cdot (3 + \dblk)$</td>
    </tr>
    <tr>
      <td>$\wog$</td>
      <td>產生<strong>輸出閘門</strong>的全連接參數</td>
      <td>$\nblk$</td>
      <td>$\din + \nblk \cdot (3 + \dblk)$</td>
    </tr>
    <tr>
      <td>$\wblk{k}$</td>
      <td>產生第 $k$ 個<strong>記憶單元淨輸入</strong>的全連接參數</td>
      <td>$\dblk$</td>
      <td>$\din + \nblk \cdot (3 + \dblk)$</td>
    </tr>
    <tr>
      <td>$\wout$</td>
      <td>產生<strong>輸出</strong>的全連接參數</td>
      <td>$\dblk$</td>
      <td>$\din + \nblk \cdot \dblk$</td>
    </tr>
  </tbody>
</table>

<p>定義 $\sigma$ 為 sigmoid 函數 $\sigma(x) = \frac{1}{1 + e^{-x}}$</p>

<table>
  <thead>
    <tr>
      <th>函數</th>
      <th>意義</th>
      <th>公式</th>
      <th>range</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$f_k^{\opfg}$</td>
      <td>第 $k$ 個<strong>遺忘閘門</strong>的啟發函數</td>
      <td>$\sigma$</td>
      <td>$[0, 1]$</td>
    </tr>
    <tr>
      <td>$f_k^{\opig}$</td>
      <td>第 $k$ 個<strong>輸入閘門</strong>的啟發函數</td>
      <td>$\sigma$</td>
      <td>$[0, 1]$</td>
    </tr>
    <tr>
      <td>$f_k^{\opog}$</td>
      <td>第 $k$ 個<strong>輸出閘門</strong>的啟發函數</td>
      <td>$\sigma$</td>
      <td>$[0, 1]$</td>
    </tr>
    <tr>
      <td>$g_i^{\blk{k}}$</td>
      <td>第 $k$ 個<strong>記憶單元</strong>第 $i$ 個<strong>內部狀態</strong>的啟發函數</td>
      <td>$4\sigma - 2$</td>
      <td>$[-2, 2]$</td>
    </tr>
    <tr>
      <td>$h_i^{\blk{k}}$</td>
      <td>第 $k$ 個<strong>記憶單元</strong>第 $i$ 個<strong>輸出</strong>的啟發函數</td>
      <td>$2\sigma - 1$</td>
      <td>$[-1, 1]$</td>
    </tr>
    <tr>
      <td>$f_i^{\opout}$</td>
      <td>第 $i$ 個<strong>輸出</strong>的啟發函數</td>
      <td>$\sigma$</td>
      <td>$[0, 1]$</td>
    </tr>
  </tbody>
</table>

<p>在 $t$ 時間點時得到<strong>輸入</strong> $x(t)$，產生 $t$ 時間點<strong>遺忘閘門</strong> $y^{\opfg}(t)$、<strong>輸入閘門</strong> $y^{\opig}(t)$ 與<strong>輸出閘門</strong> $y^{\opog}(t)$ 的方法如下</p>

\[\begin{align*}
g &amp; \in \set{\opfg, \opig, \opog} \\
\opnet^g(t) &amp; = w^g \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix} \\
y^g(t) &amp; = f^g(\opnet^g(t))
\end{align*} \tag{1}\label{1}\]

<ul>
  <li>注意與<a href="/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html">以前的筆記</a>不同，這裡是產生 $t$ 時間點的資訊而不是 $t + 1$</li>
  <li>注意是以 $t$ 時間點的輸入（不是 $t - 1$）與 $t - 1$ 時間點的計算狀態產生 $t$ 時間點的計算狀態</li>
</ul>

<p>利用 $\eqref{1}$ 產生 $t$ 時間點的<strong>記憶單元內部狀態</strong> $s^{\blk{k}}(t)$ 方法如下</p>

\[\begin{align*}
k &amp; \in \set{1, \dots, \nblk} \\
\opnet^{\blk{k}}(t) &amp; = \wblk{k} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix} \\
s^{\blk{k}}(t) &amp; = y_k^{\opfg}(t) \cdot s^{\blk{k}}(t - 1) + y_k^{\opig}(t) \cdot g^{\blk{k}}(\opnet^{\blk{k}}(t))
\end{align*} \tag{2}\label{2}\]

<p>注意第 $k$ 個記憶單元內部狀態<strong>共享遺忘閘門</strong> $y_k^{\opfg}(t)$ 與<strong>輸入閘門</strong> $y_k^{\opig}(t)$。</p>

<p>利用 $\eqref{1}\eqref{2}$ 產生 $t$ 時間點的<strong>記憶單元輸出</strong> $y^{\blk{k}}(t)$ 方法如下</p>

\[\begin{align*}
k &amp; \in \set{1, \dots, \nblk} \\
y^{\blk{k}}(t) &amp; = y_k^{\opog}(t) \cdot h^{\blk{k}}(s^{\blk{k}}(t))
\end{align*} \tag{3}\label{3}\]

<p>注意第 $k$ 個記憶單元輸出<strong>共享輸出閘門</strong> $y_k^{\opog}(t)$。
由於實驗結果作者認為 $h^{\blk{k}}$ 不是很重要，因此 $\eqref{3}$ 中的式子改為</p>

\[y^{\blk{k}}(t) = y_k^{\opog}(t) \cdot s^{\blk{k}}(t) \quad k = 1, \dots, \nblk \tag{4}\label{4}\]

<p>產生 $t$ 時間點的<strong>輸出</strong>是透過 $t$ 時間點的<strong>輸入</strong>與<strong>記憶單元輸出</strong>（見 $\eqref{4}$）而得（注意是 $t$ 時間點不是 $t - 1$，代表<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 與 <a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 都寫錯了）</p>

\[\begin{align*}
\opnet^{\opout}(t) &amp; = \wout \cdot \begin{pmatrix}
x(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix} \\
y(t) &amp; = f^{\opout}(\opnet^{\opout}(t))
\end{align*} \tag{5}\label{5}\]

<h3 id="section-2">最佳化</h3>

<p><a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM</a> 提出與 truncated BPTT 相似的概念，透過 RTRL 進行參數更新，並故意<strong>丟棄流出記憶單元的所有梯度</strong>，避免梯度爆炸或梯度消失的問題，同時節省更新所需的空間與時間（local in time and space）。（細節可見<a href="/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html">我的筆記</a>）</p>

<p>令 $t = 1, \dots, T$，最佳化的目標為每個時間點 $t$ 所產生的<strong>平方誤差總和最小化</strong></p>

\[\begin{align*}
\oploss(t) &amp; = \sum_{i = 1}^{\dout} \oploss_i(t) \\
&amp; = \sum_{i = 1}^{\dout} \frac{1}{2} \big(y_i(t) - \hat{y}_i(t)\big)^2
\end{align*} \tag{6}\label{6}\]

<p>以下我們使用 $\aptr$ 代表<strong>丟棄部份梯度後的剩餘梯度</strong>。</p>

<p>注意：論文中的式子 7 與 8 互相矛盾，式子 8 應改為 $\triangle w_{k m}(t) = \alpha \delta_k(t) y_m(t)$</p>

<h4 id="section-3">輸出參數的剩餘梯度</h4>

\[\begin{align*}
\pd{\oploss(t)}{\wout_{i, j}} &amp; = \pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pd{\netout{i}{t}}{\wout_{i, j}} \\
&amp; = \big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_j
\end{align*} \tag{7}\label{7}\]

<p>其中 $1 \leq i \leq \dout$ 且 $1 \leq j \leq \din + \nblk \cdot \dblk$。</p>

<h4 id="section-4">輸出閘門參數的剩餘梯度</h4>

\[\begin{align*}
\pd{\oploss(t)}{\wog_{k, q}} &amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \\
&amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{y_k^{\opog}(t)}} \cdot \pd{y_k^{\opog}(t)}{\netog{k}{t}} \cdot \pd{\netog{k}{t}}{\wog_{k, q}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \\
&amp; \quad \pa{\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot s_j^{\blk{k}}(t)} \cdot \dfnetog{k}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q\Bigg]
\end{align*} \tag{8}\label{8}\]

<p>其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。</p>

<h4 id="section-5">輸入閘門參數的剩餘梯度</h4>

\[\begin{align*}
&amp; \pd{\oploss(t)}{\wig_{k, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \pd{s_j^{\blk{k}}(t)}{\wig_{k, q}}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \\
&amp; \quad \quad \br{y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\wig_{k, q}} + \pd{s_j^{\blk{k}}(t)}{y_k^{\opig}(t)} \cdot \pd{y_k^{\opig}(t)}{\netig{k}{t}} \cdot \pd{\netig{k}{t}}{\wig_{k, q}}}\Bigg)\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp; \quad \quad \br{y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\wig_{k, q}} + \gnetblk{j}{k}{t} \cdot \dfnetig{k}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q}\Bigg)\Bigg]
\end{align*} \tag{9}\label{9}\]

<p>其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。</p>

<h4 id="section-6">遺忘閘門參數的剩餘梯度</h4>

\[\begin{align*}
&amp; \pd{\oploss(t)}{\wfg_{k, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \pd{s_j^{\blk{k}}(t)}{\wfg_{k, q}}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \\
&amp; \quad \quad \br{\pd{y_k^{\opfg}(t)}{\netfg{k}{t}} \cdot \pd{\netfg{k}{t}}{\wfg_{k, q}} \cdot s_j^{\blk{k}}(t - 1) + y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\wfg_{k, q}}}\Bigg)\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp; \quad \quad \br{\dfnetfg{k}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q \cdot s_j^{\blk{k}}(t - 1) + y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\wfg_{k, q}}}\Bigg)\Bigg]
\end{align*} \tag{10}\label{10}\]

<p>其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。</p>

<h4 id="section-7">記憶單元淨輸入參數的剩餘梯度</h4>

\[\begin{align*}
&amp; \pd{\oploss(t)}{\wblk{k}_{p, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \br{\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pd{\netout{i}{t}}{y_p^{\blk{k}}(t)} \cdot \pd{y_p^{\blk{k}}(t)}{s_p^{\blk{k}}(t)} \cdot \pd{s_p^{\blk{k}}(t)}{\wblk{k}_{p, q}}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pd{\netout{i}{t}}{y_p^{\blk{k}}(t)} \cdot \pd{y_p^{\blk{k}}(t)}{s_p^{\blk{k}}(t)} \cdot \\
&amp; \quad \quad \pa{f_k^{\opfg}(t) \cdot \pd{s_p^{\blk{k}}(t - 1)}{\wblk{k}_{p, q}} + \pd{s_p^{\blk{k}}(t)}{\gnetblk{j}{k}{t}} \cdot \pd{\gnetblk{j}{k}{t}}{\netblk{j}{k}{t}} \cdot \pd{\netblk{j}{k}{t}}{\wblk{k}_{p, q}}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp; \quad \br{f_k^{\opfg}(t) \cdot \pd{s_p^{\blk{k}}(t - 1)}{\wblk{k}_{p, q}} + y_k^{\opig}(t) \cdot \dgnetblk{p}{k}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q}\Bigg]
\end{align*} \tag{11}\label{11}\]

<p>其中 $1 \leq k \leq \nblk$， $1 \leq p \leq \dblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。</p>

<h4 id="section-8">梯度下降</h4>

<p>計算完上述所有參數後使用<strong>梯度下降</strong>（gradient descent）進行參數更新</p>

\[\begin{align*}
\wout_{i, j} &amp; \leftarrow \wout_{i, j} - \alpha \cdot \pd{\oploss(t)}{\wout_{i, j}} \\
\wog_{k, q} &amp; \leftarrow \wog_{k, q} - \alpha \cdot \pd{\oploss(t)}{\wog_{k, q}} \\
\wig_{k, q} &amp; \leftarrow \wig_{k, q} - \alpha \cdot \pd{\oploss(t)}{\wig_{k, q}} \\
\wfg_{k, q} &amp; \leftarrow \wig_{k, q} - \alpha \cdot \pd{\oploss(t)}{\wfg_{k, q}} \\
\wblk{k}_{p, q} &amp; \leftarrow \wblk{k}_{p, q} - \alpha \cdot \pd{\oploss(t)}{\wblk{k}_{p, q}}
\end{align*} \tag{12}\label{12}\]

<p>其中 $\alpha$ 為<strong>學習率</strong>（<strong>learning rate</strong>）。</p>

<p>由於使用基於 RTRL 的最佳化演算法，因此每個時間點 $t$ 計算完誤差後就可以更新參數。</p>

<h3 id="section-9">問題</h3>

<p>由於<strong>輸出閘門</strong>為 $0$ 時記憶單元的輸出等同於 $0$，導致基於記憶單元輸出計算所得的閘門與記憶單元本身無法觀察到<strong>記憶單元的內部狀態</strong>，作者認為在後續提出的任務中此現象會影響模型的表現。</p>

<h2 id="lstm--peephole-connections">LSTM + Peephole Connections</h2>

<h3 id="section-10">模型架構</h3>

<p><a name="paper-fig-1"></a></p>

<p>圖 1：LSTM 加上 peephole connections。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/G7Pgl3D.png" alt="圖 1" /></p>

<ul>
  <li>針對前述問題提出的解決方法為 peephole connections
    <ul>
      <li>所有閘門與記憶單元內部狀態相接</li>
      <li>最佳化時梯度不會經由 peephole connections 傳播（手動將梯度設為 $0$）</li>
    </ul>
  </li>
</ul>

<p>因此 $\eqref{1}$ 中的<strong>遺忘閘門</strong>與<strong>輸入閘門</strong>計算方法改成如下：</p>

\[\begin{align*}
g &amp; \in \set{\opfg, \opig} \\
\opnet_k^g(t) &amp; = \sum_{q = 1}^{\din + \nblk \cdot (3 + \dblk)} w_{k, q}^g \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q + u_k^g \odot s^{\blk{k}}(t - 1) \\
y^g(t) &amp; = f^g(\opnet^g(t))
\end{align*} \tag{13}\label{13}\]

<p>其中 $\ufg_k, \uig_k$ 的維度為 $1 \times \dblk$，$k$ 的範圍為 $1, \dots, \nblk$。</p>

<p>$\eqref{13}$ 的計算表示 $t$ 時間點的<strong>遺忘閘門</strong>與<strong>輸入閘門</strong>會與 $t - 1$ 時間點的<strong>記憶單元內部狀態相連</strong>，並且閘門只會與對應的記憶單元連接。</p>

<p>$\eqref{2}$ 的計算方法不變，在完成 $\eqref{2}$ 的計算後以 $t$ 時間點的記憶單元內部狀態計算<strong>輸出閘門</strong>（注意不是 $t - 1$）：</p>

\[\begin{align*}
\opnet_k^{\opog}(t) &amp; = \sum_{q = 1}^{\din + \nblk \cdot (3 + \dblk)} \wog_{k, q} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q + \uog_k \odot s^{\blk{k}}(t) \\
y^{\opog}(t) &amp; = f^{\opog}(\opnet^{\opog}(t))
\end{align*} \tag{14}\label{14}\]

<p>其中 $u_k^{\opog}$ 的維度為 $1 \times \dblk$，$k$ 的範圍為 $1, \dots, \nblk$。</p>

<p>$\eqref{14}$ 的計算表示 $t$ 時間點的<strong>輸出閘門</strong>會與 $t$ 時間點的<strong>記憶單元內部狀態相連</strong>，並且閘門只會與對應的記憶單元連接。</p>

<p>剩餘的計算方法（$\eqref{4}, \eqref{5}$）不變。</p>

<h3 id="section-11">最佳化</h3>

<p>由於只有閘門的計算方法受到影響，而且梯度不會流出 peephole connections，因此 $\eqref{8} \eqref{9} \eqref{10}$ 都不受影響，只需探討 $\ufg, \uig, \uog$ 的更新方法。</p>

<h4 id="section-12">輸出閘門參數的剩餘梯度</h4>

\[\begin{align*}
\pd{\oploss(t)}{\uog_{k, q}} &amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \\
&amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{y_k^{\opog}(t)}} \cdot \pd{y_k^{\opog}(t)}{\netog{k}{t}} \cdot \pd{\netog{k}{t}}{\uog_{k, q}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \\
&amp; \quad \pa{\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot s_j^{\blk{k}}(t)} \cdot \dfnetog{k}{t} \cdot s_q^{\blk{k}}(t)\Bigg]
\end{align*} \tag{15}\label{15}\]

<p>其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \dblk$，$\eqref{15}$ 式就是論文的 24 式。</p>

<h4 id="section-13">輸入閘門參數的剩餘梯度</h4>

\[\begin{align*}
&amp; \pd{\oploss(t)}{\uig_{k, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \pd{s_j^{\blk{k}}(t)}{\uig_{k, q}}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \\
&amp; \quad \quad \br{y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\uig_{k, q}} + \pd{s_j^{\blk{k}}(t)}{y_k^{\opig}(t)} \cdot \pd{y_k^{\opig}(t)}{\netig{k}{t}} \cdot \pd{\netig{k}{t}}{\uig_{k, q}}}\Bigg)\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp; \quad \quad \br{y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\uig_{k, q}} + \gnetblk{j}{k}{t} \cdot \dfnetig{k}{t} \cdot s_q^{\blk{k}}(t - 1)}\Bigg)\Bigg]
\end{align*} \tag{16}\label{16}\]

<p>其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \dblk$，$\eqref{16}$ 式就是論文的 22 式。</p>

<h4 id="section-14">遺忘閘門參數的剩餘梯度</h4>

\[\begin{align*}
&amp; \pd{\oploss(t)}{\ufg_{k, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \pd{s_j^{\blk{k}}(t)}{\ufg_{k, q}}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \\
&amp; \quad \quad \br{\pd{y_k^{\opfg}(t)}{\netfg{k}{t}} \cdot \pd{\netfg{k}{t}}{\ufg_{k, q}} \cdot s_j^{\blk{k}}(t - 1) + y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\ufg_{k, q}}}\Bigg)\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp; \quad \quad \br{\dfnetfg{k}{t} \cdot s_q^{\blk{k}}(t - 1) \cdot s_j^{\blk{k}}(t - 1) + y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\ufg_{k, q}}}\Bigg)\Bigg]
\end{align*} \tag{17}\label{17}\]

<p>其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \dblk$，$\eqref{17}$ 式就是論文的 23 式。</p>

<h4 id="section-15">梯度下降</h4>

<p>計算完上述所有參數後使用<strong>梯度下降</strong>（gradient descent）進行參數更新</p>

\[\begin{align*}
\uog_{k, q} &amp; \leftarrow \uog_{k, q} - \alpha \cdot \pd{\oploss(t)}{\uog_{k, q}} \\
\uig_{k, q} &amp; \leftarrow \uig_{k, q} - \alpha \cdot \pd{\oploss(t)}{\uig_{k, q}} \\
\ufg_{k, q} &amp; \leftarrow \uig_{k, q} - \alpha \cdot \pd{\oploss(t)}{\ufg_{k, q}}
\end{align*} \tag{18}\label{18}\]

<p>由於使用基於 RTRL 的最佳化演算法，因此每個時間點 $t$ 計算完誤差後就可以更新參數。</p>

<h2 id="section-16">實驗設計</h2>

<h3 id="section-17">模型架構</h3>

<p><a name="paper-fig-2"></a></p>

<p>圖 2：實驗所採用的 LSTM 架構。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/l1IUgTV.png" alt="圖 2" /></p>

<p>所有實驗都使用相同架構，根據實驗作者發現少量的參數就可以達成所有任務。</p>

<table>
  <thead>
    <tr>
      <th>參數</th>
      <th>數值（或範圍）</th>
      <th>備註</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\din$</td>
      <td>$1$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\nblk$</td>
      <td>$1$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\dblk$</td>
      <td>$1$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\dout$</td>
      <td>$1$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\dim(\wblk{1})$</td>
      <td>$\dblk \times [\din + \nblk \cdot \dblk + 1]$</td>
      <td>只與輸入和記憶單元輸出相接，有額外使用偏差項</td>
    </tr>
    <tr>
      <td>$\dim(\wfg)$</td>
      <td>$\nblk \times [\din + \nblk \cdot \dblk + 1]$</td>
      <td>只與輸入和記憶單元輸出相接，有額外使用偏差項</td>
    </tr>
    <tr>
      <td>$\dim(\wig)$</td>
      <td>$\nblk \times [\din + \nblk \cdot \dblk + 1]$</td>
      <td>只與輸入和記憶單元輸出相接，有額外使用偏差項</td>
    </tr>
    <tr>
      <td>$\dim(\wog)$</td>
      <td>$\nblk \times [\din + \nblk \cdot \dblk + 1]$</td>
      <td>只與輸入和記憶單元輸出相接，有額外使用偏差項</td>
    </tr>
    <tr>
      <td>$\dim(\ufg_k)$</td>
      <td>$1 \times \dblk$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\dim(\uig_k)$</td>
      <td>$1 \times \dblk$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\dim(\uog_k)$</td>
      <td>$1 \times \dblk$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\dim(\wout)$</td>
      <td>$\dout \times [\nblk \cdot \dblk + 1]$</td>
      <td>外部輸入沒有直接連接到總輸出，有額外使用偏差項</td>
    </tr>
    <tr>
      <td>遺忘閘門偏差項初始值</td>
      <td>$-2$</td>
      <td><a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 採用的初始值為正數，這裡居然用負數</td>
    </tr>
    <tr>
      <td>輸入閘門偏差項初始值</td>
      <td>$0$</td>
      <td><a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 採用的初始值為負數，這裡居然用 $0$</td>
    </tr>
    <tr>
      <td>輸出閘門偏差項初始值</td>
      <td>$2$</td>
      <td><a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 採用的初始值為負數，這裡居然用正數</td>
    </tr>
    <tr>
      <td>參數初始化範圍</td>
      <td>$[-0.1, 0.1]$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$g^{\blk{k}}$</td>
      <td>$g^{\blk{k}}(x) = x$</td>
      <td>identity mapping</td>
    </tr>
    <tr>
      <td>$f^{\opout}$</td>
      <td>$\sigma$</td>
      <td>只有在模擬週期函數任務中採用 identity mapping</td>
    </tr>
    <tr>
      <td>Learning rate</td>
      <td>$10^{-5}$</td>
      <td> </td>
    </tr>
    <tr>
      <td>總參數量</td>
      <td>$17$</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h3 id="section-18">實驗細節</h3>

<ul>
  <li>與預測目標相減的絕對值作為誤差進行評估
    <ul>
      <li>在凸波延遲偵測與生成任務中誤差必須小於 $0.49$</li>
      <li>在模擬週期函數任務中誤差必須小於 $0.3$</li>
    </ul>
  </li>
  <li>連續輸入只會在以下其中一個條件發生時停止
    <ul>
      <li>單一時間點模型預測誤差過大</li>
      <li>在訓練時成功連續預測 $100$ 個凸波延遲</li>
      <li>在測試時成功連續預測 $1000$ 個凸波延遲</li>
    </ul>
  </li>
  <li>一次實驗最多進行 $10^7$ 次訓練，每執行一次訓練就進行一次測試
    <ul>
      <li>每次訓練模型都是接收連續輸入</li>
      <li>在凸波延遲偵測任務中模型最多訓練 $10^8$ 次</li>
    </ul>
  </li>
  <li>總共實驗 $10$ 次，呈現平均實驗結果</li>
  <li>訓練資料與測試資料皆為隨機產生，產生方法完全相同</li>
  <li>梯度下降而外使用動量（momentum，細節請看<a href="/optimization/2021/12/07/learning-representations-by-backpropagating-errors.html">我的筆記</a>），動量超參數以 $\eta$ 表示
    <ul>
      <li>在連續凸波延遲偵測中 $\eta = 0.9999$</li>
      <li>在非連續凸波延遲偵測中 $\eta = 0.99$</li>
      <li>在凸波生成中 $\eta = 0.999$</li>
      <li>在模擬週期函數中 $\eta = 0.99$</li>
    </ul>
  </li>
  <li>論文沒寫但我猜最佳化目標一樣是 MSE</li>
</ul>

<h2 id="section-19">實驗 1：凸波延遲偵測</h2>

<h3 id="section-20">任務定義</h3>

<p>輸入只會是 $0$ 或 $1$，$1$ 代表凸波，輸入序列的產生方法如下：</p>

<ul>
  <li>第 $1$ 個凸波產生的時間點為 $T(1) = F + I(1)$
    <ul>
      <li>$F \in \N$ 代表凸波週期，是一個常數</li>
      <li>$I(1) \in \N$ 代表第 $1$ 個週期的延遲時間</li>
      <li>因此 $T(1) \geq F$</li>
    </ul>
  </li>
  <li>令 $n \geq 2$，第 $n$ 個凸波產生的時間點為 $T(n) = T(n - 1) + F + I(n)$
    <ul>
      <li>$I(n) \in \N$ 代表第 $n$ 個週期的延遲時間</li>
    </ul>
  </li>
  <li>模型必須要預測每個凸波的延遲時間（Measuring Spike Delays，MSD）
    <ul>
      <li>令 $n \in \N$，任務等同於在第 $T(n)$ 時間點輸出 $I(n)$</li>
      <li>已知週期 $F$，LSTM 必須在接收 $F - 1$ 個 $0$ 開始紀錄延遲的時間差</li>
    </ul>
  </li>
  <li>任務分為連續輸入（MSD）與非連續輸入（non MSD，NMSD）
    <ul>
      <li>NMSD 的版本一次訓練只有一筆資料，即 $n = 1$</li>
      <li>MSD 的版本一次訓練有多筆資料串接，$n = 100$</li>
    </ul>
  </li>
</ul>

<h3 id="section-21">實驗結果</h3>

<p><a name="paper-fig-3"></a></p>

<p>圖 3：凸波偵測實驗結果。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/RmIcNfd.png" alt="圖 3" /></p>

<p><a name="paper-fig-4"></a></p>

<p>圖 4：凸波偵測實驗結果，分析週期長度對於表現的影響。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/nEDzkG3.png" alt="圖 4" /></p>

<p><a name="paper-fig-5"></a></p>

<p>圖 5：凸波偵測實驗結果，增加延遲可能範圍進行實驗。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/JlSE6y7.png" alt="圖 5" /></p>

<ul>
  <li>在 NMSD 任務中根據<a href="#paper-fig-3">圖 3</a> 與<a href="#paper-fig-4">圖 4</a> 實驗結果說明週期愈長（$F$ 愈大）愈不容易偵測
    <ul>
      <li>即使 $I(n) \in \set{0, 1}$ 在週期較長的狀況下偵測延遲仍然很困難</li>
    </ul>
  </li>
  <li>雖然 peephole connections 在這個任務中不重要，但仍然比 <a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 表現還要好</li>
  <li>作者進一步將 $I(n)$ 的範圍調大，並且將 $f^{\opout}$ 從 sigmoid 函數改成 identity mapping（因為 sigmoid 的數值範圍只能落在 $[0, 1]$）進行實驗（見<a href="#paper-fig-5">圖 5</a>）
    <ul>
      <li>令 $i \in \set{1, \dots, 10}$，$I(n)$ 可以是 $\set{0, i}$ 或 $\set{0, \dots, i}$</li>
      <li>週期 $F = 10$</li>
    </ul>
  </li>
  <li>在 NMSD 任務中根據<a href="#paper-fig-5">圖 5</a> 可以得到以下結論
    <ul>
      <li>在此時驗中 <a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 表現比 peephole connection 好</li>
      <li>延遲範圍差異愈大 LSTM 收斂愈快，作者認為過大的延遲差異會有明顯的特徵（見<a href="#paper-fig-5">圖 5</a> 下半）</li>
      <li>當預測範圍可能性變多時，當最大延遲不超過 $5$ 時容易收斂，一旦超過 $5$ 則收斂變慢（見<a href="#paper-fig-5">圖 5</a> 上半）</li>
      <li>在 $I(n) \in \set{0, 1}$ 時，使用 identity mapping 作為輸出函數表現比使用 sigmoid（見<a href="#paper-fig-3">圖 3</a>）還要好，作者認為 sigmoid 會讓 gradient 變小所以收斂較慢</li>
    </ul>
  </li>
</ul>

<h3 id="section-22">分析</h3>

<p><a name="paper-fig-6"></a></p>

<p>圖 6：凸波偵測實驗中 <a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 的計算狀態。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/ma5loA3.png" alt="圖 6-1" />
<img src="https://i.imgur.com/adTLK96.png" alt="圖 6-2" /></p>

<p><a name="paper-fig-7"></a></p>

<p>圖 7：凸波偵測實驗中 <a href="https://www.jmlr.org/papers/v3/gers02a.html">LSTM + peephole connections</a> 的計算狀態。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/ZOukPCr.png" alt="圖 7-1" />
<img src="https://i.imgur.com/4GoR9TE.png" alt="圖 7-2" /></p>

<ul>
  <li>透過實驗觀察發現 <a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 學會兩種不同的方法進行凸波延遲偵測
    <ul>
      <li><a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 可以在每個時間點都增加記憶單元內部狀態 $s^{\blk{1}}$ 一點點，而預測值可以靠累加結果轉換而得（見<a href="#paper-fig-6">圖 6</a> 左半）</li>
      <li><a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 可以學會模擬振盪器，並根據振盪的次數進行預測（見<a href="#paper-fig-6">圖 6</a> 右半）</li>
    </ul>
  </li>
  <li>從<a href="#paper-fig-6">圖 6</a> 的下半可以發現<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 的輸出閘門維持在 $1$ 的狀態
    <ul>
      <li>作者認為由於預測行為很少發生，因此維持輸出並不會影響表現</li>
      <li>但當任務需要預測的頻率變高時，模型就必須只在適當的時間點開啟輸出閘門，而該行為在沒有 peephole connections 的狀況下無法達成（原始 LSTM 架構的輸出閘門只會獲得 $t - 1$ 時間點的計算狀態，並沒有 $t$ 時間點的記憶單元內部狀態）</li>
    </ul>
  </li>
  <li>從<a href="#paper-fig-7">圖 7</a> 的下半可以發現加上 peephole connections 的 LSTM 會在大多數時間關閉輸出閘門
    <ul>
      <li>由於新加入的機制比<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 更複雜，因此需要更多的時間才會收斂（見<a href="#paper-fig-3">圖 3</a> 與<a href="#paper-fig-5">圖 5</a>）</li>
    </ul>
  </li>
</ul>

<h2 id="section-23">實驗 2：凸波生成</h2>

<h3 id="section-24">任務定義</h3>

<p>將凸波延遲偵測任務的輸入與輸出互換，稱為凸波生成（Generating Timed Spikes，GTS）</p>

<ul>
  <li>論文沒說明確的輸入輸出結構，但我的猜測如下
    <ul>
      <li>輸入是 $T(n) \in \N$ 時，接下來的模型輸入會是 $T(n) - 1$ 個 $0$</li>
      <li>輸出是 $T(n) - 1$ 個 $0$，尾巴跟著一個 $1$</li>
    </ul>
  </li>
  <li>由於 LSTM 在不直接觀察記憶單元內部狀態的情況下無法完成 GTS（絕大多數的輸入都是 $0$），因此只顯示 peephole connections 的實驗</li>
</ul>

<h3 id="section-25">實驗結果</h3>

<p><a name="paper-fig-8"></a></p>

<p>圖 8：凸波生成實驗結果。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/arXExQj.png" alt="圖 8" /></p>

<p><a name="paper-fig-9"></a></p>

<p>圖 9：凸波生成實驗結果。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/HqjloKX.png" alt="圖 9-1" />
<img src="https://i.imgur.com/jMgR93f.png" alt="圖 9-2" /></p>

<p><a name="paper-fig-10"></a></p>

<p>圖 10：凸波生成實驗分析。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/qruAa3O.png" alt="圖 10-1" />
<img src="https://i.imgur.com/ZnmkEO0.png" alt="圖 10-2" /></p>

<ul>
  <li>根據<a href="#paper-fig-8">圖 8</a> 我們可以發現週期愈長收斂時間愈久，與<a href="#paper-fig-3">圖 3</a> 觀察結果相同
    <ul>
      <li>LSTM + peephole connections 可以解決圖波生成任務</li>
    </ul>
  </li>
  <li>根據<a href="#paper-fig-9">圖 9</a> 下半我們可以發現輸出閘門只在需要生成凸波時開啟，生成完畢後馬上關閉
    <ul>
      <li><a href="#paper-fig-9">圖 9</a> 左下顯示生成凸波的當下由於遺忘閘門與輸入閘門一起關閉，因此記憶單元內部狀態直接重設為 $0$</li>
    </ul>
  </li>
  <li>作者嘗試在訓練時將遺忘閘門移除，發現模型無法收斂，證實遺忘閘門的必須性</li>
  <li>根據<a href="#paper-fig-10">圖 10</a> 可以觀察到模型生成凸波的時間點跟記憶單元內部狀態的增減時間相同</li>
</ul>

<h2 id="section-26">實驗 3：模擬週期函數</h2>

<h3 id="section-27">任務定義</h3>

<p>讓 LSTM 模型模擬週期函數（Periodic Function Generation，PFG），注意訓練過程不需要給模型輸入，只要有輸出能夠模擬誤差即可，在此任務中就不得不使用 peephole connection（因為沒有輸入）。</p>

<p>令抽樣頻率為 $F$，模擬的週期函數共有三種，分別是三角函數波 $f_{\cos}$、三角波 $f_{\optri}$ 與方波 $f_{\oprect}$</p>

\[\begin{align*}
f_{\cos}(t) &amp; = \frac{1}{2} \pa{1 - \cos\pa{\frac{2\pi t}{F}}} \\
f_{\optri}(t) &amp; = \begin{dcases}
\frac{2 (t \opmod F)}{F} &amp; \text{if } (t \opmod F) &gt; \frac{F}{2} \\
2 - \frac{2 (t \opmod F)}{F} &amp; \text{otherwise}
\end{dcases} \\
f_{\oprect}(t) &amp; = \begin{dcases}
1 &amp; \text{if } (t \opmod F) &gt; \frac{F}{2} \\
0 &amp; \text{otherwise}
\end{dcases}
\end{align*}\]

<p>模擬週期函數的難度與函數本身的波型（shape）和週期有關，而波型本身可以用一次微分和二次微分進行描述，論文採用一二次微分函數的最大絕對值 $\max_t \abs{f’(t)}$ 與 $\max_t \abs{f’{}’(t)}$ 作為特徵代表。</p>

<p>由於離散的時間點無法微分，作者將不可微分的函數用以下公式模擬微分</p>

\[\begin{align*}
f'(t) &amp; \coloneqq f(t + 1) - f(t) \\
f'{}'(t) &amp; \coloneqq f'(t + 1) - f'(t) \\
&amp; \coloneqq f(t + 2) - 2 f(t + 1) + f(t)
\end{align*}\]

<p>因此當 $t^{\star} = \frac{F}{4}$ 時我們可以得到</p>

\[\begin{align*}
\max_t \abs{f_{\cos}'(t)} &amp; = \max_t \abs{\frac{1}{2} \sin\pa{\frac{2 \pi t}{F}} \frac{2 \pi}{F}} \\
&amp; = \max_t \abs{\frac{\pi}{F} \sin\pa{\frac{2 \pi t}{F}}} \\
&amp; = \abs{\frac{\pi}{F} \sin\pa{\frac{2 \pi t^{\star}}{F}}} \\
&amp; = \frac{\pi}{F}
\end{align*}\]

<p>當 $t^{\star} = 0$ 時我們可以得到</p>

\[\begin{align*}
\max_t \abs{f'{}_{\cos}'(t)} &amp; = \max_t \abs{\frac{\pi}{F} \cos\pa{\frac{2 \pi t}{F}} \frac{2 \pi}{F}} \\
&amp; = \abs{\frac{\pi}{F} \cos\pa{\frac{2 \pi t^{\star}}{F}} \frac{2 \pi}{F}} \\
&amp; = \frac{2 \pi^2}{F^2}
\end{align*}\]

<p>當 $((t^{\star} + 1) \opmod F) &lt; \frac{F}{2}$ 時我們可以得到</p>

\[\begin{align*}
\max_t \abs{f_{\optri}'(t)} &amp; = \abs{f_{\optri}(t^{\star} + 1) - f_{\optri}(t^{\star})} \\
&amp; = \abs{2 - \frac{2 ((t^{\star} + 1) \opmod F)}{F} - 2 + \frac{2 (t^{\star} \opmod F)}{F}} \\
&amp; = \abs{-\frac{2 (t^{\star} + 1)}{F} + \frac{2t^{\star}}{F}} \\
&amp; = \frac{2}{F}
\end{align*}\]

<p>當 $((t^{\star} + 1) \opmod F) = \frac{F}{2}$ 時我們可以得到</p>

\[\begin{align*}
\max_t \abs{f'{}_{\optri}'(t^{\star})} &amp; = \abs{f_{\optri}(t^{\star} + 2) - 2f_{\optri}(t^{\star} + 1) + f_{\optri}(t^{\star})} \\
&amp; = \abs{\frac{2 ((t^{\star} + 2) \opmod F)}{F} - 4 + \frac{4 ((t^{\star} + 1) \opmod F)}{F} + 2 - \frac{2 (t^{\star} \opmod F)}{F}} \\
&amp; = \abs{\frac{2(t^{\star} + 2)}{F} - 4 + \frac{4F}{2F} + 2 - \frac{2t^{\star}}{F}} \\
&amp; = \frac{4}{F}
\end{align*}\]

<p>當 $(t^{\star} \opmod F) = \frac{F}{2}$ 時我們可以得到</p>

\[\begin{align*}
\max_t \abs{f_{\oprect}'(t)} &amp; = \abs{f_{\oprect}(t^{\star} + 1) - f_{\oprect}(t^{\star})} \\
&amp; = \abs{1 - 0 + 0} \\
&amp; = 1
\end{align*}\]

<p>當 $((t^{\star} + 1) \opmod F) = \frac{F}{2}$ 時我們可以得到</p>

\[\begin{align*}
\max_t \abs{f'{}_{\oprect}'(t)} &amp; = \abs{f_{\oprect}(t^{\star} + 2) - 2f_{\oprect}(t^{\star} + 1) + f_{\oprect}(t^{\star})} \\
&amp; = \abs{1 - 0} \\
&amp; = 1
\end{align*}\]

<p>一般來說 $\max_t \abs{f’(t)}$ 與 $\max_t \abs{f’{}’(t)}$ 愈大代表波型變化愈大，因此愈難模擬。</p>

<p>而 $F$ 愈大代表同一個週期內的波型變化較多，因此 $F$ 愈大愈難模擬，此實驗的 $F \in \set{10, 25}$。</p>

<h3 id="section-28">實驗結果</h3>

<p><a name="paper-fig-11"></a></p>

<p>圖 11：模擬週期函數實驗結果。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/zIALWJF.png" alt="圖 11" /></p>

<p><a name="paper-fig-12"></a></p>

<p>圖 12：模擬週期函數實驗結果。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/v8wFmJ2.png" alt="圖 12-1" />
<img src="https://i.imgur.com/ctHi291.png" alt="圖 12-2" /></p>

<ul>
  <li><a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 只能模擬 $F = 10$ 的 $f_{\cos}$，且收斂時間長（見<a href="#paper-fig-11">圖 11</a>）</li>
  <li>不使用遺忘閘門的<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 無法模擬超過兩個以上的週期</li>
  <li>將評估標準提生成誤差低於 $0.15$ 時，模型要花更長的時間收斂
    <ul>
      <li>模擬的週期函數為 $f_{\cos}$，$F = 25$</li>
      <li>RMSE 的表現從 $0.17 \pm 0.019$ （見<a href="#paper-fig-11">圖 11</a>） 降至 $0.086 \pm 0.002$</li>
      <li>產生完美表現（$100\%$ 預測正確）的時間點為 $(2704 \pm 49) \cdot 10^3$</li>
    </ul>
  </li>
</ul>

<h3 id="section-29">分析</h3>

<p><a name="paper-fig-13"></a></p>

<p>圖 13：模擬週期函數實驗分析。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/SJ43cWb.png" alt="圖 13-1" />
<img src="https://i.imgur.com/pRKxTpM.png" alt="圖 13-2" /></p>

<p><a name="paper-fig-14"></a></p>

<p>圖 14：模擬週期函數實驗分析。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/gW5bmcu.png" alt="圖 14-1" />
<img src="https://i.imgur.com/7TIXIqV.png" alt="圖 14-2" /></p>

<p><a name="paper-fig-15"></a></p>

<p>圖 15：模擬週期函數實驗分析。
圖片來源：<a href="https://www.jmlr.org/papers/v3/gers02a.html">論文</a>。</p>

<p><img src="https://i.imgur.com/biv8smX.png" alt="圖 15" /></p>

<ul>
  <li>由於模型沒有收到任何輸入，完全只能依賴記憶單元內部狀態進行模擬，因此記憶單元內部狀態的數值變化應該要與模擬目標擁有類似的曲線（見<a href="#paper-fig-13">圖 13</a>）</li>
  <li>作者認為此任務可以在不使用 peephole connections 的狀態下完成任務，但流經閘門的梯度被手動丟棄，因此 <a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 的架構很難最佳化，導致實驗表現不佳（見<a href="#paper-fig-11">圖 11</a>）
    <ul>
      <li>LSTM + peephole connections 收斂速度比 <a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">LSTM-2000</a> 快，見<a href="#paper-fig-11">圖 11</a></li>
      <li>觀察 peephole connections 的參數數值，作者發現數值與記憶單元輸出連接到閘門的參數數量級相同，說明 peephole connections 真的有被用來協助模擬週期函數</li>
    </ul>
  </li>
  <li>從<a href="#paper-fig-14">圖 14</a> 可以觀察到以下現象
    <ul>
      <li>方波值為 $1$ 時
        <ul>
          <li>記憶單元輸出與記憶單元內部狀態的數值相同</li>
          <li>輸出閘門維持開啟</li>
          <li>模型內部狀態逐漸遞減（趨向 $0$）</li>
          <li>由於記憶單元輸出與記憶單元輸入連結的參數數值為負，因此模型有辦法遞減記憶單元內部狀態</li>
        </ul>
      </li>
      <li>方波值為 $0$ 時
        <ul>
          <li>記憶單元輸出為 $0$</li>
          <li>輸出閘門維持關閉</li>
          <li>模型內部狀態逐漸遞增（趨向 $1$）</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>從<a href="#paper-fig-15">圖 15</a> 可以發現模型的初始計算狀態為 $0$，但開始計算後模型計算狀態再也不為 $0$
    <ul>
      <li>這表示模型<strong>初始計算狀態</strong>應該也被當成<strong>參數</strong>一起訓練</li>
    </ul>
  </li>
</ul>



<!-- Avoid copy by China, method 1. -->
<script>
  console.log(`
台.灣.獨.立.香.港.獨.立.西.藏.獨.立.新.疆.獨.立.內.蒙.古.獨.立.......
..... ... ................:fffffLt.............一.九.八.九.六.四
... ,i111i;............. ;L111111LL....習.包.子.近.平.小.熊.維.尼
...iLfttttff,  ..,::;;;;;Cftt11111C:...........中.國.武.漢.肺.炎
..;C1111111tf:ittfttttttttttfffft1L,..........................
. ff11111111fCtt11111111111111ttfC1 ..........................
. tf11111111ft11111111111111111111tt;. .......................
..:L11111111t11111111111111111111111fti, .....................
.. ;Lt11111111111111111111111111111111tCt:, ..................
... ,tf111111111111111111tLLLt111111111LG00t..................
.... 1f11111111111111111LGLtt1111111111111tLt.................
....:f11111111111111111tf1111111111111111111t.................
... 1f111111111111111111111111111111tt11111t,.................
... tt111111111111111111111111111111L11LG11f:.................
... tt111111111111111111111tGC111111Lt108111tti:..............
... if111111111111111111111f8C111111ttttttfft1ttt,............
....,f111111111111111111111111111111111fG0088C11tf............
.....tt111111111111111111111111111111110@888@0111f; ..........
.....,f111111111111111111111111111111111C000CL111Li ..........
......;f11111111111111111111tt11111111111111ft111L,...........
...... tt111111111111111111tfGf111111111111ff111L; ...........
.....;fCt11111111111111111111f0Cft1111111fLt111Li ............
.... fCCLftt111111111111111111fGGGCLLfLfff1111t; .............
.....:CLLCCCCLLftt1111111111111tfLCGGG0L1111fL;...............
..... LLLLLLLLLCCCLLftt1111111111ttffttt1ttCCLGf..............
.... iCLLLLLLLLLLLLLCCCLLftt11111111tfffft1fGCC; .............
... ;CLLLLLLLLLLLLCLLLLLLLCCLLLLLLffft1111tLGLL:,.. ..........
...:GLLLLLLLLLLLLLCCCCCLLLLLLLLLCCLLLLLLLLCCLGGLLf1i:,. ......
.. iCLLLLLLLLLLLLLLLLLCCCCLLLLLLLLLLLLLLLLLLCLCCLLLCCCti. ....
...iCLLLLLLLLLLLLLLLLLLLLCCCCLLLLLLLLLLLLLLLLLLCCLLLGCCCLi,...
.. iCLLLLLLLLLLLLLLLLLLLLLLCCGCCCLLLLLLLLLLLLLLLCLLCGLCCCGCti,
. :LLLLLLLLLLLLLLLLLLLLLLCLt111tffLLCCCCCLLLLLLLGCLGGCLftLCCCL
.;CLLLLLLLLLLLLLLLLLLLLLCf11111111111tttffLLLLLLLLfLft111CCCCG
;GLLLLLLLLLLCCLLLLLLLLLCt1111111111111111111111111111111111ttt
.;CLLLLLLLLLLCCCLLLLLLCf11111111111111111111111111111111111111
`)
</script>

<!-- Avoid copy by China, method 2. -->
<!--
台獨教父 Xi Jinping
⣿⣿⣿⣿⣿⠟⠋⠄⠄⠄⠄⠄⠄⠄⢁⠈⢻⢿⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⠃⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠈⡀⠭⢿⣿⣿⣿⣿
⣿⣿⣿⣿⡟⠄⢀⣾⣿⣿⣿⣷⣶⣿⣷⣶⣶⡆⠄⠄⠄⣿⣿⣿⣿
⣿⣿⣿⣿⡇⢀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣧⠄⠄⢸⣿⣿⣿⣿
⣿⣿⣿⣿⣇⣼⣿⣿⠿⠶⠙⣿⡟⠡⣴⣿⣽⣿⣧⠄⢸⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣾⣿⣿⣟⣭⣾⣿⣷⣶⣶⣴⣶⣿⣿⢄⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣿⡟⣩⣿⣿⣿⡏⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣹⡋⠘⠷⣦⣀⣠⡶⠁⠈⠁⠄⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣍⠃⣴⣶⡔⠒⠄⣠⢀⠄⠄⠄⡨⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣦⡘⠿⣷⣿⠿⠟⠃⠄⠄⣠⡇⠈⠻⣿⣿⣿⣿
⣿⣿⣿⣿⡿⠟⠋⢁⣷⣠⠄⠄⠄⠄⣀⣠⣾⡟⠄⠄⠄⠄⠉⠙⠻
⡿⠟⠋⠁⠄⠄⠄⢸⣿⣿⡯⢓⣴⣾⣿⣿⡟⠄⠄⠄⠄⠄⠄⠄⠄
⠄⠄⠄⠄⠄⠄⠄⣿⡟⣷⠄⠹⣿⣿⣿⡿⠁⠄⠄⠄⠄⠄⠄⠄⠄
⠄⠄⠄⠄⠄⠄⣸⣿⡷⡇⠄⣴⣾⣿⣿⠃⠄⠄⠄⠄⠄⠄⠄⠄⠄
⠄⠄⠄⠄⠄⠄⣿⣿⠃⣦⣄⣿⣿⣿⠇⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄
⠄⠄⠄⠄⠄⢸⣿⠗⢈⡶⣷⣿⣿⡏⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄
动态网自由门 天安門 天安门 法輪功 李洪志 Free Tibet
六四天安門事件 The Tiananmen Square protests of 1989
天安門大屠殺 The Tiananmen Square Massacre
反右派鬥爭 The Anti-Rightist Struggle
大躍進政策 The Great Leap Forward
文化大革命 The Great Proletarian Cultural Revolution
人權 Human Rights
民運 Democratization
自由 Freedom
獨立 Independence
多黨制 Multi-party system
台灣 臺灣 Taiwan Formosa
西藏 土伯特 唐古特 Tibet
達賴喇嘛 Dalai Lama
法輪功 Falun Dafa
新疆維吾爾自治區 The Xinjiang Uyghur Autonomous Region
諾貝爾和平獎 Nobel Peace Prize
劉暁波 Liu Xiaobo
民主 言論 思想 反共 反革命 抗議 運動 騷亂 暴亂 騷擾 擾亂 抗暴 平反 維權 示威游行 李洪志
法輪大法 大法弟子 強制斷種 強制堕胎 民族淨化 人體實驗 肅清 胡耀邦 趙紫陽 魏京生 王丹
還政於民 和平演變 激流中國 北京之春 大紀元時報 九評論共産黨 獨裁 專制 壓制 統一 監視 鎮壓
迫害 侵略 掠奪 破壞 拷問 屠殺 活摘器官 誘拐 買賣人口
Winnie the Pooh
-->

    </div>

</article>
<div class="post-nav"><a class="previous" href="/text%20modeling/2021/12/21/finding-structure-in-time.html" title="Finding Structure in Time">Finding Structure in Time</a><a class="next" href="/acoustic%20modeling/2022/03/01/long-short-term-memory-based-recurrent-neural-network-architectures-for-large-scale-vocabulary-speech-recognition.html" title="Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Scale Vocabulary Speech Recognition">Long Short-Term Memory Based Recurrent Neural...</a></div><div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li><a class="post-link" href="/dataset/2022/08/06/a-standard-corpus-of-edited-present-day-american-english.html" title="Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Scale Vocabulary Speech Recognition">A Standard Corpus of Edited Present-Day American English</a></li><li><a class="post-link" href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html" title="Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Scale Vocabulary Speech Recognition">Long Short-Term Memory</a></li><li><a class="post-link" href="/text%20modeling/2022/08/29/generating-sequences-with-recurrent-neural-networks.html" title="Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Scale Vocabulary Speech Recognition">Generating Sequences with Recurrent Neural Networks</a></li><li><a class="post-link" href="/acoustic%20modeling/2021/12/06/a-time-delay-neural-network-architecture-for-isolated-word-recognition.html" title="Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Scale Vocabulary Speech Recognition">A time-delay neural network architecture for isolated word recognition</a></li></ul>
    </div><div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><!-- Copy right part has some bugs, so I replace it with my own footer.
  Code source: https://github.com/jeffreytse/jekyll-theme-yat/blob/0fea688977e16c1f1f42c23b36b14ed325ee606b/_includes/views/footer.html
-->
<footer class="site-footer h-card">

  <div class="wrapper">
    <div class="site-footer-inner">
      <div>Copyright (c) 2021-<span id='current-year'>2021</span>
        <a href='https://github.com/ProFatXuanAll'>ProFatXuanAll</a>
      </div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="http://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div>Opinions expressed are solely my own and do not express the views or opinions of my university or my lab.
      </div>
    </div>
  </div>
  <!-- Calculate full year at runtime. -->
  <script>
    document.getElementById('current-year').innerHTML = (new Date(Date.now())).getFullYear()
  </script>
</footer></body>
</html>
