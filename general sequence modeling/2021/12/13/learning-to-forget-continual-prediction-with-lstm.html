<!DOCTYPE html>
<html lang="zh-TW"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Learning to Forget: Continual Prediction with LSTM | ML Notes</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Learning to Forget: Continual Prediction with LSTM" />
<meta property="og:locale" content="zh_TW" />
<meta name="description" content="目標 提出在 LSTM 上增加 forget gate 作者 Felix A. Gers, Jürgen Schmidhuber, Fred Cummins 期刊/會議名稱 Neural Computation 發表時間 2000 論文連結 https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM" />
<meta property="og:description" content="目標 提出在 LSTM 上增加 forget gate 作者 Felix A. Gers, Jürgen Schmidhuber, Fred Cummins 期刊/會議名稱 Neural Computation 發表時間 2000 論文連結 https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM" />
<link rel="canonical" href="/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html" />
<meta property="og:url" content="/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html" />
<meta property="og:site_name" content="ML Notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-12-13T14:09:00+08:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Learning to Forget: Continual Prediction with LSTM" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-12-13T14:09:00+08:00","datePublished":"2021-12-13T14:09:00+08:00","description":"目標 提出在 LSTM 上增加 forget gate 作者 Felix A. Gers, Jürgen Schmidhuber, Fred Cummins 期刊/會議名稱 Neural Computation 發表時間 2000 論文連結 https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM","headline":"Learning to Forget: Continual Prediction with LSTM","mainEntityOfPage":{"@type":"WebPage","@id":"/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html"},"url":"/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <script src="/assets/js/main.js"></script><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="ML Notes" /><link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8"
        src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js"></script>




  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] #fff -->
  






  
      <!-- [function][get_value] #ff4e00 -->
  






  
      <!-- [function][get_value] uppercase -->
  

<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<!-- favicon. -->
<link rel='icon' type='image/png' sizes='16x16' href='/assets/images/favicon.png' />

<!-- Noto sans CJK source code: https://dotblogs.com.tw/shadow/2019/10/27/153832 -->
<style>
  @font-face {
    font-family: 'Noto Sans TC';
    font-style: normal;
    font-weight: 100;
    src: url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Thin.woff2) format('woff2'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Thin.woff) format('woff'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Thin.otf) format('opentype');
  }

  /*預設font-weight*/
  @font-face {
    font-family: 'Noto Sans TC';
    font-style: normal;
    font-weight: 400;
    src: url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Regular.woff2) format('woff2'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Regular.woff) format('woff'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Regular.otf) format('opentype');
  }

  @font-face {
    font-family: 'Noto Sans TC';
    font-style: normal;
    font-weight: 700;
    src: url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Bold.woff2) format('woff2'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Bold.woff) format('woff'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Bold.otf) format('opentype');
  }

  @font-face {
    font-family: 'Noto Sans TC';
    font-style: normal;
    font-weight: 900;
    src: url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Black.woff2) format('woff2'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Black.woff) format('woff'),
      url(//fonts.gstatic.com/ea/notosanstc/v1/NotoSansTC-Black.otf) format('opentype');
  }

  html,
  body {
    font-family: 'Noto Sans TC';
  }
</style></head>
<body>




  
      <!-- [function][get_value] uppercase -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] 0 -->
  










  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] true -->
  

<header class="site-header " role="banner">

  <div class="wrapper">
    <div class="site-header-inner"><span class="site-brand"><a class="site-brand-inner" rel="author" href="/">
  <img class="site-favicon" title="ML Notes" src="" onerror="this.style.display='none'">
  ML Notes
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>

          <div class="trigger"><a class="page-link" href="/about.html">關於</a><a class="page-link" href="/archives.html">時間表</a><a class="page-link" href="/categories.html">筆記類別</a><a class="page-link" href="/tags.html">標籤</a>




  
      <!-- [function][get_value]  -->
  

</div>
        </nav></div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>





  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value] post-header.html -->
  






  
      <!-- [function][get_value] post-header.html -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] true -->
  






  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] 0 -->
  






  
      <!-- [function][get_value] 0 -->
  





<script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




  
      <!-- [function][get_value] off -->
  

<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>




  
      <!-- [function][get_value]  -->
  






  
      <!-- [function][get_value] [] -->
  

<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">Learning to Forget: Continual Prediction with LSTM</h1>
  <h2 class="post-subtitle"></h2>

  <p class="post-meta">
    <time class="dt-published" datetime="2021-12-13T14:09:00+08:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Dec 13, 2021
    </time>

    
    

















  
      <!-- [function][get_article_words] 10723 -->
  
















    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 1 hour 7 mins</span>
  </p><div class="post-tags"><a class="post-tag" href="/tags.html#RNN">#RNN</a><a class="post-tag" href="/tags.html#LSTM">#LSTM</a><a class="post-tag" href="/tags.html#model architecture">#model architecture</a><a class="post-tag" href="/tags.html#neural network">#neural network</a></div></header>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <!-- Setup mathjax auto rendering. -->
<!--
  Load MathJax v3.
  See
  https://docs.mathjax.org/en/latest/index.html
  and
  https://docs.mathjax.org/en/latest/web/configuration.html
  for more information.
-->
<script>
  MathJax = {
    loader: {
      load: [
        '[tex]/ams',       // Equivalent to `\usepackage{amsmath}`.
        '[tex]/cancel',    // Equivalent to `\usepackage{cancel}`.
        '[tex]/mathtools', // Equivalent to `\usepackage{mathtools}`.
        '[tex]/unicode',   // Equivalent to `\usepackage{unicode}`.
      ]
    },
    tex: {
      // Extensions to use.
      packages: { '[+]': ['ams', 'cancel', 'mathtools', 'unicode'] },
      // Start/end delimiter pairs for in-line math.
      inlineMath: [
        ['$', '$'],
        ['\\(', '\\)'],
      ],
      // Start/end delimiter pairs for display math.
      displayMath: [
        ['$$', '$$'],
        ['\\[', '\\]']
      ],
      // Use \$ to produce a literal dollar sign.
      processEscapes: true,
      // Process \begin{xxx}...\end{xxx} outside math mode.
      processEnvironments: true,
      // Process \ref{...} outside of math mode.
      processRefs: true,
      // Pattern for recognizing numbers.
      digits: /^(?:[0-9]+(?:\{,\}[0-9]{3})*(?:\.[0-9]*)?|\.[0-9]+)/,
      tags: 'ams',         // Or 'ams' or 'all'.
      useLabelIds: false,  // Use label name rather than tag for ids.
      maxMacros: 1000,     // Maximum number of macro substitutions per expression.
      maxBuffer: 5 * 1024, // Maximum size for the internal TeX string (5K).
    }
  };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<!--
  Define common LaTeX commands.

  Each command must be wrapped with $ signs.
  We use "display: none;" to avoid redudant whitespaces.
 -->

<p style="display: none;">
  <!-- Fields. -->
  $\newcommand{\field}[1]{\mathbb{#1}}$
  <!-- Natural number set. -->
  $\providecommand{\N}{}$
  $\renewcommand{\N}{\field{N}}$
  <!-- Rational field. -->
  $\providecommand{\Q}{}$
  $\renewcommand{\Q}{\field{Q}}$
  <!-- Real field. -->
  $\providecommand{\R}{}$
  $\renewcommand{\R}{\field{R}}$
  <!-- Integer set. -->
  $\providecommand{\Z}{}$
  $\renewcommand{\Z}{\field{Z}}$
  <!-- Parenthese. -->
  $\providecommand{\pa}{}$
  $\renewcommand{\pa}[1]{\left\lparen #1 \right\rparen}$
  <!-- Bracket. -->
  $\providecommand{\br}{}$
  $\renewcommand{\br}[1]{\left\lbrack #1 \right\rbrack}$
  <!-- Set. -->
  $\providecommand{\set}{}$
  $\renewcommand{\set}[1]{\left\lbrace #1 \right\rbrace}$
  <!-- Absolute value. -->
  $\providecommand{\abs}{}$
  $\renewcommand{\abs}[1]{\left\lvert #1 \right\rvert}$
  <!-- Norm. -->
  $\providecommand{\norm}{}$
  $\renewcommand{\norm}[1]{\left\lVert #1 \right\rVert}$
  <!-- Floor. -->
  $\providecommand{\floor}{}$
  $\renewcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}$
  <!-- Ceiling. -->
  $\providecommand{\ceil}{}$
  $\renewcommand{\ceil}[1]{\left\lceil #1 \right\rceil}$
  <!-- Evaluate. -->
  $\providecommand{\eval}{}$
  $\renewcommand{\eval}[1]{\left. #1 \right\rvert}$
  <!-- Partial derivative. -->
  $\providecommand{\pd}{}$
  $\renewcommand{\pd}[2]{\dfrac{\partial #1}{\partial #2}}$
  <!-- Sign function. -->
  $\DeclareMathOperator{\sign}{sign}$
  <!-- Diagonal function. -->
  $\DeclareMathOperator{\diag}{diag}$
  <!-- Argmax. -->
  $\DeclareMathOperator*{\argmax}{argmax}$
  <!-- Argmin. -->
  $\DeclareMathOperator*{\argmin}{argmin}$
  <!-- Limit in display mode.  This should only be used in inline mode. -->
  $\providecommand{\Lim}{}$
  $\renewcommand{\Lim}{\lim\limits}$
  <!-- Product in display mode.  This should only be used in inline mode. -->
  $\providecommand{\Prod}{}$
  $\renewcommand{\Prod}{\prod\limits}$
  <!-- Sum in display mode.  This should only be used in inline mode. -->
  $\providecommand{\Sum}{}$
  $\renewcommand{\Sum}{\sum\limits}$

  <!-- NN related operations. -->
  <!-- Softmax. -->
  $\DeclareMathOperator{\softmax}{softmax}$
  <!-- Concatenate. -->
  $\DeclareMathOperator{\cat}{concatenate}$

  <!-- Algorithm related tools. -->
  <!-- Procedure statement. -->
  $\providecommand{\algoProc}{}$
  $\renewcommand{\algoProc}[1]{\textbf{procedure}\text{ #1}}$
  $\providecommand{\algoEndProc}{}$
  $\renewcommand{\algoEndProc}{\textbf{end procedure}}$
  <!-- If statement. -->
  $\providecommand{\algoIf}{}$
  $\renewcommand{\algoIf}[1]{\textbf{if } #1 \textbf{ do}}$
  $\providecommand{\algoEndIf}{}$
  $\renewcommand{\algoEndIf}{\textbf{end if}}$
  <!-- Assignment -->
  $\providecommand{\algoEq}{}$
  $\renewcommand{\algoEq}{\leftarrow}$
  <!-- For statement. -->
  $\providecommand{\algoFor}{}$
  $\renewcommand{\algoFor}[1]{\textbf{for } #1 \textbf{ do}}$
  $\providecommand{\algoEndFor}{}$
  $\renewcommand{\algoEndFor}{\textbf{end for}}$
  <!-- While statement. -->
  $\providecommand{\algoWhile}{}$
  $\renewcommand{\algoWhile}[1]{\textbf{while } #1 \textbf{ do}}$
  $\providecommand{\algoEndWhile}{}$
  $\renewcommand{\algoEndWhile}{\textbf{end while}}$
  <!-- Return statement. -->
  $\providecommand{\algoReturn}{}$
  $\renewcommand{\algoReturn}{\textbf{return }}$

  <!-- Some wierd symbols cannot be showed correctly. -->
  $\providecommand{\hash}{}$
  $\renewcommand{\hash}{\unicode{35}}$

</p>

<table>
  <tbody>
    <tr>
      <td>目標</td>
      <td>提出在 LSTM 上增加 forget gate</td>
    </tr>
    <tr>
      <td>作者</td>
      <td>Felix A. Gers, Jürgen Schmidhuber, Fred Cummins</td>
    </tr>
    <tr>
      <td>期刊/會議名稱</td>
      <td>Neural Computation</td>
    </tr>
    <tr>
      <td>發表時間</td>
      <td>2000</td>
    </tr>
    <tr>
      <td>論文連結</td>
      <td><a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM</a></td>
    </tr>
  </tbody>
</table>

<!--
  Define LaTeX command which will be used through out the writing.

  Each command must be wrapped with $ signs.
  We use "display: none;" to avoid redudant whitespaces.
 -->

<p style="display: none;">

  <!-- Operator in. -->
  $\providecommand{\opnet}{}$
  $\renewcommand{\opnet}{\operatorname{net}}$
  <!-- Operator in. -->
  $\providecommand{\opin}{}$
  $\renewcommand{\opin}{\operatorname{in}}$
  <!-- Operator out. -->
  $\providecommand{\opout}{}$
  $\renewcommand{\opout}{\operatorname{out}}$
  <!-- Operator cell block. -->
  $\providecommand{\opblk}{}$
  $\renewcommand{\opblk}{\operatorname{block}}$
  <!-- Operator cell multiplicative forget gate. -->
  $\providecommand{\opfg}{}$
  $\renewcommand{\opfg}{\operatorname{fg}}$
  <!-- Operator cell multiplicative input gate. -->
  $\providecommand{\opig}{}$
  $\renewcommand{\opig}{\operatorname{ig}}$
  <!-- Operator cell multiplicative output gate. -->
  $\providecommand{\opog}{}$
  $\renewcommand{\opog}{\operatorname{og}}$
  <!-- Operator sequence. -->
  $\providecommand{\opseq}{}$
  $\renewcommand{\opseq}{\operatorname{seq}}$
  <!-- Operator loss. -->
  $\providecommand{\oploss}{}$
  $\renewcommand{\oploss}{\operatorname{loss}}$

  <!-- Net input. -->
  $\providecommand{\net}{}$
  $\renewcommand{\net}[2]{\opnet_{#1}(#2)}$
  <!-- Net input with activatiton f. -->
  $\providecommand{\fnet}{}$
  $\renewcommand{\fnet}[2]{f_{#1}\big(\net{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input. -->
  $\providecommand{\dfnet}{}$
  $\renewcommand{\dfnet}[2]{f_{#1}'\big(\net{#1}{#2}\big)}$

  <!-- Input dimension. -->
  $\providecommand{\din}{}$
  $\renewcommand{\din}{d_{\opin}}$
  <!-- Output dimension. -->
  $\providecommand{\dout}{}$
  $\renewcommand{\dout}{d_{\opout}}$
  <!-- Cell block dimension. -->
  $\providecommand{\dblk}{}$
  $\renewcommand{\dblk}{d_{\opblk}}$

  <!-- Number of cell blocks. -->
  $\providecommand{\nblk}{}$
  $\renewcommand{\nblk}{n_{\opblk}}$

  <!-- Cell block k. -->
  $\providecommand{\blk}{}$
  $\renewcommand{\blk}[1]{\opblk^{#1}}$

  <!-- Weight of multiplicative forget gate. -->
  $\providecommand{\wfg}{}$
  $\renewcommand{\wfg}{w^{\opfg}}$
  <!-- Weight of multiplicative input gate. -->
  $\providecommand{\wig}{}$
  $\renewcommand{\wig}{w^{\opig}}$
  <!-- Weight of multiplicative output gate. -->
  $\providecommand{\wog}{}$
  $\renewcommand{\wog}{w^{\opog}}$
  <!-- Weight of cell units. -->
  $\providecommand{\wblk}{}$
  $\renewcommand{\wblk}[1]{w^{\blk{#1}}}$
  <!-- Weight of output units. -->
  $\providecommand{\wout}{}$
  $\renewcommand{\wout}{w^{\opout}}$

  <!-- Net input of multiplicative forget gate. -->
  $\providecommand{\netfg}{}$
  $\renewcommand{\netfg}[2]{\opnet_{#1}^{\opfg}(#2)}$
  <!-- Net input of multiplicative forget gate with activatiton f. -->
  $\providecommand{\fnetfg}{}$
  $\renewcommand{\fnetfg}[2]{f_{#1}^{\opfg}\big(\netfg{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input of forget gate. -->
  $\providecommand{\dfnetfg}{}$
  $\renewcommand{\dfnetfg}[2]{f_{#1}^{\opfg}{'}\big(\netfg{#1}{#2}\big)}$
  <!-- Net input of multiplicative input gate. -->
  $\providecommand{\netig}{}$
  $\renewcommand{\netig}[2]{\opnet_{#1}^{\opig}(#2)}$
  <!-- Net input of multiplicative input gate with activatiton f. -->
  $\providecommand{\fnetig}{}$
  $\renewcommand{\fnetig}[2]{f_{#1}^{\opig}\big(\netig{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input of input gate. -->
  $\providecommand{\dfnetig}{}$
  $\renewcommand{\dfnetig}[2]{f_{#1}^{\opig}{'}\big(\netig{#1}{#2}\big)}$
  <!-- Net input of multiplicative output gate. -->
  $\providecommand{\netog}{}$
  $\renewcommand{\netog}[2]{\opnet_{#1}^{\opog}(#2)}$
  <!-- Net input of multiplicative output gate with activatiton f. -->
  $\providecommand{\fnetog}{}$
  $\renewcommand{\fnetog}[2]{f_{#1}^{\opog}\big(\netog{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input of output gate. -->
  $\providecommand{\dfnetog}{}$
  $\renewcommand{\dfnetog}[2]{f_{#1}^{\opog}{'}\big(\netog{#1}{#2}\big)}$
  <!-- Net input of output units. -->
  $\providecommand{\netout}{}$
  $\renewcommand{\netout}[2]{\opnet_{#1}^{\opout}(#2)}$
  <!-- Net input of output units with activatiton f. -->
  $\providecommand{\fnetout}{}$
  $\renewcommand{\fnetout}[2]{f_{#1}^{\opout}\big(\netout{#1}{#2}\big)}$
  <!-- Derivative of f with respect to net input of output units. -->
  $\providecommand{\dfnetout}{}$
  $\renewcommand{\dfnetout}[2]{f_{#1}^{\opout}{'}\big(\netout{#1}{#2}\big)}$

  <!-- Net input of cell unit. -->
  $\providecommand{\netblk}{}$
  $\renewcommand{\netblk}[3]{\opnet_{#1}^{\blk{#2}}(#3)}$
  <!-- Net input of cell unit with activatiton g. -->
  $\providecommand{\gnetblk}{}$
  $\renewcommand{\gnetblk}[3]{g_{#1}\big(\netblk{#1}{#2}{#3}\big)}$
  <!-- Derivative of g with respect to net input of cell unit. -->
  $\providecommand{\dgnetblk}{}$
  $\renewcommand{\dgnetblk}[3]{g_{#1}'\big(\netblk{#1}{#2}{#3}\big)}$
  <!-- Cell unit with activatiton h. -->
  $\providecommand{\hblk}{}$
  $\renewcommand{\hblk}[3]{h_{#1}\big(s_{#1}^{\blk{#2}}(#3)\big)}$
  <!-- Derivative of h with respect to cell unit. -->
  $\providecommand{\dhblk}{}$
  $\renewcommand{\dhblk}[3]{h_{#1}'\big(s_{#1}^{\blk{#2}}(#3)\big)}$

  <!-- Gradient approximation by truncating gradient. -->
  $\providecommand{\aptr}{}$
  $\renewcommand{\aptr}{\approx_{\operatorname{tr}}}$
</p>

<!-- End LaTeX command define section. -->

<ul>
  <li><a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">此篇論文</a>與<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 都寫錯自己的數學公式，但我的筆記內容主要以正確版本為主，原版 LSTM 可以參考<a href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html">我的筆記</a></li>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 沒有遺忘閘門，現今常用的 LSTM 都有遺忘閘門，概念由這篇論文提出</li>
  <li>包含多個子序列的<strong>連續輸入</strong>會讓 LSTM 的記憶單元內部狀態沒有上下界
    <ul>
      <li>現實中的大多數資料並不存在好的分割序列演算法，導致輸入給模型的資料通常都包含多個子序列</li>
      <li>根據實驗 1 的分析發現記憶單元內部狀態的累積導致預測結果完全錯誤</li>
    </ul>
  </li>
  <li>使用遺忘閘門讓模型學會適當的忘記已經處理過的子序列資訊
    <ul>
      <li>當遺忘閘門的<strong>偏差項</strong>初始化為<strong>正數</strong>時會保持記憶單元內部狀態，等同於使用原版的 LSTM</li>
      <li>因此使用遺忘閘門的 LSTM 能夠達成原版 LSTM 的功能，並額外擁有自動重設記憶單元的機制</li>
    </ul>
  </li>
  <li>這篇模型的理論背景較少，實驗為主的描述居多</li>
</ul>

<h2 id="lstm">原始 LSTM</h2>

<h3 id="section">模型架構</h3>

<p>根據<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始論文</a>提出的架構如下（這篇論文不使用額外的<strong>隱藏單元</strong>，因此我們也完全不列出隱藏單元相關的公式）（細節可以參考<a href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html">我的筆記</a>）</p>

<table>
  <thead>
    <tr>
      <th>符號</th>
      <th>意義</th>
      <th>備註</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\din$</td>
      <td><strong>輸入層</strong>的維度</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
    <tr>
      <td>$\dblk$</td>
      <td><strong>記憶單元區塊</strong>的維度</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
    <tr>
      <td>$\nblk$</td>
      <td><strong>記憶單元區塊</strong>的個數</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
    <tr>
      <td>$\dout$</td>
      <td><strong>輸出層</strong>的維度</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
    <tr>
      <td>$T$</td>
      <td>輸入序列的長度</td>
      <td>數值範圍為 $\Z^+$</td>
    </tr>
  </tbody>
</table>

<p>以下所有符號的時間 $t$ 範圍為 $t \in \set{0, \dots, T - 1}$</p>

<table>
  <thead>
    <tr>
      <th>符號</th>
      <th>意義</th>
      <th>維度</th>
      <th>備註</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$x(t)$</td>
      <td>第 $t$ 個時間點的<strong>輸入</strong></td>
      <td>$\din$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$y^{\opig}(t)$</td>
      <td>第 $t$ 個時間點的<strong>輸入閘門</strong></td>
      <td>$\nblk$</td>
      <td>$y^{\opig}(0) = 0$，同一個記憶單元區塊<strong>共享輸入閘門</strong></td>
    </tr>
    <tr>
      <td>$y^{\opog}(t)$</td>
      <td>第 $t$ 個時間點的<strong>輸出閘門</strong></td>
      <td>$\nblk$</td>
      <td>$y^{\opog}(0) = 0$，同一個記憶單元區塊<strong>共享輸出閘門</strong></td>
    </tr>
    <tr>
      <td>$s^{\blk{k}}(t)$</td>
      <td>第 $t$ 個時間點的第 $k$ 個<strong>記憶單元區塊內部狀態</strong></td>
      <td>$\dblk$</td>
      <td>$s^{\blk{k}}(0) = 0$ 且 $k \in \set{1, \dots, \nblk}$</td>
    </tr>
    <tr>
      <td>$y^{\blk{k}}(t)$</td>
      <td>第 $t$ 個時間點的第 $k$ 個<strong>記憶單元區塊輸出</strong></td>
      <td>$\dblk$</td>
      <td>$y^{\blk{k}}(0) = 0$ 且 $k \in \set{1, \dots, \nblk}$</td>
    </tr>
    <tr>
      <td>$y(t + 1)$</td>
      <td>第 $t + 1$ 個時間點的<strong>輸出</strong></td>
      <td>$\dout$</td>
      <td>由 $t$ 時間點的<strong>輸入</strong>與<strong>記憶單元輸出</strong>透過<strong>全連接</strong>產生，因此沒有 $y(0)$</td>
    </tr>
    <tr>
      <td>$\hat{y}(t + 1)$</td>
      <td>第 $t + 1$ 個時間點的<strong>預測目標</strong></td>
      <td>$\dout$</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>符號</th>
      <th>意義</th>
      <th>下標範圍</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$x_j(t)$</td>
      <td>第 $t$ 個時間點的第 $j$ 個<strong>輸入</strong></td>
      <td>$j \in \set{1, \dots, \din}$</td>
    </tr>
    <tr>
      <td>$y_k^{\opig}(t)$</td>
      <td>第 $t$ 個時間點第 $k$ 個記憶單元區塊的<strong>輸入閘門</strong></td>
      <td>$k \in \set{1, \dots, \nblk}$</td>
    </tr>
    <tr>
      <td>$y_k^{\opog}(t)$</td>
      <td>第 $t$ 個時間點第 $k$ 個記憶單元區塊的<strong>輸出閘門</strong></td>
      <td>$k \in \set{1, \dots, \nblk}$</td>
    </tr>
    <tr>
      <td>$s_i^{\blk{k}}(t)$</td>
      <td>第 $t$ 個時間點的第 $k$ 個<strong>記憶單元區塊</strong>的第 $i$ 個<strong>記憶單元內部狀態</strong></td>
      <td>$i \in \set{1, \dots, \dblk}$</td>
    </tr>
    <tr>
      <td>$y_i^{\blk{k}}(t)$</td>
      <td>第 $t$ 個時間點的第 $k$ 個<strong>記憶單元區塊</strong>的第 $i$ 個<strong>記憶單元輸出</strong></td>
      <td>$i \in \set{1, \dots, \dblk}$</td>
    </tr>
    <tr>
      <td>$y_i(t + 1)$</td>
      <td>第 $t + 1$ 個時間點的第 $i$ 個<strong>輸出</strong></td>
      <td>$i \in \set{1, \dots, \dout}$</td>
    </tr>
    <tr>
      <td>$\hat{y}_i(t + 1)$</td>
      <td>第 $t + 1$ 個時間點的第 $i$ 個<strong>預測目標</strong></td>
      <td>$i \in \set{1, \dots, \dout}$</td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th>參數</th>
      <th>意義</th>
      <th>輸出維度</th>
      <th>輸入維度</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\wig$</td>
      <td>產生<strong>輸入閘門</strong>的全連接參數</td>
      <td>$\nblk$</td>
      <td>$\din + \nblk \cdot (2 + \dblk)$</td>
    </tr>
    <tr>
      <td>$\wog$</td>
      <td>產生<strong>輸出閘門</strong>的全連接參數</td>
      <td>$\nblk$</td>
      <td>$\din + \nblk \cdot (2 + \dblk)$</td>
    </tr>
    <tr>
      <td>$\wblk{k}$</td>
      <td>產生第 $k$ 個<strong>記憶單元區塊淨輸入</strong>的全連接參數</td>
      <td>$\dblk$</td>
      <td>$\din + \nblk \cdot (2 + \dblk)$</td>
    </tr>
    <tr>
      <td>$\wout$</td>
      <td>產生<strong>輸出</strong>的全連接參數</td>
      <td>$\dblk$</td>
      <td>$\din + \nblk \cdot \dblk$</td>
    </tr>
  </tbody>
</table>

<p>定義 $\sigma$ 為 sigmoid 函數 $\sigma(x) = \frac{1}{1 + e^{-x}}$</p>

<table>
  <thead>
    <tr>
      <th>函數</th>
      <th>意義</th>
      <th>公式</th>
      <th>range</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$f_k^{\opig}$</td>
      <td>第 $k$ 個<strong>輸入閘門</strong>的啟發函數</td>
      <td>$\sigma$</td>
      <td>$[0, 1]$</td>
    </tr>
    <tr>
      <td>$f_k^{\opog}$</td>
      <td>第 $k$ 個<strong>輸出閘門</strong>的啟發函數</td>
      <td>$\sigma$</td>
      <td>$[0, 1]$</td>
    </tr>
    <tr>
      <td>$g_i^{\blk{k}}$</td>
      <td>第 $k$ 個<strong>記憶單元區塊</strong>中第 $i$ 個<strong>記憶單元內部狀態</strong>的啟發函數</td>
      <td>$4\sigma - 2$</td>
      <td>$[-2, 2]$</td>
    </tr>
    <tr>
      <td>$h_i^{\blk{k}}$</td>
      <td>第 $k$ 個<strong>記憶單元區塊</strong>中第 $i$ 個<strong>記憶單元輸出</strong>的啟發函數</td>
      <td>$2\sigma - 1$</td>
      <td>$[-1, 1]$</td>
    </tr>
    <tr>
      <td>$f_i^{\opout}$</td>
      <td>第 $i$ 個<strong>輸出</strong>的啟發函數</td>
      <td>$\sigma$</td>
      <td>$[0, 1]$</td>
    </tr>
  </tbody>
</table>

<p>在 $t$ 時間點時得到<strong>輸入</strong> $x(t)$，產生 $t + 1$ 時間點<strong>輸入閘門</strong> $y^{\opig}(t + 1)$ 與<strong>輸出閘門</strong> $y^{\opog}(t + 1)$ 的方法如下</p>

\[\begin{align*}
\tilde{x}(t) &amp; = \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix} \\
y^{\opig}(t + 1) &amp; = f^{\opig}\pa{\opnet^{\opig}(t + 1)} = f^{\opig}\pa{\wig \cdot \tilde{x}(t)} \\
y^{\opog}(t + 1) &amp; = f^{\opog}\pa{\opnet^{\opog}(t + 1)} = f^{\opog}\pa{\wog \cdot \tilde{x}(t)}
\end{align*} \tag{1}\label{1}\]

<p>利用 $\eqref{1}$ 產生 $t + 1$ 時間點的<strong>記憶單元內部狀態</strong> $s^{\blk{k}}(t + 1)$ 方法如下</p>

\[\begin{align*}
\tilde{x}(t) &amp; = \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix} \\
k &amp; \in \set{1, \dots, \nblk} \\
\opnet^{\blk{k}}(t + 1) &amp; = \wblk{k} \cdot \tilde{x}(t) \\
s^{\blk{k}}(t + 1) &amp; = s^{\blk{k}}(t) + y_k^{\opig}(t + 1) \cdot g(\opnet^{\blk{k}}(t + 1))
\end{align*} \tag{2}\label{2}\]

<p>注意第 $k$ 個記憶單元區塊內部狀態<strong>共享</strong>輸入閘門 $y_k^{\opig}(t + 1)$。</p>

<p>利用 $\eqref{1}\eqref{2}$ 產生 $t + 1$ 時間點的<strong>記憶單元輸出</strong> $y^{\blk{k}}(t + 1)$ 方法如下</p>

\[\begin{align*}
k &amp; \in \set{1, \dots, \nblk} \\
y^{\blk{k}}(t + 1) &amp; = y_k^{\opog}(t + 1) \cdot h\pa{s^{\blk{k}}(t + 1)}
\end{align*} \tag{3}\label{3}\]

<p>注意第 $k$ 個記憶單元區塊輸出<strong>共享輸出閘門</strong> $y_k^{\opog}(t + 1)$。</p>

<p>產生 $t + 1$ 時間點的<strong>輸出</strong>是透過 $t$ 時間點的<strong>輸入</strong>與 $t + 1$ 時間點的<strong>記憶單元輸出</strong>（見 $\eqref{3}$）而得</p>

\[\begin{align*}
\tilde{x}(t + 1) &amp; = \begin{pmatrix}
x(t) \\
y^{\blk{1}}(t + 1) \\
\vdots \\
y^{\blk{\nblk}}(t + 1)
\end{pmatrix} \\
y(t + 1) &amp; = f^{\opout}(\opnet^{\opout}(t + 1)) = f^{\opout}\pa{\wout \cdot \tilde{x}(t + 1)}
\end{align*} \tag{4}\label{4}\]

<p><a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">這篇論文</a>與<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM 的論文</a> 都不小心寫成 $t$ 時間點的記憶單元輸出，在 <a href="https://www.jmlr.org/papers/v3/gers02a.html">LSTM-2002</a> 才終於寫對。</p>

<h3 id="section-1">最佳化</h3>

<p><a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM</a> 提出與 truncated BPTT 相似的概念，透過 RTRL 進行參數更新，並故意<strong>丟棄流出記憶單元的所有梯度</strong>，避免梯度爆炸或梯度消失的問題，同時節省更新所需的空間與時間（local in time and space）。（細節可見<a href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html">我的筆記</a>）</p>

<p>令 $t \in \set{0, \dots, T - 1}$，最佳化的目標為每個時間點 $t + 1$ 所產生的<strong>平方誤差總和最小化</strong></p>

\[\begin{align*}
\oploss(t + 1) &amp; = \sum_{i = 1}^{\dout} \oploss_i(t + 1) \\
&amp; = \sum_{i = 1}^{\dout} \frac{1}{2} \big(y_i(t + 1) - \hat{y}_i(t + 1)\big)^2
\end{align*} \tag{5}\label{5}\]

<p>以下我們使用 $\aptr$ 代表<strong>丟棄部份梯度後的剩餘梯度</strong>。</p>

<p>輸出參數的剩餘梯度為</p>

\[\begin{align*}
\pd{\oploss(t + 1)}{\wout_{i, j}} &amp; = \pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \pd{\netout{i}{t + 1}}{\wout_{i, j}} \\
&amp; = \big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\blk{1}}(t + 1) \\
\vdots \\
y^{\blk{\nblk}}(t + 1)
\end{pmatrix}_j
\end{align*} \tag{6}\label{6}\]

<p>其中 $1 \leq i \leq \dout$ 且 $1 \leq j \leq \din + \nblk \cdot \dblk$。</p>

<p>輸出閘門參數的剩餘梯度為</p>

\[\begin{align*}
&amp; \pd{\oploss(t + 1)}{\wog_{k, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \\
&amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{y_k^{\opog}(t + 1)}} \cdot \pd{y_k^{\opog}(t + 1)}{\netog{k}{t + 1}} \cdot \pd{\netog{k}{t + 1}}{\wog_{k, q}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \\
&amp; \quad \pa{\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot \hblk{j}{k}{t + 1}} \cdot \dfnetog{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q\Bigg]
\end{align*} \tag{7}\label{7}\]

<p>其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (2 + \dblk)$。</p>

<p>輸入閘門參數的剩餘梯度為</p>

\[\begin{align*}
&amp; \pd{\oploss(t + 1)}{\wig_{k, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \\
&amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{s_j^{\blk{k}}(t + 1)} \cdot \pd{s_j^{\blk{k}}(t + 1)}{\wig_{k, q}}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{s_j^{\blk{k}}(t + 1)} \cdot \\
&amp; \quad \quad \br{\pd{s_j^{\blk{k}}(t)}{\wig_{k, q}} + \gnetblk{j}{k}{t + 1} \cdot \pd{y_k^{\opig}(t + 1)}{\netig{k}{t + 1}} \cdot \pd{\netig{k}{t + 1}}{\wig_{k, q}}}\Bigg)\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \\
&amp; \quad \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp; \quad \quad \br{\pd{s_j^{\blk{k}}(t)}{\wig_{k, q}} + \gnetblk{j}{k}{t + 1} \cdot \dfnetig{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg)\Bigg]
\end{align*} \tag{8}\label{8}\]

<p>其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (2 + \dblk)$。</p>

<p>記憶單元淨輸入參數的剩餘梯度為</p>

\[\begin{align*}
&amp; \pd{\oploss(t + 1)}{\wblk{k}_{p, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \br{\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \pd{\netout{i}{t + 1}}{y_p^{\blk{k}}(t + 1)} \cdot \pd{y_p^{\blk{k}}(t + 1)}{s_p^{\blk{k}}(t + 1)} \cdot \pd{s_p^{\blk{k}}(t + 1)}{\wblk{k}_{p, q}}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \pd{\netout{i}{t + 1}}{y_p^{\blk{k}}(t + 1)} \cdot \pd{y_p^{\blk{k}}(t + 1)}{s_p^{\blk{k}}(t + 1)} \cdot \\
&amp; \quad \quad \pa{\pd{s_p^{\blk{k}}(t)}{\wblk{k}_{p, q}} + y_k^{\opig}(t + 1) \cdot \pd{\gnetblk{j}{k}{t + 1}}{\netblk{j}{k}{t + 1}} \cdot \pd{\netblk{j}{k}{t + 1}}{\wblk{k}_{p, q}}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot \\
&amp; \quad y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp; \quad \br{\pd{s_p^{\blk{k}}(t)}{\wblk{k}_{p, q}} + y_k^{\opig}(t + 1) \cdot \dgnetblk{p}{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg]
\end{align*} \tag{9}\label{9}\]

<p>其中 $1 \leq k \leq \nblk$， $1 \leq p \leq \dblk$ 且 $1 \leq q \leq \din + \nblk \cdot (2 + \dblk)$。</p>

<p>計算完上述所有參數後使用<strong>梯度下降</strong>（gradient descent）進行參數更新</p>

\[\begin{align*}
\wout_{i, j} &amp; \leftarrow \wout_{i, j} - \alpha \cdot \pd{\oploss(t + 1)}{\wout_{i, j}} \\
\wog_{k, q} &amp; \leftarrow \wog_{k, q} - \alpha \cdot \pd{\oploss(t + 1)}{\wog_{k, q}} \\
\wig_{k, q} &amp; \leftarrow \wig_{k, q} - \alpha \cdot \pd{\oploss(t + 1)}{\wig_{k, q}} \\
\wblk{k}_{p, q} &amp; \leftarrow \wblk{k}_{p, q} - \alpha \cdot \pd{\oploss(t + 1)}{\wblk{k}_{p, q}}
\end{align*} \tag{10}\label{10}\]

<p>其中 $\alpha$ 為<strong>學習率</strong>（<strong>learning rate</strong>）。</p>

<p>由於使用基於 RTRL 的最佳化演算法，因此每個時間點 $t + 1$ 計算完誤差後就可以更新參數。</p>

<h3 id="section-2">問題</h3>

<p>當一個輸入序列中包含多個獨立的子序列（例如一個文章段落有多個句子），則模型無法知道不同獨立子序列的起始點在哪裡（除非有明確的切斷序列演算法，但實際上不一定存在）。</p>

<p><a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM</a> 架構假設任意輸入序列都是由單一獨立序列組成，不會包含多個獨立的序列，因此會在每次序列<strong>輸入時重設模型的計算狀態</strong> $y^{\opig}(0), y^{\opog}(0), s^{\blk{k}}(0), y^{\blk{k}}(0)$，沒有<strong>需要在計算過程中重設計算狀態的需求</strong>。</p>

<p>但當輸入包含多個獨立的子序列時，且沒有明確的方法辨識不同獨立子序列的起始點時，LSTM 模型就必須要擁有能夠在任意時間點 $t$ <strong>重設計算狀態</strong> $y^{\opig}(t), y^{\opog}(t), s^{\blk{k}}(t), y^{\blk{k}}(t)$ 的功能。</p>

<h2 id="section-3">遺忘閘門</h2>

<h3 id="section-4">模型架構</h3>

<p><a name="paper-fig-1"></a></p>

<p>圖 1：在原始 LSTM 架構上增加遺忘閘門。
圖片來源：<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">論文</a>。</p>

<p><img src="https://i.imgur.com/ILRsaEU.png" alt="圖 1" /></p>

<p>作者提出在模型中加入<strong>遺忘閘門</strong>（<strong>forget gate</strong>），概念是讓<strong>記憶單元內部狀態</strong>能夠進行重設。</p>

<p>首先需要計算<strong>遺忘閘門</strong> $y^{\opfg}(t)$，定義如下</p>

\[\begin{align*}
\tilde{x}(t) &amp; = \begin{pmatrix}
x(t) \\
y^{\opfg}(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix} \\
y^{\opfg}(0) &amp; = 0 \\
y^{\opfg}(t + 1) &amp; = f^{\opfg}\pa{\opnet^{\opfg}(t + 1) = f^{\opfg}\pa{\wfg \cdot \tilde{x}(t)}}
\end{align*} \tag{11}\label{11}\]

<p>計算方法與輸入閘門和輸出閘門相同。</p>

<p>而計算過程需要做以下修改</p>

<ul>
  <li>$\eqref{1}\eqref{2}$ 中的淨輸入需要加上 $y^{\opfg}(t)$</li>
  <li>參數 $\wig, \wog, \wblk{k}$ 的輸入維度都改成 $\din + \nblk \cdot (3 + \dblk)$</li>
  <li>$\wfg$ 的維度與 $\wig$ 完全相同</li>
  <li>$f^{\opfg}$ 與 $f^{\opig}$ 的定義完全相同</li>
</ul>

<p>所謂的遺忘並不是直接設定成 $0$，而是以乘法閘門的形式進行數值重設，因此 $\eqref{2}$ 的計算改成</p>

\[s^{\blk{k}}(t + 1) = y_k^{\opfg}(t + 1) \cdot s^{\blk{k}}(t) + y_k^{\opig}(t + 1) \cdot g(\opnet^{\blk{k}}(t + 1)) \tag{12}\label{12}\]

<h3 id="section-5">偏差項</h3>

<p>如同<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM</a>，<strong>輸入閘門</strong>與<strong>輸出閘門</strong>可以使用<strong>偏差項</strong>（bias term），將偏差項初始化成<strong>負數</strong>可以讓輸入閘門與輸出閘門在需要的時候才被啟用（細節可以看<a href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html">我的筆記</a>）。</p>

<p>而<strong>遺忘閘門</strong>也可以使用偏差項，但初始化的數值應該為<strong>正數</strong>，理由是在模型計算前期應該要讓遺忘閘門開啟（$y^{\opfg} \approx 1$），讓記憶單元內部狀態的數值能夠進行改變。</p>

<p>注意遺忘閘門只有在<strong>關閉</strong>（$y^{\opfg} \approx 0$）時才能進行遺忘，這個名字取得不是很好。</p>

<h3 id="section-6">最佳化</h3>

<p>基於<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM</a> 的最佳化演算法，將流出遺忘閘門的梯度也一起<strong>丟棄</strong></p>

\[\begin{align*}
\pd{\netfg{k}{t + 1}}{y_{k^{\star}}^{\opfg}(t)} &amp; \aptr 0 &amp;&amp; k = 1, \dots, \nblk \\
\pd{\netfg{k}{t + 1}}{y_{k^{\star}}^{\opig}(t)} &amp; \aptr 0 &amp;&amp; k^{\star} = 1, \dots, \nblk \\
\pd{\netfg{k}{t + 1}}{y_{k^{\star}}^{\opog}(t)} &amp; \aptr 0 \\
\pd{\netfg{k}{t + 1}}{y_i^{\blk{k^{\star}}}(t)} &amp; \aptr 0 &amp;&amp; i = 1, \dots, \dblk
\end{align*} \tag{13}\label{13}\]

<p>因此<strong>遺忘閘門的參數</strong>剩餘梯度為</p>

\[\begin{align*}
&amp; \pd{\oploss(t + 1)}{\wfg_{k, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \\
&amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{s_j^{\blk{k}}(t + 1)} \cdot \pd{s_j^{\blk{k}}(t + 1)}{\wfg_{k, q}}}\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{s_j^{\blk{k}}(t + 1)} \cdot \\
&amp; \quad \quad \br{y_k^{\opfg}(t + 1) \cdot \pd{s_j^{\blk{k}}(t)}{\wfg_{k, q}} + s_j^{\blk{k}}(t) \cdot \pd{y_k^{\opfg}(t + 1)}{\netfg{k}{t + 1}} \cdot \pd{\netfg{k}{t + 1}}{\wfg_{k, q}}}\Bigg)\Bigg] \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \\
&amp; \quad \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp; \quad \quad \br{y_k^{\opfg}(t + 1) \cdot \pd{s_j^{\blk{k}}(t)}{\wfg_{k, q}}  + s_j^{\blk{k}}(t) \cdot \dfnetog{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg)\Bigg]
\end{align*} \tag{14}\label{14}\]

<p>$\eqref{14}$ 式就是論文的 3.12 式，其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。</p>

<p>由於 $\eqref{12}$ 的修改，$\eqref{9} \eqref{10}$ 最佳化的過程也需要跟著修改。</p>

<p>輸入閘門的參數剩餘梯度改為</p>

\[\begin{align*}
&amp; \pd{\oploss(t + 1)}{\wig_{k, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \\
&amp; \quad \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp; \quad \quad \br{y_k^{\opfg}(t + 1) \cdot \pd{s_j^{\blk{k}}(t)}{\wig_{k, q}} + \gnetblk{j}{k}{t + 1} \cdot \dfnetig{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg)\Bigg]
\end{align*} \tag{15}\label{15}\]

<p>$\eqref{14}$ 式就是論文的 3.11 式，其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。</p>

<p>記憶單元淨輸入參數的剩餘梯度改為</p>

\[\begin{align*}
&amp; \pd{\oploss(t + 1)}{\wblk{k}_{p, q}} \\
&amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot \\
&amp; \quad y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp; \quad \br{y_k^{\opfg}(t + 1) \cdot \pd{s_p^{\blk{k}}(t)}{\wblk{k}_{p, q}} + y_k^{\opig}(t + 1) \cdot \dgnetblk{p}{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg]
\end{align*} \tag{16}\label{16}\]

<p>$\eqref{14}$ 式就是論文的 3.10 式，其中 $1 \leq k \leq \nblk$， $1 \leq p \leq \dblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。</p>

<p><strong>注意錯誤</strong>：根據論文中的 3.4 式，論文 2.5 式的 $t - 1$ 應該改成 $t$。</p>

<p>根據 $\eqref{14}\eqref{15}\eqref{16}$，當遺忘閘門 $y_k^{\opfg}(t + 1) \approx 0$ （關閉）時，不只記憶單元 $s^{\blk{k}}(t + 1)$ 會重設，與其相關的梯度也會重設，因此更新時需要額外紀錄以下的項次</p>

\[\pd{s_i^{\blk{k}}(t + 1)}{\wfg_{k, q}}, \pd{s_i^{\blk{k}}(t + 1)}{\wig_{k, q}}, \pd{s_i^{\blk{k}}(t + 1)}{\wblk{k}_{p, q}}\]

<p>同樣的概念在<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM</a> 中也有出現，細節可以看<a href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html">我的筆記</a>。</p>

<h2 id="continual-embedded-reber-grammar">實驗 1：Continual Embedded Reber Grammar</h2>

<p><a name="paper-fig-2"></a></p>

<p>圖 2：Continual Embedded Reber Grammar。
圖片來源：<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">論文</a>。</p>

<p><img src="https://i.imgur.com/rhHtVRN.png" alt="圖 2" /></p>

<h3 id="section-7">任務定義</h3>

<ul>
  <li>根據<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM 論文</a>中的實驗 1（Embedded Reber Grammar）進行修改，輸入為連續序列，連續序列的定義是由多個 Embedded Reber Grammar 產生的序列組合而成（細節可以看<a href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html">我的筆記</a>）</li>
  <li>每個分支的生成機率值為 $0.5$</li>
  <li>當所有輸出單元的平方誤差低於 $0.49$ 時就當成預測正確</li>
  <li>在一次的訓練過程中，給予模型的輸入只會在以下兩種狀況之一發生時停止
    <ul>
      <li>當模型產生一次的預測錯誤</li>
      <li>模型連續接收 $10^6$ 個輸入</li>
    </ul>
  </li>
  <li>每次訓練停止就進行一次測試
    <ul>
      <li>一次測試會執行 $10$ 次的連續輸入</li>
      <li>評估結果是 $10$ 次連續輸入的平均值</li>
    </ul>
  </li>
  <li>每輸入一個訊號就進行更新（RTRL）</li>
  <li>訓練最多執行 $30000$ 次，實驗結果由 $100$ 個訓練模型實驗進行平均</li>
</ul>

<h3 id="lstm-">LSTM 架構</h3>

<p><a name="paper-fig-3"></a></p>

<p>圖 3：LSTM 架構。
圖片來源：<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">論文</a>。</p>

<p><img src="https://i.imgur.com/uUJjmSz.png" alt="圖 3" /></p>

<table>
  <thead>
    <tr>
      <th>參數</th>
      <th>數值（或範圍）</th>
      <th>備註</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>$\din$</td>
      <td>$7$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\nblk$</td>
      <td>$4$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\dblk$</td>
      <td>$2$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\dout$</td>
      <td>$7$</td>
      <td> </td>
    </tr>
    <tr>
      <td>$\dim(\wblk{k})$</td>
      <td>$\dblk \times [\din + \nblk \cdot \dblk]$</td>
      <td>訊號來源為外部輸入與記憶單元</td>
    </tr>
    <tr>
      <td>$\dim(\wfg)$</td>
      <td>$\nblk \times [\din + \nblk \cdot \dblk + 1]$</td>
      <td>訊號來源為外部輸入與記憶單元，有額外使用偏差項</td>
    </tr>
    <tr>
      <td>$\dim(\wig)$</td>
      <td>$\nblk \times [\din + \nblk \cdot \dblk + 1]$</td>
      <td>訊號來源為外部輸入與記憶單元，有額外使用偏差項</td>
    </tr>
    <tr>
      <td>$\dim(\wog)$</td>
      <td>$\nblk \times [\din + \nblk \cdot \dblk + 1]$</td>
      <td>訊號來源為外部輸入與記憶單元，有額外使用偏差項</td>
    </tr>
    <tr>
      <td>$\dim(\wout)$</td>
      <td>$\dout \times [\din + \nblk \cdot \dblk + 1]$</td>
      <td>訊號來源為外部輸入與記憶單元，有額外使用偏差項</td>
    </tr>
    <tr>
      <td>總參數量</td>
      <td>$424$</td>
      <td> </td>
    </tr>
    <tr>
      <td>參數初始化</td>
      <td>$[-0.2, 0.2]$</td>
      <td>平均分佈</td>
    </tr>
    <tr>
      <td>輸入閘門偏差項初始化</td>
      <td>$\set{-0.5, -1.0, -1.5, -2.0}$</td>
      <td>依序初始化成不同數值</td>
    </tr>
    <tr>
      <td>輸出閘門偏差項初始化</td>
      <td>$\set{-0.5, -1.0, -1.5, -2.0}$</td>
      <td>依序初始化成不同數值</td>
    </tr>
    <tr>
      <td>遺忘閘門偏差項初始化</td>
      <td>$\set{0.5, 1.0, 1.5, 2.0}$</td>
      <td>依序初始化成不同數值</td>
    </tr>
    <tr>
      <td>Learning rate $\alpha$</td>
      <td>$0.5$</td>
      <td>訓練過程可以固定 $\alpha$，或是以 $0.99$ 的 decay factor 在每次更新後進行衰減</td>
    </tr>
  </tbody>
</table>

<h3 id="section-8">實驗結果</h3>

<p><a name="paper-fig-4"></a></p>

<p>圖 4：Continual Embedded Reber Grammar 實驗結果。
圖片來源：<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">論文</a>。</p>

<p><img src="https://i.imgur.com/uu9Nccj.png" alt="圖 4" /></p>

<ul>
  <li><a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM</a> 在有手動進行計算狀態的重置時表現非常好，但當沒有手動重置時完全無法執行任務
    <ul>
      <li>就算讓記憶單元內部狀態進行 decay 也無濟於事</li>
    </ul>
  </li>
  <li>使用遺忘閘門的 LSTM 不需要手動重置計算狀態也能達成完美預測
    <ul>
      <li>完美預測指的是連續 $10^6$ 輸入都預測正確</li>
    </ul>
  </li>
  <li>有嘗試使用 $\alpha / t$ 或 $\alpha / \sqrt{T}$ 作為 learning rate，實驗發現不論是哪種最佳化的方法使用遺忘閘門的 LSTM 都表現的不錯
    <ul>
      <li>在其他模型架構上（包含原版 LSTM）就算使用這些最佳化演算法也無法解決任務</li>
    </ul>
  </li>
  <li>額外實驗在將 Embedded Reber Grammar 開頭的 <code class="language-plaintext highlighter-rouge">B</code> 與結尾的 <code class="language-plaintext highlighter-rouge">E</code> 去除的狀態下，使用遺忘閘門的 LSTM 仍然表現不錯</li>
</ul>

<h3 id="section-9">分析</h3>

<p><a name="paper-fig-5"></a></p>

<p>圖 5：<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 記憶單元內部狀態的累加值。
圖片來源：<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">論文</a>。</p>

<p><img src="https://i.imgur.com/qwU4pnG.png" alt="圖 5" /></p>

<p><a name="paper-fig-6"></a></p>

<p>圖 6：LSTM 加上遺忘閘門後第三個記憶單元內部狀態。
圖片來源：<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">論文</a>。</p>

<p><img src="https://i.imgur.com/jtLnfu2.png" alt="圖 6" /></p>

<p><a name="paper-fig-7"></a></p>

<p>圖 7：LSTM 加上遺忘閘門後第一個記憶單元內部狀態。
圖片來源：<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">論文</a>。</p>

<p><img src="https://i.imgur.com/K1mp9rg.png" alt="圖 7" /></p>

<ul>
  <li>觀察<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原版 LSTM</a> 的記憶單元內部狀態，可以發現在不進行手動重設的狀態下，記憶單元內部狀態的數值只會不斷的累加（朝向極正或極負前進）</li>
  <li>觀察架上遺忘閘門後 LSTM 的記憶單元內部狀態，可以發現模型學會自動重設
    <ul>
      <li>在第三個記憶單元中展現了長期記憶重設的能力</li>
      <li>在第一個記憶單元中展現了短期記憶重設的能力</li>
    </ul>
  </li>
</ul>

<h2 id="noisy-temporal-order-problem">實驗 2：Noisy Temporal Order Problem</h2>

<h3 id="section-10">任務定義</h3>

<ul>
  <li>就是<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM 論文</a>中的實驗 6b，細節可以看<a href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html">我的筆記</a></li>
  <li>由於此任務需要讓記憶維持一段不短的時間，因此遺忘資訊對於這個任務可能有害，透過這個任務想要驗證是否有任務是只能使用原版 LSTM 可以解決但增加遺忘閘門後不能解決</li>
</ul>

<h3 id="lstm--1">LSTM 架構</h3>

<p>與實驗 1 大致相同，只做以下修改</p>

<ul>
  <li>$\din = \dout = 8$</li>
  <li>將遺忘閘門的偏差項初始化成較大的正數（論文使用 $5$），讓遺忘閘門很難被關閉，藉此達到跟原本 LSTM 幾乎相同的計算能力</li>
</ul>

<h3 id="section-11">實驗結果</h3>

<ul>
  <li>使用遺忘閘門的 LSTM 仍然能夠解決 Noisy Temporal Order Problem
    <ul>
      <li>當偏差項初始化成較大的正數（例如 $5$）時，收斂速度與原版 LSTM 一樣快</li>
      <li>當偏差項初始化成較小的正數（例如 $1$）時，收斂速度約為原版 LSTM 的 $3$ 倍</li>
    </ul>
  </li>
  <li>因此根據實驗沒有什麼任務是原版 LSTM 可以解決但加上遺忘閘門後不能解決的</li>
</ul>

<h2 id="continual-noisy-temporal-order-problem">實驗 3：Continual Noisy Temporal Order Problem</h2>

<h3 id="section-12">任務定義</h3>

<ul>
  <li>根據<a href="https://ieeexplore.ieee.org/abstract/document/6795963">原始 LSTM 論文</a>中的實驗 6b 進行修改，輸入為連續序列，連續序列的定義是由 $100$ 筆 Noisy Temporal Order 序列所組成</li>
  <li>在一次的訓練過程中，給予模型的輸入只會在以下兩種狀況之一發生時停止
    <ul>
      <li>當模型產生一次的預測錯誤</li>
      <li>模型連續接收 $100$ 個 Noisy Temporal Order 序列</li>
    </ul>
  </li>
  <li>每次訓練停止就進行一次測試
    <ul>
      <li>一次測試會執行 $10$ 次的連續輸入</li>
      <li>評估結果是 $10$ 次連續輸入中預測正確的序列個數平均值</li>
    </ul>
  </li>
  <li>論文沒有講怎麼計算誤差與更新，我猜變成每個非預測時間點必須輸出 $0$，預測時間點時輸出預測結果</li>
  <li>訓練最多執行 $10^5$ 次，實驗結果由 $100$ 個訓練模型實驗進行平均</li>
</ul>

<h3 id="lstm--2">LSTM 架構</h3>

<p>與實驗 2 相同。</p>

<h3 id="section-13">實驗結果</h3>

<p><a name="paper-fig-8"></a></p>

<p>圖 8：Continual Noisy Temporal Order Problem 實驗結果。
圖片來源：<a href="https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM">論文</a>。</p>

<p><img src="https://i.imgur.com/VV5wQVG.png" alt="圖 8" /></p>

<ul>
  <li><a href="#paper-fig-8">圖 8</a> 中的註解 a 應該寫錯了，應該改為 correct classification of 100 successive NTO sequences</li>
  <li>實驗再次驗證原版 LSTM 無法解決連續輸入，但使用輸入閘門後就能夠解決問題</li>
  <li>將 learning rate 使用 decay factor $0.9$ 逐漸下降可以讓模型表現變更好，但作者認為這不重要</li>
</ul>



<!-- Avoid copy by China, method 1. -->
<script>
  console.log(`
台.灣.獨.立.香.港.獨.立.西.藏.獨.立.新.疆.獨.立.內.蒙.古.獨.立.......
..... ... ................:fffffLt.............一.九.八.九.六.四
... ,i111i;............. ;L111111LL....習.包.子.近.平.小.熊.維.尼
...iLfttttff,  ..,::;;;;;Cftt11111C:...........中.國.武.漢.肺.炎
..;C1111111tf:ittfttttttttttfffft1L,..........................
. ff11111111fCtt11111111111111ttfC1 ..........................
. tf11111111ft11111111111111111111tt;. .......................
..:L11111111t11111111111111111111111fti, .....................
.. ;Lt11111111111111111111111111111111tCt:, ..................
... ,tf111111111111111111tLLLt111111111LG00t..................
.... 1f11111111111111111LGLtt1111111111111tLt.................
....:f11111111111111111tf1111111111111111111t.................
... 1f111111111111111111111111111111tt11111t,.................
... tt111111111111111111111111111111L11LG11f:.................
... tt111111111111111111111tGC111111Lt108111tti:..............
... if111111111111111111111f8C111111ttttttfft1ttt,............
....,f111111111111111111111111111111111fG0088C11tf............
.....tt111111111111111111111111111111110@888@0111f; ..........
.....,f111111111111111111111111111111111C000CL111Li ..........
......;f11111111111111111111tt11111111111111ft111L,...........
...... tt111111111111111111tfGf111111111111ff111L; ...........
.....;fCt11111111111111111111f0Cft1111111fLt111Li ............
.... fCCLftt111111111111111111fGGGCLLfLfff1111t; .............
.....:CLLCCCCLLftt1111111111111tfLCGGG0L1111fL;...............
..... LLLLLLLLLCCCLLftt1111111111ttffttt1ttCCLGf..............
.... iCLLLLLLLLLLLLLCCCLLftt11111111tfffft1fGCC; .............
... ;CLLLLLLLLLLLLCLLLLLLLCCLLLLLLffft1111tLGLL:,.. ..........
...:GLLLLLLLLLLLLLCCCCCLLLLLLLLLCCLLLLLLLLCCLGGLLf1i:,. ......
.. iCLLLLLLLLLLLLLLLLLCCCCLLLLLLLLLLLLLLLLLLCLCCLLLCCCti. ....
...iCLLLLLLLLLLLLLLLLLLLLCCCCLLLLLLLLLLLLLLLLLLCCLLLGCCCLi,...
.. iCLLLLLLLLLLLLLLLLLLLLLLCCGCCCLLLLLLLLLLLLLLLCLLCGLCCCGCti,
. :LLLLLLLLLLLLLLLLLLLLLLCLt111tffLLCCCCCLLLLLLLGCLGGCLftLCCCL
.;CLLLLLLLLLLLLLLLLLLLLLCf11111111111tttffLLLLLLLLfLft111CCCCG
;GLLLLLLLLLLCCLLLLLLLLLCt1111111111111111111111111111111111ttt
.;CLLLLLLLLLLCCCLLLLLLCf11111111111111111111111111111111111111
`)
</script>

<!-- Avoid copy by China, method 2. -->
<!--
台獨教父 Xi Jinping
⣿⣿⣿⣿⣿⠟⠋⠄⠄⠄⠄⠄⠄⠄⢁⠈⢻⢿⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⠃⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠈⡀⠭⢿⣿⣿⣿⣿
⣿⣿⣿⣿⡟⠄⢀⣾⣿⣿⣿⣷⣶⣿⣷⣶⣶⡆⠄⠄⠄⣿⣿⣿⣿
⣿⣿⣿⣿⡇⢀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣧⠄⠄⢸⣿⣿⣿⣿
⣿⣿⣿⣿⣇⣼⣿⣿⠿⠶⠙⣿⡟⠡⣴⣿⣽⣿⣧⠄⢸⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣾⣿⣿⣟⣭⣾⣿⣷⣶⣶⣴⣶⣿⣿⢄⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣿⡟⣩⣿⣿⣿⡏⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣹⡋⠘⠷⣦⣀⣠⡶⠁⠈⠁⠄⣿⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣍⠃⣴⣶⡔⠒⠄⣠⢀⠄⠄⠄⡨⣿⣿⣿⣿⣿⣿
⣿⣿⣿⣿⣿⣿⣿⣦⡘⠿⣷⣿⠿⠟⠃⠄⠄⣠⡇⠈⠻⣿⣿⣿⣿
⣿⣿⣿⣿⡿⠟⠋⢁⣷⣠⠄⠄⠄⠄⣀⣠⣾⡟⠄⠄⠄⠄⠉⠙⠻
⡿⠟⠋⠁⠄⠄⠄⢸⣿⣿⡯⢓⣴⣾⣿⣿⡟⠄⠄⠄⠄⠄⠄⠄⠄
⠄⠄⠄⠄⠄⠄⠄⣿⡟⣷⠄⠹⣿⣿⣿⡿⠁⠄⠄⠄⠄⠄⠄⠄⠄
⠄⠄⠄⠄⠄⠄⣸⣿⡷⡇⠄⣴⣾⣿⣿⠃⠄⠄⠄⠄⠄⠄⠄⠄⠄
⠄⠄⠄⠄⠄⠄⣿⣿⠃⣦⣄⣿⣿⣿⠇⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄
⠄⠄⠄⠄⠄⢸⣿⠗⢈⡶⣷⣿⣿⡏⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄⠄
动态网自由门 天安門 天安门 法輪功 李洪志 Free Tibet
六四天安門事件 The Tiananmen Square protests of 1989
天安門大屠殺 The Tiananmen Square Massacre
反右派鬥爭 The Anti-Rightist Struggle
大躍進政策 The Great Leap Forward
文化大革命 The Great Proletarian Cultural Revolution
人權 Human Rights
民運 Democratization
自由 Freedom
獨立 Independence
多黨制 Multi-party system
台灣 臺灣 Taiwan Formosa
西藏 土伯特 唐古特 Tibet
達賴喇嘛 Dalai Lama
法輪功 Falun Dafa
新疆維吾爾自治區 The Xinjiang Uyghur Autonomous Region
諾貝爾和平獎 Nobel Peace Prize
劉暁波 Liu Xiaobo
民主 言論 思想 反共 反革命 抗議 運動 騷亂 暴亂 騷擾 擾亂 抗暴 平反 維權 示威游行 李洪志
法輪大法 大法弟子 強制斷種 強制堕胎 民族淨化 人體實驗 肅清 胡耀邦 趙紫陽 魏京生 王丹
還政於民 和平演變 激流中國 北京之春 大紀元時報 九評論共産黨 獨裁 專制 壓制 統一 監視 鎮壓
迫害 侵略 掠奪 破壞 拷問 屠殺 活摘器官 誘拐 買賣人口
Winnie the Pooh
-->

    </div>

</article>
<div class="post-nav"><a class="previous" href="/optimization/2021/12/07/learning-representations-by-backpropagating-errors.html" title="Learning representations by back-propagating errors">Learning representations by back-propagating errors</a><a class="next" href="/text%20modeling/2021/12/21/finding-structure-in-time.html" title="Finding Structure in Time">Finding Structure in Time</a></div><div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li><a class="post-link" href="/dataset/2022/08/06/a-standard-corpus-of-edited-present-day-american-english.html" title="Finding Structure in Time">A Standard Corpus of Edited Present-Day American English</a></li><li><a class="post-link" href="/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html" title="Finding Structure in Time">Long Short-Term Memory</a></li><li><a class="post-link" href="/acoustic%20modeling/2022/03/01/long-short-term-memory-recurrent-neural-network-architectures-for-large-scale-acoustic-modeling.html" title="Finding Structure in Time">Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling...</a></li><li><a class="post-link" href="/optimization/2021/12/07/learning-representations-by-backpropagating-errors.html" title="Finding Structure in Time">Learning representations by back-propagating errors</a></li></ul>
    </div><div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><!-- Copy right part has some bugs, so I replace it with my own footer.
  Code source: https://github.com/jeffreytse/jekyll-theme-yat/blob/0fea688977e16c1f1f42c23b36b14ed325ee606b/_includes/views/footer.html
-->
<footer class="site-footer h-card">

  <div class="wrapper">
    <div class="site-footer-inner">
      <div>Copyright (c) 2021-<span id='current-year'>2021</span>
        <a href='https://github.com/ProFatXuanAll'>ProFatXuanAll</a>
      </div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="http://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div>Opinions expressed are solely my own and do not express the views or opinions of my university or my lab.
      </div>
    </div>
  </div>
  <!-- Calculate full year at runtime. -->
  <script>
    document.getElementById('current-year').innerHTML = (new Date(Date.now())).getFullYear()
  </script>
</footer></body>
</html>
