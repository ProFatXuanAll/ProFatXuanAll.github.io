<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-TW"><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" hreflang="zh-TW" /><updated>2022-12-19T15:21:39+08:00</updated><id>/feed.xml</id><title type="html">ML Notes</title><subtitle>ML notes all written by {{site.author}}.</subtitle><author><name>ProFatXuanAll</name></author><entry><title type="html">Generating Sequences with Recurrent Neural Networks</title><link href="/text%20modeling/2022/08/29/generating-sequences-with-recurrent-neural-networks.html" rel="alternate" type="text/html" title="Generating Sequences with Recurrent Neural Networks" /><published>2022-08-29T15:30:00+08:00</published><updated>2022-08-29T15:30:00+08:00</updated><id>/text%20modeling/2022/08/29/generating-sequences-with-recurrent-neural-networks</id><content type="html" xml:base="/text%20modeling/2022/08/29/generating-sequences-with-recurrent-neural-networks.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;使用 LSTM 進行 sequence generation&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;Alex Graves&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隸屬單位&lt;/td&gt;
      &lt;td&gt;University of Toronto&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;arXiv&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;2013&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;https://arxiv.org/abs/1308.0850&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--
  Define LaTeX command which will be used through out the writing.

  Each command must be wrapped with $ signs.
  We use &quot;display: none;&quot; to avoid redudant whitespaces.
 --&gt;

&lt;p style=&quot;display: none;&quot;&gt;

  &lt;!-- c but bold. --&gt;
  $\providecommand{\cb}{}$
  $\renewcommand{\cb}{\mathbf{c}}$
  &lt;!-- h but bold. --&gt;
  $\providecommand{\hb}{}$
  $\renewcommand{\hb}{\mathbf{h}}$
  &lt;!-- x but bold. --&gt;
  $\providecommand{\xb}{}$
  $\renewcommand{\xb}{\mathbf{x}}$
  &lt;!-- y but bold. --&gt;
  $\providecommand{\yb}{}$
  $\renewcommand{\yb}{\mathbf{y}}$

  &lt;!-- H but curly. --&gt;
  $\providecommand{\Hc}{}$
  $\renewcommand{\Hc}{\mathcal{H}}$
  &lt;!-- L but curly. --&gt;
  $\providecommand{\Lc}{}$
  $\renewcommand{\Lc}{\mathcal{L}}$
  &lt;!-- N but curly. --&gt;
  $\providecommand{\Nc}{}$
  $\renewcommand{\Nc}{\mathcal{N}}$
  &lt;!-- Y but curly. --&gt;
  $\providecommand{\Yc}{}$
  $\renewcommand{\Yc}{\mathcal{Y}}$

  &lt;!-- alpha with hat. --&gt;
  $\providecommand{\alphah}{}$
  $\renewcommand{\alphah}{\hat{\alpha}}$
  &lt;!-- beta with hat. --&gt;
  $\providecommand{\betah}{}$
  $\renewcommand{\betah}{\hat{\beta}}$
  &lt;!-- e with hat. --&gt;
  $\providecommand{\eh}{}$
  $\renewcommand{\eh}{\hat{e}}$
  &lt;!-- gamma with hat. --&gt;
  $\providecommand{\gammah}{}$
  $\renewcommand{\gammah}{\hat{\gamma}}$
  &lt;!-- kappa with hat. --&gt;
  $\providecommand{\kappah}{}$
  $\renewcommand{\kappah}{\hat{\kappa}}$
  &lt;!-- mu with hat. --&gt;
  $\providecommand{\muh}{}$
  $\renewcommand{\muh}{\hat{\mu}}$
  &lt;!-- pi with hat. --&gt;
  $\providecommand{\pih}{}$
  $\renewcommand{\pih}{\hat{\pi}}$
  &lt;!-- rho with hat. --&gt;
  $\providecommand{\rhoh}{}$
  $\renewcommand{\rhoh}{\hat{\rho}}$
  &lt;!-- sigma with hat. --&gt;
  $\providecommand{\sigmah}{}$
  $\renewcommand{\sigmah}{\hat{\sigma}}$
  &lt;!-- y with hat. --&gt;
  $\providecommand{\yh}{}$
  $\renewcommand{\yh}{\hat{y}}$

&lt;/p&gt;

&lt;!-- End LaTeX command define section. --&gt;

&lt;h2 id=&quot;section&quot;&gt;重點&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;採用全微分更新 LSTM 時要記得作 gradient clipping&lt;/li&gt;
  &lt;li&gt;LSTM + residual connection 才能讓多層模型正常訓練&lt;/li&gt;
  &lt;li&gt;作者使用單詞中的平均字數估算 BPC 與 perplexity 的轉換公式
    &lt;ul&gt;
      &lt;li&gt;早年的 BPC 與 perplexity 需要對不同的 n-gram 進行計算，由於神經網路可以直接模擬完整序列因此不需要呈現不同 n 對應的數據&lt;/li&gt;
      &lt;li&gt;不論是 BPC 還是 perplexity 都是以 $2$ 為底，Pytorch 與 Tensorflow 中的 entropy 都是以自然對數為底，計算數值要小心&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;透過 Hutter prize 實驗證實 LSTM 擁有處理長距離資訊的能力
    &lt;ul&gt;
      &lt;li&gt;能夠生成對稱的 XML tags&lt;/li&gt;
      &lt;li&gt;能夠生成對稱的標點符號&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LSTM + Gaussian mixture models 可以模擬實數序列，例如生成手寫字跡
    &lt;ul&gt;
      &lt;li&gt;同時提出 seq2seq 架構的雛型&lt;/li&gt;
      &lt;li&gt;同時提出控制生成風格的方法雛型&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;模型&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;整體模型架構&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 1：整體模型架構。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Sv7n7wk.png&quot; alt=&quot;圖 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;令 $(x_1, \dots, x_T, x_{T+1})$ 為一向量序列：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\xb = (x_1, \dots, x_T)$ 會作為模型的輸入，得到輸出向量序列 $\yb = (y_1, \dots, y_T)$&lt;/li&gt;
  &lt;li&gt;$(x_2, \dots, x_{T+1})$ 會作為模型的預測目標，對任意的 $t \in \set{1, \dots, T}$，模型在輸入 $x_t$ 之後必須要讓輸出 $x_{t+1}$ 的機率值最大化，即最大化 $\Pr(x_{t + 1} \vert y_t)$&lt;/li&gt;
  &lt;li&gt;令 $x_1$ 代表 begin-of-sequence token，作者將 $x_1$ 所有數值設成 $0$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;令 $N$ 為序列模型中的 RNN 層數，令第 $n$ 層（$n \in \set{1, \dots, N}$）的輸出結果為 $\hb^n = (h_1^n, \dots, h_T^n)$：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;將 RNN 模型以 $\Hc$ 表示&lt;/li&gt;
  &lt;li&gt;論文中每一層 RNN 模型都是採用 LSTM 模型（見&lt;a href=&quot;#paper-fig-2&quot;&gt;圖 2&lt;/a&gt;），但當前討論先假設是廣義的 RNN&lt;/li&gt;
  &lt;li&gt;根據&lt;a href=&quot;#paper-fig-1&quot;&gt;圖 1&lt;/a&gt;，每層模型都有 skip connections 將輸入 $x_t$ 與 $h_t^n$ 相連，作者說這是為了避免梯度消失（gradient vanishing）問題&lt;/li&gt;
  &lt;li&gt;根據&lt;a href=&quot;#paper-fig-1&quot;&gt;圖 1&lt;/a&gt;，每個時間點不同 RNN 隱藏層的輸出會進行轉換並疊加，計算結果作為最後的預測的來源&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;正式的數學描述如下：&lt;/p&gt;

\[\begin{align*}
&amp;amp; \algoProc{SeqModel}(\xb, h_0^1, \dots, h_0^N) \\
&amp;amp; \hspace{1em} \algoFor{t \in \set{1, \dots, T}} \\
&amp;amp; \hspace{2em} h_t^1 \algoEq \Hc(W_{i h^1} x_t + W_{h^1 h^1} h_{t-1}^1 + b_h^1) &amp;amp;&amp;amp; \tag{1}\label{1} \\
&amp;amp; \hspace{2em} \algoFor{n \in \set{2, \dots, N}} \\
&amp;amp; \hspace{3em} h_t^n \algoEq \Hc(W_{i h^n} x_t + W_{h^{n-1} h^n} h_t^{n-1} + W_{h^n h^n} h_{t-1}^n + b_h^n) &amp;amp;&amp;amp; \tag{2}\label{2} \\
&amp;amp; \hspace{2em} \algoEndFor \\
&amp;amp; \hspace{2em} \yh_t \algoEq b_y + \sum_{n = 1}^N W_{h^n y} h_t^n &amp;amp;&amp;amp; \tag{3}\label{3} \\
&amp;amp; \hspace{2em} y_t \algoEq \Yc(\yh_t) &amp;amp;&amp;amp; \tag{4}\label{4} \\
&amp;amp; \hspace{1em} \algoEndFor \\
&amp;amp; \hspace{1em} \Pr(\xb) \algoEq \prod_{t = 1}^T \Pr(x_{t+1} \vert y_t) &amp;amp;&amp;amp; \tag{5}\label{5} \\
&amp;amp; \hspace{1em} \Lc(\xb) \algoEq -\sum_{t = 1}^T \log \Pr(x_{t+1} \vert y_t) &amp;amp;&amp;amp; \tag{6}\label{6} \\
&amp;amp; \hspace{1em} \algoReturn \Pr(\xb), \Lc(\xb) \\
&amp;amp; \algoEndProc
\end{align*}\]

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;節點&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{i h^1}$&lt;/td&gt;
      &lt;td&gt;連接輸入與第 $1$ 層 RNN&lt;/td&gt;
      &lt;td&gt;$\xb$&lt;/td&gt;
      &lt;td&gt;輸入向量序列&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{h^n h^n}$&lt;/td&gt;
      &lt;td&gt;第 $n$ 層 RNN 的自連接層&lt;/td&gt;
      &lt;td&gt;$h_0^n$&lt;/td&gt;
      &lt;td&gt;第 $n$ 層 RNN 的起始狀態&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{h^{n-1} h^n}$&lt;/td&gt;
      &lt;td&gt;連接第 $n-1$ 層 RNN 與第 $n$ 層 RNN&lt;/td&gt;
      &lt;td&gt;$h_t^n$&lt;/td&gt;
      &lt;td&gt;第 $n$ 層 RNN 第 $t$ 時間點的狀態&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{h^n y}$&lt;/td&gt;
      &lt;td&gt;連接第 $N$ 層 RNN 的狀態與輸出層&lt;/td&gt;
      &lt;td&gt;$\yh_t$&lt;/td&gt;
      &lt;td&gt;所有 RNN 隱藏第 $t$ 時間點的狀態疊加結果&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_h^n$&lt;/td&gt;
      &lt;td&gt;第 $n$ 層 RNN 的輸入 bias&lt;/td&gt;
      &lt;td&gt;$y_t$&lt;/td&gt;
      &lt;td&gt;$t$ 時間點的輸出&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\Yc$&lt;/td&gt;
      &lt;td&gt;輸出層&lt;/td&gt;
      &lt;td&gt;$\Pr(x_{t+1} \vert y_t)$&lt;/td&gt;
      &lt;td&gt;$y_t$ 預測答案 $x_{t+1}$ 的機率值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;$\Lc(\xb)$&lt;/td&gt;
      &lt;td&gt;Negative log-likelihood of $\xb$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;lstm-&quot;&gt;LSTM 模型架構&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 2：LSTM 模型架構。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。
&lt;img src=&quot;https://i.imgur.com/quKKMOh.png&quot; alt=&quot;圖 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;將&lt;a href=&quot;#paper-fig-1&quot;&gt;圖 1&lt;/a&gt; 中的 $\Hc$ 定義成 &lt;a href=&quot;LSTM2002&quot;&gt;LSTM-2002&lt;/a&gt;，正式定義如下&lt;/p&gt;

\[\begin{align*}
&amp;amp; \algoProc{LSTM}(x_t, h_{t-1}, c_{t-1}) \\
&amp;amp; \hspace{1em} i_t \algoEq \sigma(W_{x i} x_t + W_{h i} h_{t-1} + W_{c i} \odot c_{t - 1} + b_i) &amp;amp;&amp;amp; \tag{7}\label{7} \\
&amp;amp; \hspace{1em} f_t \algoEq \sigma(W_{x f} x_t + W_{h f} h_{t-1} + W_{c f} \odot c_{t - 1} + b_f) &amp;amp;&amp;amp; \tag{8}\label{8} \\
&amp;amp; \hspace{1em} c_t \algoEq f_t \odot c_{t-1} + i_t \odot \tanh(W_{x c} x_t + W_{h c} h_{t-1} + b_c) &amp;amp;&amp;amp; \tag{9}\label{9} \\
&amp;amp; \hspace{1em} o_t \algoEq \sigma(W_{x o} x_t + W_{h o} h_{t-1} + W_{c o} c_t + b_o) &amp;amp;&amp;amp; \tag{10}\label{10} \\
&amp;amp; \hspace{1em} h_t \algoEq o_t \odot \tanh(c_t) &amp;amp;&amp;amp; \tag{11}\label{11} \\
&amp;amp; \hspace{1em} \algoReturn h_t, c_t \\
&amp;amp; \algoEndProc
\end{align*}\]

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;節點&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{x g}$&lt;/td&gt;
      &lt;td&gt;連接輸入與 gate $g \in \set{i, f, o}$&lt;/td&gt;
      &lt;td&gt;$x_t$&lt;/td&gt;
      &lt;td&gt;$t$ 時間點的輸入向量&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{h g}$&lt;/td&gt;
      &lt;td&gt;連接 LSTM hidden states 與 gate $g \in \set{i, f, o}$&lt;/td&gt;
      &lt;td&gt;$h_t$&lt;/td&gt;
      &lt;td&gt;$t$ 時間點的 LSTM hidden states&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{c g}$&lt;/td&gt;
      &lt;td&gt;連接 LSTM internal states 與 gate $g \in \set{i, f, o}$&lt;/td&gt;
      &lt;td&gt;$c_t$&lt;/td&gt;
      &lt;td&gt;$t$ 時間點的 LSTM internal states&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_g$&lt;/td&gt;
      &lt;td&gt;bias of gate $g \in \set{i, f, o}$&lt;/td&gt;
      &lt;td&gt;$i_t$&lt;/td&gt;
      &lt;td&gt;$t$ 時間點的 input gate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{x c}$&lt;/td&gt;
      &lt;td&gt;連接輸入與 memory cell&lt;/td&gt;
      &lt;td&gt;$f_t$&lt;/td&gt;
      &lt;td&gt;$t$ 時間點的 forget gate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{h c}$&lt;/td&gt;
      &lt;td&gt;連接 LSTM hidden states 與 memory cell&lt;/td&gt;
      &lt;td&gt;$o_t$&lt;/td&gt;
      &lt;td&gt;$t$ 時間點的 output gate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_c$&lt;/td&gt;
      &lt;td&gt;memory cell bias&lt;/td&gt;
      &lt;td&gt;$c_t$&lt;/td&gt;
      &lt;td&gt;$t$ 時間點的 LSTM internal states&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在替換 $\eqref{1} \eqref{2}$ 中的 RNN $\mathcal{H}$ 為上述定義的 LSTM 後，仍然採用&lt;a href=&quot;#paper-fig-1&quot;&gt;圖 1&lt;/a&gt; 中的計算方法，可以想成把 LSTM 的輸入 $x_t$ 換成 $W_{i h^n} x_t + W_{h^{n-1} h^n} h_t^{n-1}$。&lt;/p&gt;

&lt;h3 id=&quot;lstm--1&quot;&gt;LSTM 最佳化&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版的 LSTM&lt;/a&gt; 使用 truncated RTRL 的變種計算梯度，而在此篇論文中作者採用 BPTT 全微分作為更新梯度。
然而原始的梯度計算方法是為了避免梯度爆炸（gradient explosion）問題，因此作者提出手動將超過數值範圍的梯度設為符合事先定義的範圍（gradient clipping）。
作者說他之前發表的研究其實都有作 gradient clipping，但他忘記講了，好壞。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;預測文字&lt;/h2&gt;

&lt;p&gt;定義 $\eqref{4}$ 的計算方法為&lt;/p&gt;

\[\forall k \in \set{1, \dots, K}, \Pr(x_{t+1} = k \vert y_t) = y_t^k = \dfrac{\exp(\yh_t^k)}{\sum_{k' = 1}^K \exp(\yh_t^{k'})}. \tag{12}\label{12}\]

&lt;p&gt;其中 $K$ 代表類別數量，在預測文字的任務中代表字典大小。
而輸入 $x_t \in \R^K$ 則對應到 one-hot encoding。
將 $\eqref{12}$ 代入 $\eqref{6}$ 可以推得&lt;/p&gt;

\[\Lc(\xb) = -\sum_{t = 1}^T \log y_t^{x_{t + 1}}. \tag{13}\label{13}\]

&lt;p&gt;而 $\Lc(\xb)$ 相對於 $\yh_t^k$ 的梯度為&lt;/p&gt;

\[\begin{align*}
\pd{\Lc(\xb)}{\yh_t^k} &amp;amp; = \pd{\Lc(\xb)}{y_t^{x_{t+1}}} \pd{y_t^{x_{t+1}}}{\yh_t^k} \\
&amp;amp; = -\pd{\log y_t^{x_{t+1}}}{y_t^{x_{t+1}}} \pd{y_t^{x_{t+1}}}{\yh_t^k} \\
&amp;amp; = \dfrac{-1}{y_t^{x_{t+1}}} \pd{y_t^{x_{t+1}}}{\yh_t^k} \\
&amp;amp; = \dfrac{-1}{y_t^{x_{t+1}}} \pa{\dfrac{\exp(\yh_t^{x_{t+1}})}{\sum_{k' = 1}^K \exp(\yh_t^{k'})} \delta_{k, x_{t+1}} - \dfrac{\exp(y_t^{x_{t+1}}) \exp(\yh_t^k)}{\pa{\sum_{k' = 1}^K \exp(\yh_t^k)}^2}} \\
&amp;amp; = \dfrac{-1}{y_t^{x_{t+1}}} \pa{y_t^{x_{t+1}} \delta_{k, x_{t+1}} - y_t^{x_{t+1}} y_t^k} \\
&amp;amp; = y_t^k - \delta_{k, x_{t + 1}}. &amp;amp;&amp;amp; \tag{14}\label{14}
\end{align*}\]

&lt;p&gt;當 $K$ 很大時（常見的單詞數量 $&amp;gt; 100000$），計算 softmax 成本就會很高，因此不少論文提出減少 $K$ 的方法，其中 character-level language model 就是能夠有效減少 $K$ 的手段。&lt;/p&gt;

&lt;h3 id=&quot;penn-treebank---wall-street-journal&quot;&gt;實驗 1：Penn Treebank - Wall Street Journal&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 3：PTB-WSJ 實驗結果。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。
&lt;img src=&quot;https://i.imgur.com/n9115bj.png&quot; alt=&quot;圖 3&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Penn Treebank 中的 Wall Street Journal（WSJ）用來作為 language modeling benchmark
    &lt;ul&gt;
      &lt;li&gt;Training set：930000 words&lt;/li&gt;
      &lt;li&gt;Validation set：74000 words&lt;/li&gt;
      &lt;li&gt;Test set：82000 words&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;評估方法
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;BPC（bit-per-character）：average value of $-\log_2 \Pr(x_{t+1} \vert y_t)$ over the whole test set, i.e.,&lt;/p&gt;

\[\dfrac{-1}{\hash\text{characters in dataset}} \sum_{\xb \text{ in dataset}} \log_2 \Pr(x_{t+1} \vert y_t)\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Perplexity：two to the power of the average number of bits per word, i.e.,&lt;/p&gt;

\[\dfrac{-1}{\hash\text{words in dataset}} \sum_{\xb \text{ in dataset}} \log_2 \Pr(x_{t+1} \vert y_t)\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;當 token 設定成 word 時，BPC 無法直接計算，作者認為由於 WSJ 中的所有 word 平均擁有 5.6 個 characters，因此套用以下公式進行近似計算：&lt;/p&gt;

\[\operatorname{perplexity} \approx 2^{5.6 \operatorname{BPC}}\]
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;當 token 設定成 character 時，word 機率值為組成的 characters 的機率值連乘積，因此 perplexity 仍然可以正常計算&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;每個文字序列都有插入 end-of-sentence token，注意是 sentence 不是 sequence，代表單一文字序列中可能有多個 end-of-sentence token
    &lt;ul&gt;
      &lt;li&gt;End-of-sentence token 會參與誤差計算&lt;/li&gt;
      &lt;li&gt;由於 begin-of-sequence token 已經定義，因此不需要定義 start-of-sentence token，可以由 end-of-sentence token 替代 start-of-sentence token&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型架構
    &lt;ul&gt;
      &lt;li&gt;只使用 $1$ 層 LSTM（$N = 1$），隱藏層單元數為 $1000$（$h_t^1 \in \R^{1000}$）&lt;/li&gt;
      &lt;li&gt;作者將模型字典大小設定為 $10000$ tokens，不在字典中的所有字都定義成 unknown token&lt;/li&gt;
      &lt;li&gt;當 token 設定成 character 時
        &lt;ul&gt;
          &lt;li&gt;字典大小為 $49$&lt;/li&gt;
          &lt;li&gt;資料集中幾乎沒有任何的 unknown token&lt;/li&gt;
          &lt;li&gt;參數總數約為 $4.3$ M&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 token 設定成 word 時
        &lt;ul&gt;
          &lt;li&gt;字典大小為 $10000$&lt;/li&gt;
          &lt;li&gt;資料集中會包含不少的 unknown token&lt;/li&gt;
          &lt;li&gt;參數總數約為 $54$ M&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;最佳化演算法為 SGD（stochastic gradient descent）
    &lt;ul&gt;
      &lt;li&gt;Learning rate 設為 $0.0001$&lt;/li&gt;
      &lt;li&gt;Momentum 設為 $0.99$&lt;/li&gt;
      &lt;li&gt;Gradient clipping 的範圍為 $[-1, 1]$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;由於模型容易 overfitting，因此作者加入 2 種不同的 regularization 技巧
    &lt;ul&gt;
      &lt;li&gt;Weight noise with std set to $0.075$&lt;/li&gt;
      &lt;li&gt;Adaptive weight noise + minimum description length&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;參數初始化策略
    &lt;ul&gt;
      &lt;li&gt;使用 weight noise 的模型參數初始化為不使用 weight noise 訓練的模型參數&lt;/li&gt;
      &lt;li&gt;使用 adaptive weight noise 的模型參數初始化為使用 weight noise 訓練的模型參數&lt;/li&gt;
      &lt;li&gt;作者發現使用訓練過的模型作為初始化的參數收斂速度快於從頭訓練 + regularization&lt;/li&gt;
      &lt;li&gt;Word-level language model 使用 adaptive weight noise 訓練太慢，因此只使用 fix weight noise&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;結論
    &lt;ul&gt;
      &lt;li&gt;Word-level 表現比 character-level 好，但使用 regularization 可讓兩者差異減少&lt;/li&gt;
      &lt;li&gt;當 test set 也被用來更新模型（只更新 1 次，稱為 dynamic evaluation），模型表現更好&lt;/li&gt;
      &lt;li&gt;作者與 Tomas Mikolov 的實驗結果比較，發現 LSTM + dynamic evaluation 表現比 RNN + dynamic evaluation 更好&lt;/li&gt;
      &lt;li&gt;此階段的 NN 模型比 N-gram 統計模型表現還要差（perplexity 最低可到 $89.4$）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hutter-prize&quot;&gt;實驗 2：Hutter prize&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 4：Hutter prize 實驗結果。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。
&lt;img src=&quot;https://i.imgur.com/ZK80PEG.png&quot; alt=&quot;圖 4&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First $100$ million bytes of English Wikipedia dumped at March 3rd 2006
    &lt;ul&gt;
      &lt;li&gt;包含 XML、數字、中文等非英文文字&lt;/li&gt;
      &lt;li&gt;Hutter prize 是在比賽壓縮演算法，作者用 LSTM 來測試 BPC&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;作者自己定義 train-valid split
    &lt;ul&gt;
      &lt;li&gt;Training set：the first $96$ M bytes&lt;/li&gt;
      &lt;li&gt;Validation set：the remaining $4$ M bytes&lt;/li&gt;
      &lt;li&gt;切成以 $100$ bytes 為單位的 sequence 進行計算&lt;/li&gt;
      &lt;li&gt;以 $1$ byte 為單位來計算共有 $205$ 個不同的 unicode symbols&lt;/li&gt;
      &lt;li&gt;以 character 為單位來計算遠超過 $205$ 個不同的 unicode characters&lt;/li&gt;
      &lt;li&gt;作者決定以 $1$ byte 作為字典基礎單位，所以字典大小為 $205$&lt;/li&gt;
      &lt;li&gt;由於以 $1$ byte 作為單位，因此計算 BPC 時每個 character 的機率是由組成的 bytes 的機率值的連乘積估算而得&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;由於資料中有 XML 結構的存在，模型需要能夠記憶長距離資訊才能讓 bit per byte 下降
    &lt;ul&gt;
      &lt;li&gt;特別寫出結構與長距離問題其實是作者想要強調 LSTM 擁有該能力&lt;/li&gt;
      &lt;li&gt;每隔 $100$ 個 sequences（共 $10000$ bytes）才手動重設一次 LSTM 的狀態（$h_t$ 與 $c_t$）&lt;/li&gt;
      &lt;li&gt;每次的模型更新都只限制在 $1$ 個 sequence（共 $100$ bytes）內，以此加速訓練&lt;/li&gt;
      &lt;li&gt;由於想要摹擬長距離的能力，因此資料集不進行隨機抽樣&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型架構
    &lt;ul&gt;
      &lt;li&gt;使用 $7$ 層 LSTM（$N = 7$），隱藏層單元數為 $700$（$h_t^1 \in \R^{700}$）&lt;/li&gt;
      &lt;li&gt;定義 token 為 $1$ byte，參數總量約為 $21.3$ M&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;最佳化演算法為 SGD
    &lt;ul&gt;
      &lt;li&gt;Learning rate 設為 $0.0001$&lt;/li&gt;
      &lt;li&gt;Momentum 設為 $0.9$&lt;/li&gt;
      &lt;li&gt;Gradient clipping 的範圍為 $[-1, 1]$&lt;/li&gt;
      &lt;li&gt;需要訓練 $4$ 個 epoch 才會收斂&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;結論
    &lt;ul&gt;
      &lt;li&gt;使用 dynamic evaluation 在 validation set 上表現比較好
        &lt;ul&gt;
          &lt;li&gt;與 PTB-WSJ 實驗結果一致&lt;/li&gt;
          &lt;li&gt;作者認為在 validation set 上的分佈與 training set 不同，因此 dynamic evaluation 表現較好是理所當然&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;使用 dynamic evaluation 在 validation set 上表現比 training set 好
        &lt;ul&gt;
          &lt;li&gt;作者認為可能是模型在 training set 上仍然 underfitting 所導致&lt;/li&gt;
          &lt;li&gt;作者認為可能部份資料比較難 fitting（例如 plain text），而 validation set 都拿到剛好比較容易 fitting 的資料（例如 XML tag）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;此階段的 NN 模型比 N-gram 統計模型表現還要差，但 LSTM 在 NN 模型中表現最好
        &lt;ul&gt;
          &lt;li&gt;N-gram 模型 BPC 最低可到 $1.28$&lt;/li&gt;
          &lt;li&gt;zip 壓縮演算法 BPC 會高於 $2$&lt;/li&gt;
          &lt;li&gt;Character-level RNN，且只處理 plain text 不包含 XML，BPC 最低可到 $1.47$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;將 LSTM 模型用於生成文字內容，作者觀察發現 LSTM 有學到文字中包含的結構
        &lt;ul&gt;
          &lt;li&gt;學到常見單詞與 sub-words，並且能夠任意組合 sub-words&lt;/li&gt;
          &lt;li&gt;學到標點符號的用法&lt;/li&gt;
          &lt;li&gt;學到對稱的 quotation marks 與 parentheses 證明模型擁有記憶長距離資訊的能力&lt;/li&gt;
          &lt;li&gt;進一步學會對稱生成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;==&lt;/code&gt; 符號（wiki 用來標記 header 的符號）、XML tags 與縮排結構（indentation）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;雖然生成的有模有樣，但作者認為從 phrase 結構來看生成的結果是無法作為正常文字使用的
        &lt;ul&gt;
          &lt;li&gt;作者認為更好的模擬方法可以從加大與加深模型下手（與 2019–2022 的趨勢相同）&lt;/li&gt;
          &lt;li&gt;作者認為增加訓練資料也是一個強化模型的方法（再度與 2019–2022 的趨勢相同）&lt;/li&gt;
          &lt;li&gt;作者認為不論讓模型多有效的模擬文字生成，最後都無法模擬人類（想法不如 GPT 等研究樂觀）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;手寫文字預測&lt;/h2&gt;

&lt;p&gt;模型將會接收到連續的數值，代表筆尖（pen-tip）的座標隨著時間移動的軌跡&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;作者把這類型的連續數值稱為 online handwriting data&lt;/li&gt;
  &lt;li&gt;相對於 online，offline 的意思是直接獲得最終的手寫圖片進行辨識&lt;/li&gt;
  &lt;li&gt;相較於 offline，online 的優勢為輸入數字每個時間點都只有 2 個數值，因此數值結構比起完整的圖片來說維度較低&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;沿用 $\eqref{1}–\eqref{11}$ 定義的 LSTM 模型，定義每個時間點的模型輸入如下&lt;/p&gt;

\[x_t = (x_{t, 1}, x_{t, 2}, x_{t, 3}) \tag{15}\label{15}\]

&lt;ul&gt;
  &lt;li&gt;$(x_{t, 1}, x_{t, 2}) \in \R^2$ 代表平面座標&lt;/li&gt;
  &lt;li&gt;$x_{t, 3} \in \set{0, 1}$ 代表筆尖是否離開智慧白板（end-of-stroke）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;模型預測目標共有兩個，分別為下個時間點的筆跡座標值與筆尖是否離開智慧白板。
作者將 LSTM 模型結合 mixture density network 使用，主要概念為將 LSTM 模型的輸出作為 Gaussian mixture models（GMM）的參數（parameters）使用，因此定義每個時間點的輸出如下&lt;/p&gt;

\[y_t = \pa{e_t, \set{\pi_t^j, \mu_t^j, \sigma_t^j, \rho_t^j}_{j = 1}^M} \tag{16}\label{16}\]

&lt;ul&gt;
  &lt;li&gt;由於預測目標共有兩個，因此使用兩個不同的機率分佈進行模擬&lt;/li&gt;
  &lt;li&gt;$e_t \in (0, 1)$ 代表 end-of-stroke 的機率值，因此採用 Bernoulli distribution 進行模擬&lt;/li&gt;
  &lt;li&gt;作者假設下個時間點的筆尖座標值符合 bivariate Gaussian distribution，因此使用 GMM 進行模擬&lt;/li&gt;
  &lt;li&gt;$M$ 代表 bivariate Gaussian models 的數量&lt;/li&gt;
  &lt;li&gt;$\mu_t^j \in \R^2$ 代表第 $j$ 個 bivariate Gaussian model 的平均值&lt;/li&gt;
  &lt;li&gt;$\sigma_t^j \in \R^2$ 代表第 $j$ 個 bivariate Gaussian model 的標準差&lt;/li&gt;
  &lt;li&gt;$\rho_t^j \in \R$ 代表第 $j$ 個 bivariate Gaussian model 的相關係數&lt;/li&gt;
  &lt;li&gt;$\pi_t^j \in \R$ 代表第 $j$ 個 bivariate Gaussian model 的組合權重&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根據 $\eqref{16}$，作者將 $\eqref{3}$ 的定義改為&lt;/p&gt;

\[\yh_t = \pa{\eh_t, \set{\pih_t^j, \muh_t^j, \sigmah_t^j, \rhoh_t^j}_{j = 1}^M} = b_y + \Sum_{n = 1}^N W_{h^n y} h_t^n \tag{17}\label{17}\]

&lt;p&gt;接著定義 $\eqref{16}$ 的計算方法：&lt;/p&gt;

\[\begin{align*}
e_t        &amp;amp; = \dfrac{1}{1 + \exp(\eh_t)}                                                              &amp;amp;&amp;amp; \implies e_t \in (0, 1)                                 &amp;amp;&amp;amp; \tag{18}\label{18} \\
\pi_t^j    &amp;amp; = \softmax\pa{\pih_t^j} = \dfrac{\exp\pa{\pih_t^j}}{\Sum_{j' = 1}^M \exp\pa{\pih_t^{j'}}} &amp;amp;&amp;amp; \implies \pi_t^j \in (0, 1), \Sum_{j = 1}^M \pi_t^j = 1 &amp;amp;&amp;amp; \tag{19}\label{19} \\
\mu_t^j    &amp;amp; = \muh_t^j                                                                                &amp;amp;&amp;amp; \implies \muh_t^j \in \R^2                              &amp;amp;&amp;amp; \tag{20}\label{20} \\
\sigma_t^j &amp;amp; = \exp\pa{\sigmah_t^j}                                                                    &amp;amp;&amp;amp; \implies \sigma_t^j \in \R_+^2                          &amp;amp;&amp;amp; \tag{21}\label{21} \\
\rho_t^j   &amp;amp; = \tanh\pa{\rhoh_t^j}                                                                     &amp;amp;&amp;amp; \implies \rho_t^j \in (-1, 1)                           &amp;amp;&amp;amp; \tag{22}\label{22}
\end{align*}\]

&lt;p&gt;注意 $\eqref{18}$ 與 sigmoid function 類似但不是 sigmoid function。
接著定義 GMM 的計算方法：&lt;/p&gt;

\[\Pr(x_{t+1} \vert y_t) = \begin{dcases}
  \Sum_{j = 1}^M \pi_t^j \cdot \Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j} \cdot e_t       &amp;amp; \text{if } x_{t+1, 3} = 1 \\
  \Sum_{j = 1}^M \pi_t^j \cdot \Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j} \cdot (1 - e_t) &amp;amp; \text{otherwise}
\end{dcases} \tag{23}\label{23}\]

&lt;p&gt;其中 $\Nc$ 代表 bivariate Gaussian distribution，計算方法為&lt;/p&gt;

\[\begin{align*}
&amp;amp; \Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j} = \dfrac{1}{2 \pi \sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}} \cdot \exp\pa{\dfrac{-Z}{2 (1 - (\rho_t^j)^2)}} &amp;amp;&amp;amp; \tag{24}\label{24} \\
&amp;amp; Z = \pa{\dfrac{x_{t+1, 1} - \mu_{t, 1}^j}{\sigma_{t, 1}^j}}^2 + \pa{\dfrac{x_{t+1, 2} - \mu_{t, 2}^j}{\sigma_{t, 2}^j}}^2 - \dfrac{2 \rho_t^j \pa{x_{t+1, 1} - \mu_{t, 1}^j} \pa{x_{t+1, 2} - \mu_{t, 2}^j}}{\sigma_{t, 1}^j \sigma_{t, 2}^j} &amp;amp;&amp;amp; \tag{25}\label{25}
\end{align*}\]

&lt;p&gt;注意 $\eqref{23}$ 中的 $e_t$ 與 $\Sum_{j = 1}^M$ 無關，因此 $\eqref{6}$ 可以改寫為&lt;/p&gt;

\[\begin{align*}
\Lc(\xb) &amp;amp; = \Sum_{t = 1}^T -\log \Pr(x_{t+1} \vert y_t) \\
&amp;amp; = \begin{dcases}
  \Sum_{t = 1}^T -\log \pa{\Sum_{j = 1}^M \pi_t^j \cdot \Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j} \cdot e_t}       &amp;amp; \text{if } x_{t+1, 3} = 1 \\
  \Sum_{t = 1}^T -\log \pa{\Sum_{j = 1}^M \pi_t^j \cdot \Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j} \cdot (1 - e_t)} &amp;amp; \text{otherwise}
\end{dcases} \\
&amp;amp; = \begin{dcases}
  \Sum_{t = 1}^T \pa{-\log \pa{\Sum_{j = 1}^M \pi_t^j \cdot \Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j}} - \log e_t}       &amp;amp; \text{if } x_{t+1, 3} = 1 \\
  \Sum_{t = 1}^T \pa{-\log \pa{\Sum_{j = 1}^M \pi_t^j \cdot \Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j}} - \log (1 - e_t)} &amp;amp; \text{otherwise}
\end{dcases} &amp;amp;&amp;amp; \tag{26}\label{26}
\end{align*}\]

&lt;p&gt;作者接著推導微分，首先計算 $\pd{\Lc(\xb)}{\eh_t}$：&lt;/p&gt;

\[\begin{align*}
\pd{\Lc(\xb)}{\eh_t} &amp;amp; = \begin{dcases}
  \pd{-\log(e_t)}{e_t} \pd{e_t}{\eh_t}     &amp;amp; \text{if } x_{t+1, 3} = 1 \\
  \pd{-\log(1 - e_t)}{e_t} \pd{e_t}{\eh_t} &amp;amp; \text{otherwise}
\end{dcases} \\
&amp;amp; = \begin{dcases}
  \dfrac{-1}{e_t} \dfrac{-\exp(\eh_t)}{(1 + \exp(\eh_t))^2}    &amp;amp; \text{if } x_{t+1, 3} = 1 \\
  \dfrac{1}{1 - e_t} \dfrac{-\exp(\eh_t)}{(1 + \exp(\eh_t))^2} &amp;amp; \text{otherwise}
\end{dcases} \\
&amp;amp; = \begin{dcases}
  1 - e_t &amp;amp; \text{if } x_{t+1, 3} = 1 \\
  -e_t    &amp;amp; \text{otherwise}
\end{dcases} \\
&amp;amp; = x_{t+1, 3} - e_t. &amp;amp;&amp;amp; \tag{27}\label{27}
\end{align*}\]

&lt;p&gt;推導與 GMM 相關的微分比較複雜，首先定義 $\gammah_t^j$ 輔助推導過程&lt;/p&gt;

\[\begin{align*}
\gammah_t^j &amp;amp; = \pi_t^j \cdot \Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j} &amp;amp;&amp;amp; \tag{28}\label{28} \\
\gamma_t^j  &amp;amp; = \dfrac{\gammah_t^j}{\Sum_{j' = 1}^M \gammah_t^{j'}}               &amp;amp;&amp;amp; \tag{29}\label{29}
\end{align*}\]

&lt;p&gt;接著推導 $\pd{\Lc(\xb)}{\pih_t^j}$：&lt;/p&gt;

\[\begin{align*}
\pd{\Lc(\xb)}{\pih_t^j} &amp;amp; = \Sum_{i = 1}^M \pa{\pd{-\log \Sum_{j' = 1}^M \gammah_t^{j'}}{\pi_t^i} \pd{\pi_t^i}{\pih_t^j}} \\
&amp;amp; = \Sum_{i = 1}^M \pa{\dfrac{-1}{\Sum_{j' = 1}^M \gammah_t^{j'}} \cdot \pd{\gammah_t^i}{\pi_t^i} \cdot (\delta_{i, j} \pi_t^i - \pi_t^i \pi_t^j)} \\
&amp;amp; = \Sum_{i = 1}^M \pa{\dfrac{-\Nc\pa{x_{t+1} \vert \mu_t^i, \sigma_t^i, \rho_t^i}}{\Sum_{j' = 1}^M \gammah_t^{j'}} \cdot (\delta_{i, j} \pi_t^i - \pi_t^i \pi_t^j)} \\
&amp;amp; = \Sum_{i = 1}^M \pa{\dfrac{-\delta_{i, j} \pi_t^i \Nc\pa{x_{t+1} \vert \mu_t^i, \sigma_t^i, \rho_t^i}}{\Sum_{j' = 1}^M \gammah_t^{j'}}} + \Sum_{i = 1}^M \pa{\dfrac{\pi_t^i \pi_t^j \Nc\pa{x_{t+1} \vert \mu_t^i, \sigma_t^i, \rho_t^i}}{\Sum_{j' = 1}^M \gammah_t^{j'}}} \\
&amp;amp; = -\gamma_t^j + \pi_t^j \\
&amp;amp; = \pi_t^j - \gamma_t^j. &amp;amp;&amp;amp; \tag{30}\label{30}
\end{align*}\]

&lt;p&gt;接著推導 $\Lc(\xb)$ 對於 $\muh_{t, 1}^j, \muh_{t, 2}^j, \sigmah_{t, 1}^j, \sigmah_{t, 2}^j, \rhoh_t^j$ 的微分，由於結構類似，為了方便一起表達成 $\pd{\Lc(\xb)}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)}$：&lt;/p&gt;

\[\begin{align*}
&amp;amp; \pd{\Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)} \\
&amp;amp; = \pd{\dfrac{1}{2 \pi \sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)} \cdot \exp\pa{\dfrac{-Z}{2 (1 - (\rho_t^j)^2)}} \\
&amp;amp; \quad + \dfrac{1}{2 \pi \sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}} \cdot \pd{\exp\pa{\dfrac{-Z}{2 (1 - (\rho_t^j)^2)}}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)} \\
&amp;amp; = \dfrac{-\Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j}}{2 \pi \sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}} \cdot \pd{2 \pi \sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)} + \Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j} \cdot \pd{\dfrac{-Z}{2 (1 - (\rho_t^j)^2)}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)} \\
&amp;amp; = -\Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j} \cdot \pa{\dfrac{1}{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}} \cdot \pd{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)} + \pd{\dfrac{Z}{2 (1 - (\rho_t^j)^2)}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)}}.
\end{align*}\]

\[\begin{align*}
&amp;amp; \pd{\Lc(\xb)}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)} \\
&amp;amp; = \pd{-\log \Sum_{j' = 1}^M \gammah_t^{j'}}{\gammah_t^j} \pd{\gammah_t^j}{\Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j}} \pd{\Nc\pa{x_{t+1} \vert \mu_t^j, \sigma_t^j, \rho_t^j}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)} \\
&amp;amp; = \gamma_t^j \cdot \pa{\dfrac{1}{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}} \cdot \pd{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)} + \pd{\dfrac{Z}{2 (1 - (\rho_t^j)^2)}}{(\muh_t^j, \sigmah_t^j, \rhoh_t^j)}} &amp;amp;&amp;amp; \tag{31}\label{31}
\end{align*}\]

&lt;p&gt;各個微分項次各自展開可以得到以下結果：&lt;/p&gt;

\[\begin{align*}
\pd{\Lc(\xb)}{\muh_{t, 1}^j} &amp;amp; = r_t^j \cdot \pd{\dfrac{Z}{2 (1 - (\rho_t^j)^2)}}{\muh_{t, 1}^j} \\
&amp;amp; = \dfrac{r_t^j}{2 (1 - (\rho_t^j)^2)} \cdot \pa{-2 \dfrac{x_{t+1, 1} - \mu_{t, 1}^j}{(\sigma_{t, 1}^j)^2} + \dfrac{2 \rho_t^j (x_{t+1, 2} - \mu_{t, 2}^j)}{\sigma_{t, 1}^j \sigma_{t, 2}^j}} \\
&amp;amp; = \dfrac{-r_t^j}{\sigma_{t, 1}^j (1 - (\rho_t^j)^2)} \cdot \pa{\dfrac{x_{t+1, 1} - \mu_{t, 1}^j}{\sigma_{t, 1}^j} - \dfrac{\rho_t^j (x_{t+1, 2} - \mu_{t, 2}^j)}{\sigma_{t, 2}^j}} &amp;amp;&amp;amp; \tag{32}\label{32} \\
\pd{\Lc(\xb)}{\muh_{t, 2}^j} &amp;amp; = \dfrac{-r_t^j}{\sigma_{t, 2}^j (1 - (\rho_t^j)^2)} \cdot \pa{\dfrac{x_{t+1, 2} - \mu_{t, 2}^j}{\sigma_{t, 2}^j} - \dfrac{\rho_t^j (x_{t+1, 1} - \mu_{t, 2}^j)}{\sigma_{t, 1}^j}} &amp;amp;&amp;amp; \tag{33}\label{33} \\
\pd{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}}{\sigmah_{t, 1}^j} &amp;amp; = \pd{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}}{\sigma_{t, 1}^j} \cdot \pd{\sigma_{t, 1}^j}{\sigmah_{t, 1}^j} \\
&amp;amp; = \sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2} \\
\pd{\dfrac{Z}{2 (1 - (\rho_t^j)^2)}}{\sigmah_{t, 1}^j} &amp;amp; = \dfrac{1}{2 (1 - (\rho_t^j)^2)} \pd{Z}{\sigma_{t, 1}^j} \pd{\sigma_{t, 1}^j}{\sigmah_{t, 1}^j} \\
&amp;amp; = \dfrac{1}{2 (1 - (\rho_t^j)^2)} \pa{\dfrac{-2 (x_{t+1, 1} - \mu_{t, 1}^j)^2}{(\sigma_{t, 1}^j)^2} + \dfrac{2 \rho_t^j \pa{x_{t+1, 1} - \mu_{t, 1}^j} \pa{x_{t+1, 2} - \mu_{t, 2}^j}}{\sigma_{t, 1}^j \sigma_{t, 2}^j}} \\
&amp;amp; = \dfrac{-\pa{x_{t+1, 1} - \mu_{t, 1}^j}}{\sigma_{t, 1}^j (1 - (\rho_t^j)^2)} \pa{\dfrac{x_{t+1, 1} - \mu_{t, 1}^j}{\sigma_{t, 1}^j} - \dfrac{\rho_t^j \pa{x_{t+1, 2} - \mu_{t, 2}^j}}{\sigma_{t, 2}^j}} \\
\pd{\Lc(\xb)}{\sigmah_{t, 1}^j} &amp;amp; = r_t^j \cdot \pa{\dfrac{1}{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}} \cdot \pd{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}}{\sigmah_{t, 1}^j} + \pd{\dfrac{Z}{2 (1 - (\rho_t^j)^2)}}{\sigmah_{t, 1}^j}} \\
&amp;amp; = r_t^j \pa{1 - \dfrac{x_{t+1, 1} - \mu_{t, 1}^j}{\sigma_{t, 1}^j (1 - (\rho_t^j)^2)} \pa{\dfrac{x_{t+1, 1} - \mu_{t, 1}^j}{\sigma_{t, 1}^j} - \dfrac{\rho_t^j \pa{x_{t+1, 2} - \mu_{t, 2}^j}}{\sigma_{t, 2}^j}}} &amp;amp;&amp;amp; \tag{34}\label{34} \\
\pd{\Lc(\xb)}{\sigmah_{t, 2}^j} &amp;amp; = r_t^j \pa{1 - \dfrac{x_{t+1, 2} - \mu_{t, 2}^j}{\sigma_{t, 2}^j (1 - (\rho_t^j)^2)} \pa{\dfrac{x_{t+1, 2} - \mu_{t, 2}^j}{\sigma_{t, 2}^j} - \dfrac{\rho_t^j \pa{x_{t+1, 1} - \mu_{t, 1}^j}}{\sigma_{t, 1}^j}}} &amp;amp;&amp;amp; \tag{35}\label{35} \\
\pd{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}}{\rhoh_t^j} &amp;amp; = \sigma_{t, 1}^j \sigma_{t, 2}^j \pd{\sqrt{1 - (\rho_t^j)^2}}{\rho_t^j} \pd{\rho_t^j}{\rhoh_t^j} \\
&amp;amp; = \sigma_{t, 1}^j \sigma_{t, 2}^j \dfrac{-2 \rho_t^j}{2 \sqrt{1 - (\rho_t^j)^2}} (1 - (\rho_t^j)^2) \\
&amp;amp; = -\sigma_{t, 1}^j \sigma_{t, 2}^j \rho_t^j \sqrt{1 - (\rho_t^j)^2} \\
\pd{\dfrac{Z}{2 (1 - (\rho_t^j)^2)}}{\rhoh_t^j} &amp;amp; = \pd{Z}{\rho_t^j} \pd{\rho_t^j}{\rhoh_t^j} \dfrac{1}{2 (1 - (\rho_t^j)^2)} + \dfrac{Z}{2} \pd{\dfrac{1}{1 - (\rho_t^j)^2}}{\rho_t^j} \pd{\rho_t^j}{\rhoh_t^j} \\
&amp;amp; = -\dfrac{\pa{x_{t+1, 1} - \mu_{t, 1}^j} \pa{x_{t+1, 2} - \mu_{t, 2}^j}}{\sigma_{t, 1}^j \sigma_{t, 2}^j} + Z \dfrac{\rho_t^j}{1 - (\rho_t^j)^2} \\
\pd{\Lc(\xb)}{\rhoh_t^j} &amp;amp; = r_t^j \cdot \pa{\dfrac{1}{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}} \cdot \pd{\sigma_{t, 1}^j \sigma_{t, 2}^j \sqrt{1 - (\rho_t^j)^2}}{\rhoh_t^j} + \pd{\dfrac{Z}{2 (1 - (\rho_t^j)^2)}}{\rhoh_t^j}} \\
&amp;amp; = r_t^j \pa{-\rho_t^j - \dfrac{\pa{x_{t+1, 1} - \mu_{t, 1}^j} \pa{x_{t+1, 2} - \mu_{t, 2}^j}}{\sigma_{t, 1}^j \sigma_{t, 2}^j} + Z \dfrac{\rho_t^j}{1 - (\rho_t^j)^2}} \\
&amp;amp; = r_t^j \pa{-\dfrac{\pa{x_{t+1, 1} - \mu_{t, 1}^j} \pa{x_{t+1, 2} - \mu_{t, 2}^j}}{\sigma_{t, 1}^j \sigma_{t, 2}^j} + \rho_t^j \pa{\dfrac{Z}{1 - (\rho_t^j)^2} - 1}} \tag{36}\label{36}
\end{align*}\]

&lt;h3 id=&quot;iam-ondb&quot;&gt;實驗 3：IAM-OnDB&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 5：手寫辨識訓練結果。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。
&lt;img src=&quot;https://i.imgur.com/NEM0awl.png&quot; alt=&quot;圖 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-6&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 6：手寫輸入為 &lt;em&gt;under&lt;/em&gt; 時模型的預測分佈。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。
&lt;img src=&quot;https://i.imgur.com/LBVkors.png&quot; alt=&quot;圖 6&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;此論文所有與手寫文字相關的資料都是來自於 IAM-OnDB 資料集
    &lt;ul&gt;
      &lt;li&gt;資料來源為 $221$ 位不同的寫作者在智慧白板（smart whiteboard）上的筆跡&lt;/li&gt;
      &lt;li&gt;所有寫作者都被要求在智慧白板上寫下 Lancaster-Oslo-bergen text corpus&lt;/li&gt;
      &lt;li&gt;所有的筆跡都是由智慧白板角落配備的紅外線感應裝置進行偵測與紀錄&lt;/li&gt;
      &lt;li&gt;缺少的紀錄數值都用內插法（interpolation）進行填補&lt;/li&gt;
      &lt;li&gt;超過特定長度的手寫數值序列會被從資料集中移除&lt;/li&gt;
      &lt;li&gt;平均一個字母有 $25$ 個座標值，白板上的一行文字平均有 $700$ 個座標值&lt;/li&gt;
      &lt;li&gt;Training set 有 $5364$ 行&lt;/li&gt;
      &lt;li&gt;共有兩個 validation sets，分別有 $1438$ 與 $1518$ 行
        &lt;ul&gt;
          &lt;li&gt;作者把比較大的 validation set 作為 training data 使用&lt;/li&gt;
          &lt;li&gt;作者把比較小的 validation set 作為 early-stopping 的評估手段&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Test set 有 $3859$ 行&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;預測結果將會作為下個時間點的輸入，概念跟 language model 一樣
    &lt;ul&gt;
      &lt;li&gt;此論文是第一篇論文採用 language model 的作法模擬與生成手寫文字&lt;/li&gt;
      &lt;li&gt;一個字母的最後預測目標為 end-of-stroke&lt;/li&gt;
      &lt;li&gt;與大部分的方法不同，LSTM 沒有任何特殊的前處理，只有簡單的將座標值 normalize 成平均值為 $0$ 標準差為 $1$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型架構
    &lt;ul&gt;
      &lt;li&gt;共有 $20$ 個 mixture models（$M = 20$）
        &lt;ul&gt;
          &lt;li&gt;每個時間點包含 $20$ 個組合權重 $\pi_t^1, \dots, \pi_t^{20}$&lt;/li&gt;
          &lt;li&gt;每個時間點包含 $40$ 個平均值 $\mu_{t, 1}^1, \mu_{t, 2}^1, \dots, \mu_{t, 1}^{20}, \mu_{t, 2}^{20}$&lt;/li&gt;
          &lt;li&gt;每個時間點包含 $40$ 個標準差 $\sigma_{t, 1}^1, \sigma_{t, 2}^1, \dots, \sigma_{t, 1}^{20}, \sigma_{t, 2}^{20}$&lt;/li&gt;
          &lt;li&gt;每個時間點包含 $20$ 個相關係數 $\rho_t^1, \dots, \rho_t^{20}$&lt;/li&gt;
          &lt;li&gt;每個時間點共有 $120$ 個參數用於模擬座標值&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;每個時間點都有 $1$ 個 end-of-stoke 機率值，因此每個時間點共有 $121$ 個參數用於模擬 $x_{t+1}$&lt;/li&gt;
      &lt;li&gt;實驗兩種大小不同的 LSTM
        &lt;ul&gt;
          &lt;li&gt;版本 1：$3$ 層 LSTM（$N = 3$），隱藏層單元數為 $400$（$h_t^n \in \R^{400}$）&lt;/li&gt;
          &lt;li&gt;版本 2：$1$ 層 LSTM（$N = 1$），隱藏層單元數為 $900$（$h_t^1 \in \R^{900}$）&lt;/li&gt;
          &lt;li&gt;兩個版本的總參數量約為 $3.4$ M&lt;/li&gt;
          &lt;li&gt;版本 1 有額外使用 adaptive weight noise，std 初始化為 $0.075$&lt;/li&gt;
          &lt;li&gt;實驗證實使用 fixed weight noise 對模型沒有幫助，因此不採用&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;最佳化演算法為 RMSProp，公式定義如下&lt;/p&gt;

\[\begin{align*}
  n_i &amp;amp; = \aleph n_{i - 1} + (1 - \aleph) \pa{\pd{\Lc(\xb)}{w_i}}^2 \\
  g_i &amp;amp; = \aleph g_{i - 1} + (1 - \aleph) \pd{\Lc(\xb)}{w_i} \\
  \Delta_i &amp;amp; = \lambda \Delta_{i - 1} - \eta \dfrac{\pd{\Lc(\xb)}{w_i}}{\sqrt{n_i - g_i^2 + \epsilon}} \\
  w_i &amp;amp; = w_{i - 1} + \Delta_i
\end{align*}\]

    &lt;ul&gt;
      &lt;li&gt;定義 $w_i$ 為參數&lt;/li&gt;
      &lt;li&gt;$\aleph = 0.95$&lt;/li&gt;
      &lt;li&gt;$\lambda = 0.9$&lt;/li&gt;
      &lt;li&gt;$\eta = 0.0001$&lt;/li&gt;
      &lt;li&gt;$\epsilon = 0.0001$&lt;/li&gt;
      &lt;li&gt;Gradient clipping 的範圍為 $[-10, 10]$&lt;/li&gt;
      &lt;li&gt;針對 $\pd{\Lc(\xb)}{\yh_t}$，gradient clipping 的範圍為 $[-100, 100]$，實驗證實此舉能夠穩定數值計算&lt;/li&gt;
      &lt;li&gt;當模型開始 overfitting 時仍然會出現數值問題&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#paper-fig-5&quot;&gt;圖 5&lt;/a&gt; 中紀錄了模型平均預測誤差（以對數呈現，愈低愈好），平均值的計算方法區分成以序列為資料點或以每個時間點的座標為資料點
    &lt;ul&gt;
      &lt;li&gt;以序列為資料點時，$3$ 層 LSTM 模型的平均預測誤差低於 $1$ 層 LSTM 模型&lt;/li&gt;
      &lt;li&gt;以每個時間點的座標為資料點時，$3$ 層 LSTM 模型的平均預測誤差高於 $1$ 層 LSTM 模型&lt;/li&gt;
      &lt;li&gt;使用 adaptive weight noise 能夠降低以序列為資料點的平均預測誤差&lt;/li&gt;
      &lt;li&gt;使用 adaptive weight noise 無法降低以每個時間點的座標為資料點的平均預測誤差&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#paper-fig-6&quot;&gt;圖 6&lt;/a&gt; 的上半部是預測手寫文字 &lt;em&gt;under&lt;/em&gt; 的筆跡時畫出的座標值機率分佈
    &lt;ul&gt;
      &lt;li&gt;觀察發現預測分佈出現兩種不同的泡泡（blobs）&lt;/li&gt;
      &lt;li&gt;小的泡泡是單一字母內的連續筆跡，由於下一個的座標有主要軌跡，造成標準差 $\sigma_{t, 1}^j, \sigma_{t, 2}^j$ 較小&lt;/li&gt;
      &lt;li&gt;大的泡泡出現在字的結尾，由於下一個字母的起始座標比較沒有預測邏輯，造成標準差 $\sigma_{t, 1}^j, \sigma_{t, 2}^j$ 較大&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#paper-fig-6&quot;&gt;圖 6&lt;/a&gt; 的下半部是預測手寫文字 &lt;em&gt;under&lt;/em&gt; 的筆跡時 $\pi_t^j$ 的權重分佈
    &lt;ul&gt;
      &lt;li&gt;觀察發現 end-of-stroke 所使用的權重分佈與單一字母中使用的權重分佈不同&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;從生成結果來看能夠生成出 end-of-stroke 以及一些常用字（&lt;em&gt;of&lt;/em&gt;、&lt;em&gt;the&lt;/em&gt; 等）
    &lt;ul&gt;
      &lt;li&gt;由於一個字母平均由 $25$ 個座標點組合而成，因此作者認為這證實 LSTM 擁有模擬長距離資訊的能力&lt;/li&gt;
      &lt;li&gt;作者沒有給出具體的生成方法，推測是將輸出的平均值 $\mu_{t, 1}^j \mu_{t, 2}^j$ 以 $\pi_t^j$ 進行加權平均當成下個時間點的座標點&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;透過文字序列生成手寫字跡&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-7&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 7：文字需列生成手寫字跡模型架構。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。
&lt;img src=&quot;https://i.imgur.com/TENhiJm.png&quot; alt=&quot;圖 7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-8&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 8：驗證模型學會文字與筆跡間的對應關係（alignment）。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。
&lt;img src=&quot;https://i.imgur.com/bgJf57n.png&quot; alt=&quot;圖 8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;作者嘗試讓模型在獲得文字輸入的狀態下生成手寫字跡。
當時並沒有 sequence-to-sequence 架構，並且資料集也沒有標記文字與筆跡之間的對應關係（alignment），因此作者基於 LSTM + GMM 提出了新架構。
從現在（2022）的角度來看，此架構就是 encoder + decoder，並且擁有類似 attention 的計算架構。&lt;/p&gt;

&lt;p&gt;首先定義 window $w_t$，功能就是 encoder，目標為將文字序列的所有資訊輸入給 LSTM 幫助生成手寫字跡。
令 $\cb = (c_1, \dots, c_U)$ 為文字序列，每個 $c_u$ 代表一個文字對應的 one-hot-encoding。
令 $\xb = (x_1, \dots, x_T)$ 為座標值序列，每個 $x_t$ 定義如同 $\eqref{15}$。
則 $w_t$ 的計算方法如下&lt;/p&gt;

\[\begin{align*}
\phi(t, u) &amp;amp; = \Sum_{k = 1}^K \alpha_t^k \exp\pa{-\beta_t^k \pa{\kappa_t^k - u}^2} &amp;amp;&amp;amp; \tag{37}\label{37} \\
w_t &amp;amp; = \Sum_{u = 1}^U \phi(t, u) c_u &amp;amp;&amp;amp; \tag{38}\label{38}
\end{align*}\]

&lt;ul&gt;
  &lt;li&gt;共有 $K$ 個 Gaussian distribution&lt;/li&gt;
  &lt;li&gt;$\phi(t, u)$ 代表文字 $c_u$ 對於時間點 $t$ 的預測重要程度（window weight of $c_u$ at time step $t$）
    &lt;ul&gt;
      &lt;li&gt;這代表 $\phi(t, u)$ 應該要是一個非負實數，甚至應該要加上 $\Sum_{u = 1}^U \phi(t, u) = 1$ 的限制&lt;/li&gt;
      &lt;li&gt;作者在這篇論文中並沒有加上總和為 $1$ 的限制&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\pa{\kappa_t^k - u}^2$ 代表距離（window location）
    &lt;ul&gt;
      &lt;li&gt;距離愈大數值就愈大&lt;/li&gt;
      &lt;li&gt;此項次的目標是希望距離愈遠的文字影響力愈小&lt;/li&gt;
      &lt;li&gt;這代表 $\kappa_t^k$ 應該要是一個非負實數&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\beta_t^k$ 代表距離的影響力（window width）
    &lt;ul&gt;
      &lt;li&gt;數值愈大影響力愈小，理由是會將計算結果取負號後在取指數&lt;/li&gt;
      &lt;li&gt;這代表 $\beta_t^k$ 應該要是一個非負實數&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\alpha_t^k$ 代表 GMM 的結合權重
    &lt;ul&gt;
      &lt;li&gt;這代表 $\alpha_t^k$ 應該要是一個非負實數&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$w_t$ 的維度與 $c_u$ 的維度相同&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接著定義 $\eqref{37} \eqref{38}$ 的符號細節&lt;/p&gt;

\[\begin{align*}
p &amp;amp; = \set{\alphah_t^k, \betah_t^k, \kappah_t^k}_{k = 1}^K = W_{h^1 p} h_t^1 + b_p &amp;amp;&amp;amp; \tag{39}\label{39} \\
\alpha_t^k &amp;amp; = \exp(\alphah_t^k) &amp;amp;&amp;amp; \tag{40}\label{40} \\
\beta_t^k &amp;amp; = \exp(\betah_t^k) &amp;amp;&amp;amp; \tag{41}\label{41} \\
\kappa_t^k &amp;amp; = \kappa_{t-1}^k + \exp(\kappah_t^k) &amp;amp;&amp;amp; \tag{42}\label{42} \\
h_t^1 &amp;amp; = \Hc\pa{W_{i h^1} x_t + W_{h^1 h^1} h_{t-1}^1 + W_{w h^1} w_{t-1} + b_h^1} &amp;amp;&amp;amp; \tag{43}\label{43} \\
h_t^n &amp;amp; = \Hc\pa{W_{i h^n} x_t + W_{h^{n-1} h^n} h_t^{n-1} + W_{h^n h^n} h_{t-1}^n + W_{w h^n} w_t + b_h^n} &amp;amp;&amp;amp; \tag{44}\label{44} \\
\Lc(\xb) &amp;amp; = -\log \Pr(\xb \vert \cb) &amp;amp;&amp;amp; \tag{45}\label{45} \\
\Pr(\xb \vert \cb) &amp;amp; = \Prod_{t = 1}^T \Pr(x_{t+1} \vert y_t) &amp;amp;&amp;amp; \tag{46}\label{46}
\end{align*}\]

&lt;ul&gt;
  &lt;li&gt;$p \in \R^{3K}$，$y_t$ 的定義沿用 $\eqref{17} – \eqref{22}$&lt;/li&gt;
  &lt;li&gt;$\eqref{42}$ 的定義是為了讓模型能夠模擬 sliding window 的概念&lt;/li&gt;
  &lt;li&gt;$\eqref{43} \eqref{44}$ 是修改 $\eqref{1} \eqref{2}$ 而得，架構見&lt;a href=&quot;#paper-fig-7&quot;&gt;圖 7&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;由於 $h_t^1$ 用於建立 $w_t$，因此 $h_t^1$ 會收到 $w_{t-1}$ 而不是 $w_t$&lt;/li&gt;
  &lt;li&gt;由於 $h_t^n$ 的輸出是基於 encoder 資訊，因此 $h_t^n$ 會收到 $w_t$ 而不是 $w_{t-1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;接下來的微分推導我就跳過了，因為很懶，先相信他是對的。&lt;/p&gt;

&lt;h3 id=&quot;iam-ondb-1&quot;&gt;實驗 4：IAM-OnDB&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-9&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 9：給予文字預測手寫軌跡的訓練結果。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。
&lt;img src=&quot;https://i.imgur.com/h7VV4Zp.png&quot; alt=&quot;圖 9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-10&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 10：給予文字 &lt;em&gt;under&lt;/em&gt; 時預測手寫軌跡的機率分佈。
圖片來源：&lt;a href=&quot;https://arxiv.org/abs/1308.0850&quot;&gt;論文&lt;/a&gt;。
&lt;img src=&quot;https://i.imgur.com/bbrhMg7.png&quot; alt=&quot;圖 10&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;資料集一樣是 IAM-OnDB&lt;/li&gt;
  &lt;li&gt;將 token 定義為 character
    &lt;ul&gt;
      &lt;li&gt;區分大小寫，包含標點符號與數字&lt;/li&gt;
      &lt;li&gt;共有 $80$ 個不同的符號，但作者只使用 $57$ 個符號，將大多數數字與標點符號換成 non-letter&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型架構
    &lt;ul&gt;
      &lt;li&gt;預測座標一共使用 $20$ 個 mixture models（$M = 20$）&lt;/li&gt;
      &lt;li&gt;Encoder 一共使用 $10$ 個 mixture models（$K = 10$）&lt;/li&gt;
      &lt;li&gt;$w_t \in \R^{57}$&lt;/li&gt;
      &lt;li&gt;$3$ 層 LSTM（$N = 3$），隱藏層單元數為 $400$（$h_t^n \in \R^{400}$）&lt;/li&gt;
      &lt;li&gt;總參數量約為 $3.7$ M&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;最佳化演算法為 RMSProp，設定與實驗 3 完全相同&lt;/li&gt;
  &lt;li&gt;根據&lt;a href=&quot;#paper-fig-9&quot;&gt;圖 9&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;以序列為資料點時，使用 adaptive weight noise 表現較好&lt;/li&gt;
      &lt;li&gt;以每個時間點的座標為資料點時，使用 adaptive weight noise 沒有差別&lt;/li&gt;
      &lt;li&gt;相比於&lt;a href=&quot;#paper-fig-5&quot;&gt;圖 5&lt;/a&gt;，作者認為實驗 4 表現較好&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;比較&lt;a href=&quot;#paper-fig-10&quot;&gt;圖 10&lt;/a&gt; 與&lt;a href=&quot;#paper-fig-6&quot;&gt;圖 6&lt;/a&gt;，可以發現預測的信心度上升導致圓形泡泡都縮小&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;unbiased--biased-sampling&quot;&gt;Unbiased / Biased Sampling&lt;/h2&gt;

&lt;p&gt;首先定義 unbiased sampling 的概念：
生成一個座標序列 $x_1, x_2, \dots, x_T$ 時，最佳解為從機率分佈中挑選機率值最高的答案。
但由於排列組合數過高，因此實行上效率很低。
作者便採用每個時間點獨立挑選機率值最高的答案作為近似解。
而生成結束的判斷依據則是當生成 end-of-sequence（index 定義成 $U + 1$）的機率值高於其他 token 時則結束，即滿足以下條件&lt;/p&gt;

\[\forall 1 \leq u \leq U, \phi(t, U + 1) &amp;gt; \phi(t, u).\]

&lt;p&gt;而作者認為在 inference 階段可以人為刻意讓生成的機率值偏向特定 Gaussian distribution，甚至讓 Gaussian distribution 的 std 變小確保生成差異性變小，概念就像是生成「平均手寫結果」。
作者稱此方法為 biased sampling，具體上採用的公式如下（修改 $\eqref{19} \eqref{21}$ 而得）：&lt;/p&gt;

\[\begin{align*}
\sigma_t^j &amp;amp; = \exp\pa{\sigmah_j^t - b}; \\
\pi_t^j &amp;amp; = \dfrac{\exp(\pih_t^j (1 + b))}{\Sum_{j' = 1}^M \exp\pa{\pih_t^{j'} (1 + b)}}.
\end{align*}\]

&lt;p&gt;其中 $b \in [0, \infty)$.
當 $b = 0$ 時等同於採用 unbiased sampling，當 $b \to \infty$ 時等同於 std 為 0 且永遠採用一個 Gaussian distribution。&lt;/p&gt;

&lt;p&gt;根據生成範例可以發現，使用 biased sampling 的結果真的能夠讓生成差異變小，而在 $b = 0.15$ 時能夠在維持生成多樣性與減少差異達到不錯的平衡點。&lt;/p&gt;

&lt;h2 id=&quot;primed-sampling&quot;&gt;Primed Sampling&lt;/h2&gt;

&lt;p&gt;作者認為如果想要指定生成特定寫作者的字跡，最簡單的作法就是讓模型只訓練在該作者的字跡資料上。
而作者額外提出以特殊符號代表寫作者，並加入至 encoder 的輸入 $\cb$ 之中，如此就可以訓練一個模型模擬所有寫作者的風格。
此方法稱為 primed sampling，根據生成範例證實此方法可行。&lt;/p&gt;</content><author><name>[&quot;Alex Graves&quot;]</name></author><category term="Text Modeling" /><category term="BPTT" /><category term="LSTM" /><category term="RNN" /><category term="language model" /><category term="model architecture" /><category term="neural network" /><summary type="html">目標 使用 LSTM 進行 sequence generation 作者 Alex Graves 隸屬單位 University of Toronto 期刊/會議名稱 arXiv 發表時間 2013 論文連結 https://arxiv.org/abs/1308.0850</summary></entry><entry><title type="html">The Penn Treebank: Annotating Predicate Argument Structure</title><link href="/dataset/2022/08/19/the-penn-treebank-annotating-predicate-argument-structure.html" rel="alternate" type="text/html" title="The Penn Treebank: Annotating Predicate Argument Structure" /><published>2022-08-19T22:26:00+08:00</published><updated>2022-08-19T22:26:00+08:00</updated><id>/dataset/2022/08/19/the-penn-treebank-annotating-predicate-argument-structure</id><content type="html" xml:base="/dataset/2022/08/19/the-penn-treebank-annotating-predicate-argument-structure.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;在 Penn Treebank 上額外標記 text categories、predicate argument structure、grammatical functions、semantic roles&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隸屬單位&lt;/td&gt;
      &lt;td&gt;University of Pennsylvania&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;Human Language Technology&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;1994&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://aclanthology.org/H94-1020/&quot;&gt;https://aclanthology.org/H94-1020/&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;參考手冊&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/penn-etb-2-style-guidelines.pdf&quot;&gt;https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/penn-etb-2-style-guidelines.pdf&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section&quot;&gt;重點&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;此論文是 Penn Treebank project 的第二階段
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC93T1&quot;&gt;第一階段&lt;/a&gt;已經標記 part of speech 與 constituent&lt;/li&gt;
      &lt;li&gt;第一階段已經蒐集大量的資料（over 1 million words）&lt;/li&gt;
      &lt;li&gt;新的標記資料可在 &lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC95T7&quot;&gt;LCD&lt;/a&gt; 下載&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;由於 Penn Treebank 在研究上的成效卓越，成功替代了 Brown Corpus 作為英文計算語言學主要研究資料集，同時進一步擴充標記內容
    &lt;ul&gt;
      &lt;li&gt;增加 predicate-argument structure 標記&lt;/li&gt;
      &lt;li&gt;新增加的標記啟蒙了後續的 SemEval 比賽內容&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;為了有效的標記 predicate argument structure，且不破壞 treebank context-free 的結構，作者提出以下手段進行標記
    &lt;ul&gt;
      &lt;li&gt;Null elements&lt;/li&gt;
      &lt;li&gt;Co-index&lt;/li&gt;
      &lt;li&gt;Pseudo-attach&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;新的標記準則&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;作者花了非常大量的時間撰寫新的標記準則，並編寫成參考手冊（style-book）
    &lt;ul&gt;
      &lt;li&gt;主要目的是讓所有標記人員在標記資料上達成共識，增加標記的一致性&lt;/li&gt;
      &lt;li&gt;希望標記人員能夠透過討論標記手冊的內容更加了解標記流程與規則，並藉此提高標記資料單位時間的產出（throughput）&lt;/li&gt;
      &lt;li&gt;不同標記人員取得的資料中擁有 10% 的重疊，以此確保標記的一致性（之前 POS 與 constituent 的標記並沒有此規則）&lt;/li&gt;
      &lt;li&gt;標記時間共花了 8 個月，每個禮拜標記工時為 10 小時&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;新的標記準則希望能夠達成 4 個目標
    &lt;ul&gt;
      &lt;li&gt;讓標記具有一致性&lt;/li&gt;
      &lt;li&gt;描述 null element 的使用方法&lt;/li&gt;
      &lt;li&gt;提供 non-context free 標記方法&lt;/li&gt;
      &lt;li&gt;清楚的區分 verb arguments and adjuncts&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;predicate-argument-structure&quot;&gt;Predicate Argument Structure&lt;/h2&gt;

&lt;p&gt;原始 Penn Treebank 每個 constituent 只有一個標記，並只與語法有關。
明顯的缺點就是當一個 constituent 語法上有明確分類，但語意上卻作為其他功能使用（例如：logical subject）時無法清楚標記。
新版的標記允許同一個 constituent 最多同時擁有 4 個不同的標記，包含 standard syntactic labels、text categories、grammatical functions 與 semantic roles。&lt;/p&gt;

&lt;h3 id=&quot;text-categories&quot;&gt;Text Categories&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;tag&lt;/th&gt;
      &lt;th&gt;meaning&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-HLN&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;headlines and datelines&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-LST&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;list markers&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-TTL&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;titles&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;grammatical-functions&quot;&gt;Grammatical Functions&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;tag&lt;/th&gt;
      &lt;th&gt;meaning&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-CLF&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;true clefs&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-NOM&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;non &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;s that function as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;s&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-ADV&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;clausal and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt; adverbials&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-LGS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;logical subjects in passives&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-PRD&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;non &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VP&lt;/code&gt; predicates&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-SBJ&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;surface subject&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-TPC&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;topicalized and fronted constituents&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-CLR&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;closed related&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;當 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt; 或 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S&lt;/code&gt; 明確的作為 predicate argument 使用時則不需任何標記
    &lt;ul&gt;
      &lt;li&gt;常見的 predicate argument 有兩種
        &lt;ul&gt;
          &lt;li&gt;Predicate argument 是 lowest（right most branching）&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Predicate argument 是 copular BE 的 immediately subconstituent&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;如果不是以上狀況則額外使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-PRD&lt;/code&gt; 進行標記&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;當 constituent 明確的作為 predication adjunction 使用時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-CLR&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;作者說在實際上區隔 argument 與 adjunction 是非常困難的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;I am happy .

(S
  (NP-SBJ I)
  (VP am
    (ADJP happy)
  )
  .
)

Predicate Argument Structure:
be(I, happy)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;I consider Kris a fool .

(S
  (NP-SBJ I)
  (VP consider
    (S
      (NP-SBJ Kris)
      (NP-PRD a fool)
    )
  )
  .
)

Predicate Argument Structure:
consider(I, a fool)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Was he ever successful ?

(SQ Was
  (NP-SBJ he)
  (ADVP-TMP ever)
  (ADJP-PRD successful)
  ?
)

Predicate Argument Structure:
be(he, successful)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;semantic-roles&quot;&gt;Semantic Roles&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;tag&lt;/th&gt;
      &lt;th&gt;meaning&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-VOC&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;vocatives&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-DIR&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;direction &amp;amp; trajectory&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-LOC&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;location&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-MNR&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;manner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-PRP&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;purpose and reason&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-TMP&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;temporal phrase&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;null-elements--co-indexing&quot;&gt;Null Elements + Co-indexing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*T*&lt;/code&gt; 標記 wh-movement 與 topicalization&lt;/li&gt;
  &lt;li&gt;使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; 標記其他 null elements&lt;/li&gt;
  &lt;li&gt;所有的 null elements 都會補上數字（稱為 co-indexing）連結替代的 constituents&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;What is Tim eating ?

(SBARQ
  (WHNP-1 What)
  (SQ is
    (NP-SBJ Tim)
    (VP eating
      (NP *T*-1)
    )
  )
  ?
)

Predicate Argument Structure:
eat(Tim, what)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;表達 passives（被動語氣）時，surface subject 會標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-SBJ&lt;/code&gt;，並加入提示 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(NP *)&lt;/code&gt;，且與 surface subject 共享數字（co-indexing）。
而 logical subject 會標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-LGS&lt;/code&gt;。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;The ball was thrown by Chris .

(S
  (NP-SBJ-1 The ball)
  (VP was
    (VP thrown
      (NP *-1)
      (PP by
        (NP-LGS Chris)
      )
    )
  )
  .
)

Predicate Argument Structure:
throw(Chris, ball)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Who was believed to have been shot ?

(SBARQ
  (WHNP-1 Who)
  (SQ was
    (NP-SBJ-2 *T*-1)
    (VP believed
      (S
        (NP-SBJ-3 *-2)
        (VP to
          (VP have
            (VP been
              (VP shot
                (NP *-3)
              )
            )
          )
        )
      )
    )
  )
  ?
)

Predicate Argument Structure:
believe(*someone*, shoot(*someone*, Who))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Null elements 也用來作為 null subject of infinitive complement clause。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Chris wants to throw the ball .

(S
  (NP-SBJ-1 Chris)
  (VP wants
    (S
      (NP-SBJ *-1)
      (VP to
        (VP throw
          (NP the ball)
        )
      )
    )
  )
  .
)

Predicate Argument Structure:
want(Chris, throw(Chris, ball))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;當 argument 被強調（topicalized）導致順序掉換時使用 null element 表達原本的結構，而 adjunct 被強調時則不使用 null element 做任何標記。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;This every man contains within him .

(S
  (NP-TPC-1 This)
  (NP-SBJ every man)
  (VP contains
    (NP *T*-1)
    (PP-LOC within
      (NP him)
    )
  )
  .
)

Predicate Argument Structure:
contains(every man, this)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;當 argument 是由 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VP&lt;/code&gt; 組成，且因 topicalized 導致順序掉換時，使用 null element 表達原本的結構。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Marching past the reviewing stand were 500 musicians .

(SINV
  (VP-TPC-1 Marching
    (PP-CLR past
      (NP the reviewing stand)
    )
  )
  (VP were
    (VP *T*-1)
  )
  (NP-SBJ 500 musicians)
  .
)

Predicate Argument Structure:
be(500 musicians, marching)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;pseudo-attach&quot;&gt;Pseudo-Attach&lt;/h2&gt;

&lt;p&gt;由於 Penn Treebank 的標記方法屬於 context-free，因此部份資訊無法在此標記架構下清楚表達。
主要的例子是出現在 verb + sentential adverb 之後，做為補語使用的 constituents（constituents which serve as complements to the verb occur after a sentential level adverb）。
這類型的 constituents 有兩種標記方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;將 adverb 留在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VP&lt;/code&gt; 內，導致 constituents 一起留在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VP&lt;/code&gt; 之中&lt;/li&gt;
  &lt;li&gt;將 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VP&lt;/code&gt; 結束，獨立出 adverb，導致 constituents 與 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VP&lt;/code&gt; 平行&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此類現象通稱為 discontinuous constituents 或 trapping problems。
作者認為可以透過 co-index + null element 進行標記解決 discontinuous constituents 的現象。
由於該現象的特殊性，作者決定給予獨特的標記，稱為 pseudo-attach。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;tag&lt;/th&gt;
      &lt;th&gt;meaning&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*ICH*&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;interpret constituent here&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*PPA*&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;permanent predictable ambiguity&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*RNR*&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;right node raising&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*EXP*&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;expletive&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*ICH*&lt;/code&gt; 就是專門用來解決簡單版 discontinuous constituents 的標記。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Chris knew yesterday that Terry would catch the ball .

(S
  (NP-SBJ Chris)
  (VP knew
    (SBAR *ICH*-1)
    (NP-TMP yesterday)
    (SBAR-1 that
      (S
        (NP-SBJ Terry)
        (VP would
          (VP catch
            (NP the ball)
          )
        )
      )
    )
  )
  .
)

Predicate Argument Structure:
know(Chris, that)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*PPA*&lt;/code&gt; 是專門用來表達永遠無法判斷 subconstituent 要連結到哪個 constituent 的狀態。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;I saw the man with the telescope .

(S
  (NP-SBJ I)
  (VP saw
    (NP
      (NP the man)
      (PP *PPA*-1)
    )
    (PP-CLR-1 with
      (NP the telescope)
    )
  )
  .
)

Predicate Argument Structure:
see(I, the man)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;當使用連接詞（conjunction）連接不同片段（conjuncts），且將共同的 constituent 提出時，使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*RNR*&lt;/code&gt; 進行標記。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;But our outlook has been , and continues to be , defensive .

(S But
  (NP-SBJ-2 our outlook)
  (VP
    (VP has
      (VP been
        (ADJP *RNR*-1)
      )
    )
    ,
    and
    (VP continues
      (S
        (NP-SBJ *-2)
        (VP to
          (VP be
            (ADJP *RNR*-1)
          )
        )
      )
    )
    ,
    (ADJP-1 defensive)
  )
  .
)

Predicate Argument Structure:
be(our outlook, defensive)
continues(our outlook, be(our outlook, defensive))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;由於用 It 作為句子開頭進行強調的用法很常見，作者決定給與特別標籤 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*EXP*&lt;/code&gt;。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;It is a pleasure to teach her .

(S
  (NP-SBJ
    (NP It)
    (S *EXP*-1)
  )
  (VP is
    (NP a pleasure)
  )
  (S-1
    (NP-SBJ *)
    (VP to
      (VP teach
        (NP her)
      )
    )
  )
  .
)

Predicate Argument Structure:
pleasure(teach(*someone*, her))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;conjunction-and-gapping&quot;&gt;Conjunction and Gapping&lt;/h2&gt;

&lt;p&gt;作者採用 Chomsky adjunction structure 標記 coordination。
Word level conjuntion 不給予標記，兩個字以上的 conjunction 才會給予明確標記。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Terry knew the person who threw the ball and who caught it .

(S
  (NP-SBJ Terry)
  (VP knew
    (NP
      (NP the person)
      (SBAR
        (SBAR
          (WHNP-1 who)
          (S
            (NP-SBJ T-1)
            (VP threw
              (NP the ball)
            )
          )
        )
        and
        (SBAR
          (WHNP-2 who)
          (S
            (NP-SBJ T-2)
            (VP caught
              (NP it)
            )
          )
        )
      )
    )
  )
  .
)

Predicate Argument Structure:
know(Terry, person(
  and(
    throw(*who*, ball),
    catch(*who*, it)
  )
))&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;當遇到 conjuntion 共享 predicate 時採用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;=&lt;/code&gt; 進行標記。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;John gave Mary a book and Bill a pencil .

(S
  (S
    (NP-SBJ John)
    (VP gave
      (NP-1 Mary)
      (NP-2 a book)
    )
  )
  and
  (S
    (NP=1 Bill)
    (NP=2 a pencil)
  )
  .
)

Predicate Argument Structure:
give(John, Mary, a book) and give(John, Bill, a pencil)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;I eat breakfast in the morning and lunch in the afternoon .

(S
  (S
    (NP-SBJ I)
    (VP eat
      (NP-1 breakfast)
      (PP-TMP-2 in
        (NP the morning)
      )
    )
  )
  and
  (S
    (NP=1 lunch)
    (PP-TMP=2 in
      (NP the afternoon)
    )
  )
  .
)

Predicate Argument Structure:
eat(I, breakfast) and eat(I, lunch)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;在口語上的共享詞時常省略且必須依賴前後文還原。
作者認為在此狀態下不需進行標記還原，並給予特殊標記 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FRAG&lt;/code&gt; 表示無法取得 predicate argument structure。
例如：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;Who threw the ball ? Chris , yesterday .

(FRAG
  (NP Chris)
  ,
  (NP-TMP yesterday)
  .
)&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;</content><author><name>[&quot;Mitchell Marcus&quot;, &quot;Grace Kim&quot;, &quot;Mary Ann Marcinkiewicz&quot;, &quot;Robert MacIntyre&quot;, &quot;Ann Bies&quot;, &quot;Mark Ferguson&quot;, &quot;Karen Katz&quot;, &quot;Britta Schasberger&quot;]</name></author><category term="Dataset" /><category term="Penn Treebank" /><category term="part of speech" /><category term="constituent parse" /><category term="predicate argument structure" /><category term="semantic role labeling" /><summary type="html">目標 在 Penn Treebank 上額外標記 text categories、predicate argument structure、grammatical functions、semantic roles 作者 Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger 隸屬單位 University of Pennsylvania 期刊/會議名稱 Human Language Technology 發表時間 1994 論文連結 https://aclanthology.org/H94-1020/ 參考手冊 https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/penn-etb-2-style-guidelines.pdf</summary></entry><entry><title type="html">Building a Large Annotated Corpus of English : The Penn Treebank</title><link href="/dataset/2022/08/12/building-a-large-annotated-corpus-of-english-the-penn-treebank.html" rel="alternate" type="text/html" title="Building a Large Annotated Corpus of English : The Penn Treebank" /><published>2022-08-12T15:24:00+08:00</published><updated>2022-08-12T15:24:00+08:00</updated><id>/dataset/2022/08/12/building-a-large-annotated-corpus-of-english-the-penn-treebank</id><content type="html" xml:base="/dataset/2022/08/12/building-a-large-annotated-corpus-of-english-the-penn-treebank.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;建立大型文字標記資料集，基於 Brown Corpus 的詞性標記作修改&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;Mitchell Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隸屬單位&lt;/td&gt;
      &lt;td&gt;University of Pennsylvania&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;Computational Linguistics&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;1993&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://alliance.seas.upenn.edu/~nlp/publications/pdf/marcus1993.pdf&quot;&gt;https://alliance.seas.upenn.edu/~nlp/publications/pdf/marcus1993.pdf&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;詞性標記參考手冊&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&amp;amp;context=cis_reports&quot;&gt;https://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&amp;amp;context=cis_reports&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;語法成份分析標記參考手冊&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://languagelog.ldc.upenn.edu/myl/PennTreebank1995.pdf&quot;&gt;http://languagelog.ldc.upenn.edu/myl/PennTreebank1995.pdf&lt;/a&gt;（找不到原版）&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section&quot;&gt;重點&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;包含超過 4.5M 個詞的英文（American）資料集&lt;/li&gt;
  &lt;li&gt;標記資料執行了三年（1989–1992），標記了詞性（part-of-speech，POS）與語法成份分析（constituent parse）&lt;/li&gt;
  &lt;li&gt;資料可以在 &lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC93T1&quot;&gt;Linguistic Data Consortium（LDC）&lt;/a&gt;付費下載&lt;/li&gt;
  &lt;li&gt;資料是先採用自動化標記，後由人工校正的標記流程
    &lt;ul&gt;
      &lt;li&gt;此流程速度較快、一致性高&lt;/li&gt;
      &lt;li&gt;當不採用自動化標記的流程時，速度較慢，一致性降低&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Penn Treebank 詞性標記受到 Brown Corpus 的啟發
    &lt;ul&gt;
      &lt;li&gt;Brown Corpus 共有 87 種基礎詞性標記&lt;/li&gt;
      &lt;li&gt;Brown Corpus 允許透過基礎詞性標記進行組合，因此共有 187 種基礎型 + 組合型詞性標記&lt;/li&gt;
      &lt;li&gt;Brown Corpus 後續的研究通常都是將詞性標記分的更細，目的是為了將每個詞在不同文法下的功能進行區分（The ideal of providing distinct codings for all classes of words having distinct grammatical behaviour）
        &lt;ul&gt;
          &lt;li&gt;Lancaster-Oslo / Bergen（LOB）Corpus 擁有 135 種詞性標記&lt;/li&gt;
          &lt;li&gt;Lancaster UCREL group 擁有 165 種詞性標記&lt;/li&gt;
          &lt;li&gt;London-Lund Corpus of Spoken English 擁有 197 種詞性標記&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Penn Treebank 的詞性標記是將 Brown Corpus 的詞性標記種類減少後得到的結構
    &lt;ul&gt;
      &lt;li&gt;作者宣稱這是基於&lt;strong&gt;統計&lt;/strong&gt;且考量詞彙（lexical）與語法（syntactic）的資訊下進行標記簡化&lt;/li&gt;
      &lt;li&gt;在 Brown Corpus 中隸屬於特定詞彙的標籤都在 Penn Treebank 中被視為多餘資訊並移除&lt;/li&gt;
      &lt;li&gt;Penn Treebank 的去除標籤範例見&lt;a href=&quot;#paper-fig-1&quot;&gt;圖 1&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;標記總詞數約為 4.88M&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Penn Treebank 的語法成份分析標記將 Lancaster UCREL Treebank Project 中的結構精減（skeletal syntactic structure）
    &lt;ul&gt;
      &lt;li&gt;以 &lt;strong&gt;context free grammar&lt;/strong&gt; 為前提進行標記&lt;/li&gt;
      &lt;li&gt;標記總詞數約為 2.88M&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;詞性標記&lt;/h2&gt;

&lt;p&gt;作者認為 Penn Treebank 的出發點為&lt;strong&gt;統計&lt;/strong&gt;，因此在考量&lt;strong&gt;詞彙&lt;/strong&gt;（&lt;strong&gt;lexical&lt;/strong&gt;）與&lt;strong&gt;語法&lt;/strong&gt;（&lt;strong&gt;syntactic&lt;/strong&gt;）的資訊下將 Brown Corpus 中的 POS 標記種類種類減少。&lt;/p&gt;

&lt;h3 id=&quot;brown-corpus-&quot;&gt;與 Brown Corpus 的差別&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 1：Penn Treebank 動詞標記。
圖片來源：&lt;a href=&quot;https://alliance.seas.upenn.edu/~nlp/publications/pdf/marcus1993.pdf&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/CMZtL3B.png&quot; alt=&quot;圖 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Penn Treebank 基於&lt;strong&gt;詞彙還原度&lt;/strong&gt;（&lt;strong&gt;lexical recoverability&lt;/strong&gt;）減少標記種類
    &lt;ul&gt;
      &lt;li&gt;在 Brown Corpus 中許多類似的標記被進一步細分，作者認為這些區分只要&lt;strong&gt;觀察詞彙本身&lt;/strong&gt;就能夠區分差異，不需要在標記進行區別&lt;/li&gt;
      &lt;li&gt;例如：Brown Corpus 中的&lt;strong&gt;助動詞&lt;/strong&gt;（&lt;strong&gt;auxiliary verb&lt;/strong&gt;）有獨立標記，在 Penn Treebank 中採用一致的&lt;strong&gt;動詞&lt;/strong&gt;（&lt;strong&gt;verb&lt;/strong&gt;）標記（見&lt;a href=&quot;#paper-fig-1&quot;&gt;圖 1&lt;/a&gt;）&lt;/li&gt;
      &lt;li&gt;例如：Brown Corpus 中&lt;strong&gt;限定詞前置程度詞&lt;/strong&gt;（&lt;strong&gt;pre-qualifiers&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABL&lt;/code&gt;）、&lt;strong&gt;限定詞前置量詞&lt;/strong&gt;（&lt;strong&gt;pre-quantifiers&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABN&lt;/code&gt;）或是 &lt;em&gt;both&lt;/em&gt;（標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABX&lt;/code&gt;）都有不同的標記，Penn Treebank 統一標記為&lt;strong&gt;限定詞前置詞&lt;/strong&gt;（&lt;strong&gt;predeterminer&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;）&lt;/li&gt;
      &lt;li&gt;例如：Brown Corpus 中&lt;strong&gt;單數反身人稱代名詞&lt;/strong&gt;（&lt;strong&gt;singular reflexive personal pronoun&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPL&lt;/code&gt;）與&lt;strong&gt;複數反身人稱代名詞&lt;/strong&gt;（&lt;strong&gt;plural reflexive personal pronoun&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPLS&lt;/code&gt;）各有不同標記，在 Penn Treebank 中都合併進入&lt;strong&gt;人稱代名詞&lt;/strong&gt;（&lt;strong&gt;personal pronoun&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRP&lt;/code&gt;）&lt;/li&gt;
      &lt;li&gt;例如：Brown Corpus 中&lt;strong&gt;名詞性副詞&lt;/strong&gt;（&lt;strong&gt;nominal adverb&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RN&lt;/code&gt;）有獨立標記，在 Penn Treebank 中合併進入&lt;strong&gt;副詞&lt;/strong&gt;（&lt;strong&gt;adverb&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Penn Treebank 基於&lt;strong&gt;語法還原度&lt;/strong&gt;（&lt;strong&gt;syntactic structure recoverability&lt;/strong&gt;）減少標記種類
    &lt;ul&gt;
      &lt;li&gt;在 Brown Corpus 中許多名詞因為語法結構不同被賦與特殊標記，作者認為只要&lt;strong&gt;觀察語法結構&lt;/strong&gt;就能夠區分差異，不需要在標記上進行區別&lt;/li&gt;
      &lt;li&gt;例如：Brown Corpus 中&lt;strong&gt;主詞代名詞&lt;/strong&gt;（&lt;strong&gt;subject pronoun&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPSS&lt;/code&gt;）與&lt;strong&gt;受詞代名詞&lt;/strong&gt;（&lt;strong&gt;object pronoun&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPO&lt;/code&gt;）有不同標記，在 Penn Treebank 中合併成&lt;strong&gt;人身代名詞&lt;/strong&gt;（&lt;strong&gt;personal pronoun&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRP&lt;/code&gt;）&lt;/li&gt;
      &lt;li&gt;例如：Brown Corpus 中&lt;strong&gt;從屬子句連接詞&lt;/strong&gt;（&lt;strong&gt;subordinating conjunction&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CS&lt;/code&gt;）有獨立標記，在 Penn Treebank 中合併進入&lt;strong&gt;介系詞&lt;/strong&gt;（&lt;strong&gt;preposition&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Penn Treebank 具有&lt;strong&gt;一致性&lt;/strong&gt;（&lt;strong&gt;consistency&lt;/strong&gt;）
    &lt;ul&gt;
      &lt;li&gt;作者宣稱在同時考量詞彙還原度與語法還原度後能夠減少標記不一致的問題&lt;/li&gt;
      &lt;li&gt;例如：在 Brown Corpus 中 &lt;em&gt;there&lt;/em&gt;、&lt;em&gt;now&lt;/em&gt; 都被標記為&lt;strong&gt;副詞&lt;/strong&gt;（&lt;strong&gt;adverb&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;），但在完全相同的語法結構下 &lt;em&gt;here&lt;/em&gt; 與 &lt;em&gt;then&lt;/em&gt; 有可能被標記為&lt;strong&gt;副詞&lt;/strong&gt;或&lt;strong&gt;名詞性副詞&lt;/strong&gt;（&lt;strong&gt;nominal adverb&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RN&lt;/code&gt;）；而 Penn Treebank 都一致標記為&lt;strong&gt;副詞&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Penn Treebank 標記有融入&lt;strong&gt;語法功能&lt;/strong&gt;（&lt;strong&gt;syntactic function&lt;/strong&gt;）
    &lt;ul&gt;
      &lt;li&gt;例如：Brown Corpus 在進行&lt;strong&gt;名詞片語&lt;/strong&gt;（&lt;strong&gt;noun phrase）標記&lt;/strong&gt;時，&lt;strong&gt;中心語&lt;/strong&gt;（&lt;strong&gt;head&lt;/strong&gt;）不一定會被標記成&lt;strong&gt;名詞&lt;/strong&gt;（&lt;strong&gt;noun&lt;/strong&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;）；而 Penn Treebank 會考量中心語的&lt;strong&gt;語法功能&lt;/strong&gt;進行標記，因此&lt;strong&gt;中心語&lt;/strong&gt;會標記成&lt;strong&gt;名詞&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;例如：Brown Corpus 永遠將 &lt;em&gt;both&lt;/em&gt; 標記成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABX&lt;/code&gt;；而 Penn Treebank 會依照 &lt;em&gt;both&lt;/em&gt; 出現的位置給予不同標記
        &lt;ul&gt;
          &lt;li&gt;&lt;em&gt;both the boys&lt;/em&gt; 中的 &lt;em&gt;both&lt;/em&gt; 為冠詞 &lt;em&gt;the&lt;/em&gt; 的前置詞，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;the boys both&lt;/em&gt; 中的 &lt;em&gt;both&lt;/em&gt; 為名詞片語 &lt;em&gt;the boys&lt;/em&gt; 的後置修飾詞，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;both of the boys&lt;/em&gt; 中的 &lt;em&gt;both&lt;/em&gt; 為名詞片語中心語且為複數，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;both boys and girls&lt;/em&gt; 中的 &lt;em&gt;both&lt;/em&gt; 為名詞 &lt;em&gt;boys&lt;/em&gt; 與 &lt;em&gt;girls&lt;/em&gt; 的連接詞，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;唯一的例外是 existential &lt;em&gt;there&lt;/em&gt;，Penn Treebank 中維持標記 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EX&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Penn Treebank 針對&lt;strong&gt;非第三人稱單數型動詞&lt;/strong&gt;（&lt;strong&gt;non-3rd person singular present tense&lt;/strong&gt;）新增一個標籤 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBP&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;Brown Corpus 使用動詞原型（verb，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;）標記不定式（infinitive）、命令語句（imperative）與非第三人稱單數型動詞（non-3rd person singular present tense）&lt;/li&gt;
      &lt;li&gt;Penn Treebank 對不定式原型與命令語句都採用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Penn Treebank 能夠容忍&lt;strong&gt;不確定性&lt;/strong&gt;（&lt;strong&gt;indeterminacy&lt;/strong&gt;）
    &lt;ul&gt;
      &lt;li&gt;在沒有足夠資訊的前提下，並不是所有標籤都是唯一的
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Grant can be outspoken–but not by anyone I know&lt;/em&gt;，一般來說 &lt;em&gt;outspoken&lt;/em&gt; 可以標記為形容詞（adjective，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;），但後面的 &lt;em&gt;by&lt;/em&gt; 告訴我們 &lt;em&gt;outspoken&lt;/em&gt; 在這裡其實是過去分詞（past participle）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;因此作者認為可以容忍多個標記答案，不同標記間以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;|&lt;/code&gt; 進行區隔&lt;/li&gt;
      &lt;li&gt;實際上出現的多標記組合種類並不多
        &lt;ul&gt;
          &lt;li&gt;形容詞或名詞，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ|NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;形容詞或現在分詞（present participle），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ|VBG&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;形容詞或過去分詞，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ|VBN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;名詞或現在分詞，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN|VBG&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;副詞或助詞，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB|RP&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-2&quot;&gt;標記總表&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 2：Penn Treebank 詞性標記總表。
圖片來源：&lt;a href=&quot;https://alliance.seas.upenn.edu/~nlp/publications/pdf/marcus1993.pdf&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/2Gggpbf.png&quot; alt=&quot;圖 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;總共包含了 36 個詞性標籤與 12 個其他標籤（主要是給標點符號與貨幣符號）&lt;/li&gt;
  &lt;li&gt;與 Brown Corpus 相比
    &lt;ul&gt;
      &lt;li&gt;少了
        &lt;ul&gt;
          &lt;li&gt;限定詞前置程度詞（pre-qualifier），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABL&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;限定詞前置量詞（pre-quantifier），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;單詞 &lt;em&gt;both&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABX&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;限定詞後置詞（post-determiner），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;冠詞（article），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;be&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BE&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;were&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BED&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;was&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEDZ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;being&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEG&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;am&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEM&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;been&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;are&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BER&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;is&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEZ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;關係子句連接詞（subordinating conjunction），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;do&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DO&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;did&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DOD&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;does&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DOZ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;單或複數限定詞 / 量詞（singular or plural determiner / quantifier），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTI&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;複數限定詞（plural determiner），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;雙數限定詞（determiner / double conjunction），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTX&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;標題（word occurring in headline），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HL&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;have&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HV&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;have&lt;/em&gt; 的過去式 &lt;em&gt;had&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVD&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;having&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVG&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;have&lt;/em&gt; 的過去分詞 &lt;em&gt;had&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;助動詞 &lt;em&gt;has&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVZ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;最高級形容詞（morphologically superlative adjective），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;語意最高級形容詞（semantically superlative adjective），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;單數名詞所有格（possessive singular noun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN$&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;複數名詞所有格（possessive plural noun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS$&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;單數專有名詞（proper noun or part of name phrase），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;單數專有名詞所有格（possessive proper noun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP$&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;複數專有名詞（plural proper noun or part of name phrase），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;複數專有名詞所有格（possessive plural proper noun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS$&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;單數副詞性名詞（adverbial noun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NR&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;複數副詞性名詞（plural adverbial noun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NRS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;序數（ordinal number），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OD&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;名詞性代詞（nominal pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;名詞性代詞 + &lt;em&gt;'s&lt;/em&gt;（possessive nominal pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PN$&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;人稱代名詞受格（possessive personal pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP$$&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;單數反身人稱代名詞（singular reflexive/intensive personal pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPL&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;複數反身人稱代名詞（plural reflexive/intensive personal pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPLS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;人稱代名詞受格（objective personal pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPO&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;第三人稱單數型代名詞主格（3rd. singular nominative pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;非第三人稱代名詞主格（other nominative personal pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPSS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;程度詞（qualifier），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QL&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;形容詞後置程度詞（post-qualifier），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QLP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;副詞最高級（superlative adverb），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;名詞性副詞（nominal adverb），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;頭銜（word occurring in title），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TL&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;wh-&lt;/em&gt; 代名詞受格（objective &lt;em&gt;wh-&lt;/em&gt; pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WPO&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;wh-&lt;/em&gt; 代名詞主格（nominative &lt;em&gt;wh-&lt;/em&gt; pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WPS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;wh-&lt;/em&gt; 程度詞（wh- qualifier），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WQL&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;not&lt;/em&gt;、&lt;em&gt;n't&lt;/em&gt;，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;刪節號（dash），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;多了
        &lt;ul&gt;
          &lt;li&gt;形容詞最高級（adjective，superlative），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;列表項目符號（list item marker），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;單數專有名詞（proper noun，singular），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;複數專有名詞（proper noun，plural），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNPS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;限定詞前置詞（predeterminer），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;所有格縮寫 &lt;em&gt;'s&lt;/em&gt;（possessive ending），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;人稱代名詞（personal pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;副詞最高級（adverb、superlative），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;科學符號（symbol，mathematical or scientific），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SYM&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;非第三人稱、單數動詞現在式（verb，non-3rd person singular present tense），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;em&gt;wh-&lt;/em&gt; 代名詞（&lt;em&gt;wh-&lt;/em&gt; pronoun），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;英鎊（pound sign），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;美金（dollar sign），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;對稱雙引號（stright double quote），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;左雙引號（left open double quote），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;“&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;右雙引號（right closed double quote），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;”&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;左單引號（left open single quote），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;右單引號（right closed single quote），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-3&quot;&gt;詞性標記參考手冊&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&amp;amp;context=cis_reports&quot;&gt;詞性標記參考手冊&lt;/a&gt;中額外描述的細節如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;對等連接詞（conjunction，coordinating）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含 &lt;em&gt;and&lt;/em&gt;、&lt;em&gt;but&lt;/em&gt;、&lt;em&gt;nor&lt;/em&gt;、&lt;em&gt;or&lt;/em&gt;、&lt;em&gt;yet&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;包含數學運算 &lt;em&gt;plus&lt;/em&gt;、&lt;em&gt;minus&lt;/em&gt;、&lt;em&gt;less&lt;/em&gt;、&lt;em&gt;times&lt;/em&gt;、&lt;em&gt;over&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;for&lt;/em&gt; 作為 &lt;em&gt;because&lt;/em&gt; 使用時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如 &lt;em&gt;He asked to be transferred, for he was unhappy.&lt;/em&gt; 中的 &lt;em&gt;for&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;both … and&lt;/em&gt; 配合出現且作為雙連接詞（double conjunction）的開頭時標記 &lt;em&gt;both&lt;/em&gt; 為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;both boys and girls are happy&lt;/em&gt; 中的 &lt;em&gt;both&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;either … or&lt;/em&gt; 配合出現且作為雙連接詞（double conjunction）的開頭時標記 &lt;em&gt;either&lt;/em&gt; 為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Either a boy could sing or a girl could dance.&lt;/em&gt; 中的 &lt;em&gt;Either&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Either a boy or a girl could sing.&lt;/em&gt; 中的 &lt;em&gt;Either&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Either a boy or girl could sing.&lt;/em&gt; 中的 &lt;em&gt;Either&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Either child could sing.&lt;/em&gt; 中的 &lt;em&gt;Either&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Either boy or girl could sing.&lt;/em&gt; 中的 &lt;em&gt;Either&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;neither … nor&lt;/em&gt; 配合出現且作為雙連接詞（double conjunction）的開頭時標記 &lt;em&gt;neither&lt;/em&gt; 為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;基數（cardinal number）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CD&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;當數字以 &lt;em&gt;number-number&lt;/em&gt; 的方式出現在形容詞的位置時則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;a 50-3 victory&lt;/em&gt; 中的 &lt;em&gt;50-3&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當數字以 &lt;em&gt;number-number&lt;/em&gt; 的方式出現在副詞的位置時則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;They won 50-3.&lt;/em&gt; 中的 &lt;em&gt;50-3&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當分數（hyphenated fraction）出現在類名詞之前進行修飾（prenominal modifier）時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;，但如果可以用 &lt;em&gt;double&lt;/em&gt; 或 &lt;em&gt;twice&lt;/em&gt; 替換時則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;one-half cup&lt;/em&gt; 中的 &lt;em&gt;one-half&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;one-half the amount&lt;/em&gt; 中的 &lt;em&gt;one-half&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;one&lt;/em&gt; 出現時有可能是基數或名詞，但在不確定是否描述基數的狀態下應標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CD&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;one of the best reasons&lt;/em&gt; 中的 &lt;em&gt;one&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CD&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;one&lt;/em&gt; 出現且可以被複數化（pluralized）時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the only one of its kind&lt;/em&gt; 中的 &lt;em&gt;one&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the only ones of its kind&lt;/em&gt; 中的 &lt;em&gt;ones&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;one&lt;/em&gt; 出現且可以被形容詞修飾時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the good one of its kind&lt;/em&gt; 中的 &lt;em&gt;one&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the good ones of its kind&lt;/em&gt; 中的 &lt;em&gt;ones&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;another one&lt;/em&gt; 一起出現時 &lt;em&gt;one&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;限定詞（determiner）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含冠詞（article）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;a&lt;/em&gt;、&lt;em&gt;an&lt;/em&gt;、&lt;em&gt;every&lt;/em&gt;、&lt;em&gt;no&lt;/em&gt;、&lt;em&gt;the&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含不定限定詞（indefinite determiner）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;another&lt;/em&gt;、&lt;em&gt;any&lt;/em&gt;、&lt;em&gt;some&lt;/em&gt;、&lt;em&gt;each&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;either way&lt;/em&gt; 中的 &lt;em&gt;either&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;neither decision&lt;/em&gt; 中的 &lt;em&gt;neither&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;that&lt;/em&gt;、&lt;em&gt;these&lt;/em&gt;、&lt;em&gt;this&lt;/em&gt;、&lt;em&gt;those&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;all&lt;/em&gt; 與 &lt;em&gt;both&lt;/em&gt; 不是出現在其他限定詞之前，也不是出現在代名詞所有格之前時，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;，否則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;all girls&lt;/em&gt; 中的 &lt;em&gt;all&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;all the girls&lt;/em&gt; 中的 &lt;em&gt;all&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;both little boys&lt;/em&gt; 中的 &lt;em&gt;both&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;both the little boys&lt;/em&gt; 中的 &lt;em&gt;both&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;由於一個名詞片語（noun phrase）中只能出現一個限定詞，而 &lt;em&gt;such&lt;/em&gt; 可以同時與限定詞一起出現，因此 &lt;em&gt;such&lt;/em&gt; 應該要標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the only such case&lt;/em&gt; 中的 &lt;em&gt;such&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;such&lt;/em&gt; 出現在限定詞之前時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;such a good time&lt;/em&gt; 中的 &lt;em&gt;such&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當限定詞作為代名詞使用時應標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;I can't stand this.&lt;/em&gt; 中的 &lt;em&gt;this&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I'll take both.&lt;/em&gt; 中的 &lt;em&gt;both&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Either would be fine.&lt;/em&gt; 中的 &lt;em&gt;Either&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Existential there 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EX&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;通常作為句子開頭銜接 be 動詞（be verb）或情態動詞（modal），發音上無重音（unstressed），並造成主詞（subject）與動詞（verb）順序掉換
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;There was a party in progress.&lt;/em&gt; 中的 &lt;em&gt;There&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EX&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;There ensued a melee.&lt;/em&gt; 中的 &lt;em&gt;There&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EX&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當作為副詞使用時發音會有重音（stress），主詞（subject）與動詞（verb）不會掉換順序
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;There, a party was in progress.&lt;/em&gt; 中的 &lt;em&gt;There&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;There, a melee ensued.&lt;/em&gt; 中的 &lt;em&gt;There&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;一個句子中可以同時出現 existential &lt;em&gt;there&lt;/em&gt; 與 adverbial &lt;em&gt;there&lt;/em&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;There was a party in progress there.&lt;/em&gt; 中的 &lt;em&gt;There&lt;/em&gt; 與 &lt;em&gt;there&lt;/em&gt; 分別標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EX&lt;/code&gt; 與 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;外語（foreign word）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FW&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;作者認為外語判別標準較為寬鬆
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;yoga&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;bête noire&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FW FW&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;persona non grata&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FW FW FW&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;介系詞（prepositions）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;to&lt;/em&gt; 擁有專屬的標記 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TO&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;包含從屬連接詞（subordinating conjunction）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;so that&lt;/em&gt; 中的 &lt;em&gt;so&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;從屬連接詞會放在子句（clause）之前，而介系詞會放在名詞片語（noun phrase）或介系詞片語（prepositional phrase）之前&lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;that&lt;/em&gt; 作為名詞補語（complements of noun）使用時功能為從屬連接詞，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the fact that you're here&lt;/em&gt; 中的 &lt;em&gt;that&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the claim that angels have wings&lt;/em&gt; 中的 &lt;em&gt;that&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;that&lt;/em&gt; 作為關係從句（relative clause，子句作為形容詞使用）時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;a man that I know&lt;/em&gt; 中的 &lt;em&gt;that&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;大部分的情況下介系詞會放在名詞片語之前，但有時候會因為強調語法導致介系詞與名詞片語位置錯開
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the credit card you won't want to do without&lt;/em&gt; 中的 &lt;em&gt;without&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;you won't want to do without the credit card&lt;/em&gt; 中的 &lt;em&gt;without&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the picture which we will look at next&lt;/em&gt; 中的 &lt;em&gt;at&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;we will look at the picture next&lt;/em&gt; 中的 &lt;em&gt;at&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He doesn't know what he is up against.&lt;/em&gt; 中的 &lt;em&gt;against&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He is up against what he doesn't know.&lt;/em&gt; 中的 &lt;em&gt;against&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當句子中的介系詞並沒有連接任何用語時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt; 或 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RP&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;We'll just have to do without.&lt;/em&gt; 中的 &lt;em&gt;without&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;We'll just have to do without it.&lt;/em&gt; 中的 &lt;em&gt;without&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;介系詞可以放在介系詞片語之前，這表示可以出現連續兩個介系詞
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;blaze out into space&lt;/em&gt; 中 &lt;em&gt;out&lt;/em&gt; 與 &lt;em&gt;into&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;come out of the woodwork&lt;/em&gt; 中 &lt;em&gt;out&lt;/em&gt; 與 &lt;em&gt;of&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;look up to someone&lt;/em&gt; 中 &lt;em&gt;up&lt;/em&gt; 與 &lt;em&gt;to&lt;/em&gt; 分別標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt; 與 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TO&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;because of her late arrival&lt;/em&gt; 中 &lt;em&gt;because&lt;/em&gt; 與 &lt;em&gt;of&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;to plant on into spring&lt;/em&gt; 中 &lt;em&gt;on&lt;/em&gt; 與 &lt;em&gt;into&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當介系詞只能出現在名詞片語之前，卻不能出現在之後時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;She stepped off the train.&lt;/em&gt; 中的 &lt;em&gt;off&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;將介系詞後出現的名詞片語替換成代名詞後，代名詞不能出現在介系詞之前，則該介系詞標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;She has been into it for a year.&lt;/em&gt; 中的 &lt;em&gt;into&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當介系詞為單音節（monosyllabic）、出現在句尾且不表重音（stress）則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Real bargains are hard to come by.&lt;/em&gt; 中的 &lt;em&gt;by&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Why don't you come by?&lt;/em&gt; 中的 &lt;em&gt;by&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ed&lt;/em&gt; 或 &lt;em&gt;-ing&lt;/em&gt; 的分詞（participles）作為介系詞使用時應該標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 或 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Granted that he is coming&lt;/em&gt; 中的 &lt;em&gt;Granted&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Provided that he comes&lt;/em&gt; 中的 &lt;em&gt;Provided&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;According to reliable sources&lt;/em&gt; 中的 &lt;em&gt;According&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Concerning your request of last week&lt;/em&gt; 中的 &lt;em&gt;Concerning&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;形容詞（adjective）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含以 hyphen 組成的複合修飾詞（hyphenated compounds used as modifier）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;happy-go-lucky&lt;/em&gt;、&lt;em&gt;one-of-a-kind&lt;/em&gt;、&lt;em&gt;run-of-the-mill&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;income-tax return&lt;/em&gt; 中的 &lt;em&gt;income-tax&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;income tax return&lt;/em&gt; 中的 &lt;em&gt;income tax&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;value-added tax&lt;/em&gt; 中的 &lt;em&gt;value-added&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;value added tax&lt;/em&gt; 中的 &lt;em&gt;value added&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN VBN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含序數（ordinal numbers）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;first&lt;/em&gt;、&lt;em&gt;2nd&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;不論是單一名詞或是多個名詞一起作為修飾詞使用時應標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;wool sweater&lt;/em&gt; 中的 &lt;em&gt;wool&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;woollen sweater&lt;/em&gt; 中的 &lt;em&gt;woollen&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;terminal type&lt;/em&gt; 中的 &lt;em&gt;terminal&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;terminal cancer&lt;/em&gt; 中的 &lt;em&gt;terminal&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;life insurance company&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN NN NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;顏色（color）作為形容詞使用時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;，作為名詞使用時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;These plants are dark green.&lt;/em&gt; 中的 &lt;em&gt;green&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;These plants are a dark green.&lt;/em&gt; 中的 &lt;em&gt;green&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當形容詞作為名詞使用，且該形容詞可被副詞修飾時應被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;，不論有無觸發主謂一致（subject-verb agreement）現象
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;The rich in this country pay far too few tax.&lt;/em&gt; 中的 &lt;em&gt;rich&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;The very rich in this country pay far too few tax.&lt;/em&gt; 中的 &lt;em&gt;rich&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;The handicapped&lt;/em&gt; 中的 &lt;em&gt;handicapped&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;The multiply handicapped&lt;/em&gt; 中的 &lt;em&gt;handicapped&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當形容詞作為名詞使用，但該形容詞不能被副詞修飾時應被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Little good will come of it.&lt;/em&gt; 中的 &lt;em&gt;good&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;方位名詞作為類名詞前置修飾詞（prenominal modifier）時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;top&lt;/em&gt;、&lt;em&gt;side&lt;/em&gt;、&lt;em&gt;bottom&lt;/em&gt;、&lt;em&gt;front&lt;/em&gt;、&lt;em&gt;back&lt;/em&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;開放式複合詞作為名詞使用時應該一起標記 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;mild flavored&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;語言名稱或國家名稱可以作為形容詞使用，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;English cuisine tends to be uninspired.&lt;/em&gt; 中的 &lt;em&gt;English&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;The English tends to be uninspired cooks.&lt;/em&gt; 中的 &lt;em&gt;English&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;國家以開放式複合詞（open compound）型態出現時標記一致
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the West German mark&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT JJ JJ NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He's a West German .&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRP POS DT NP NP .&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;標記謂詞形容詞（predicate adjective）時應標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;make life simple&lt;/em&gt; 中的 &lt;em&gt;simple&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;如果 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞能夠區分程度（gradable），則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Her talk was interesting.&lt;/em&gt; 中的 &lt;em&gt;interesting&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Her talk was very interesting.&lt;/em&gt; 中的 &lt;em&gt;interesting&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Her talk was more interesting than theirs.&lt;/em&gt; 中的 &lt;em&gt;interesting&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞如果存在以 &lt;em&gt;un-&lt;/em&gt; 開頭的反義詞，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;an interesting conversation&lt;/em&gt; 中的 &lt;em&gt;interesting&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;an uninteresting conversation&lt;/em&gt; 中的 &lt;em&gt;uninteresting&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞如果與 &lt;em&gt;be&lt;/em&gt; 動詞一起出現，且 &lt;em&gt;be&lt;/em&gt; 動詞可以替換成 &lt;em&gt;become&lt;/em&gt;、&lt;em&gt;feel&lt;/em&gt;、&lt;em&gt;look&lt;/em&gt;、&lt;em&gt;remain&lt;/em&gt;、&lt;em&gt;seem&lt;/em&gt;、&lt;em&gt;sound&lt;/em&gt;，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;The conversation is depressing.&lt;/em&gt; 中的 &lt;em&gt;depressing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;The conversation became depressing.&lt;/em&gt; 中的 &lt;em&gt;depressing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;That place feels depressing.&lt;/em&gt; 中的 &lt;em&gt;depressing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;That place looks depressing.&lt;/em&gt; 中的 &lt;em&gt;depressing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;That place remains depressing.&lt;/em&gt; 中的 &lt;em&gt;depressing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;That place seems depressing.&lt;/em&gt; 中的 &lt;em&gt;depressing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;That place sounds depressing.&lt;/em&gt; 中的 &lt;em&gt;depressing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞如果出現在名詞前，且該 &lt;em&gt;-ing&lt;/em&gt; 結尾詞的動詞原型是不及物動詞（intransitive），則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;an appealing face&lt;/em&gt; 中的 &lt;em&gt;appealing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;an appetizing dish&lt;/em&gt; 中的 &lt;em&gt;appetizing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a revolving fund&lt;/em&gt; 中的 &lt;em&gt;revolving&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the existing safeguards&lt;/em&gt; 中的 &lt;em&gt;existing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a holding company&lt;/em&gt; 中的 &lt;em&gt;holding&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a managing director&lt;/em&gt; 中的 &lt;em&gt;managing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a ruling class&lt;/em&gt; 中的 &lt;em&gt;ruling&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞如果出現在名詞前，且該 &lt;em&gt;-ing&lt;/em&gt; 結尾詞改為動詞原型時表達的意思不同，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;a winning smile&lt;/em&gt; 中的 &lt;em&gt;winning&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a striking hat&lt;/em&gt; 中的 &lt;em&gt;striking&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the striking teachers&lt;/em&gt; 中的 &lt;em&gt;striking&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞如果沒有對應的動詞原型則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;a thoroughgoing investigation&lt;/em&gt; 中的 &lt;em&gt;thoroughgoing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the outgoing president&lt;/em&gt; 中的 &lt;em&gt;outgoing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a outgoing type of guy&lt;/em&gt; 中的 &lt;em&gt;outgoing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;and outstanding record&lt;/em&gt; 中的 &lt;em&gt;outstanding&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;outstanding debts&lt;/em&gt; 中的 &lt;em&gt;outstanding&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;如果以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 形式出現的詞能夠區分程度（gradable），則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;He was superised.&lt;/em&gt; 中的 &lt;em&gt;superised&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He was very superised.&lt;/em&gt; 中的 &lt;em&gt;superised&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He was more superised than she was.&lt;/em&gt; 中的 &lt;em&gt;superised&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 形式出現的詞如果存在以 &lt;em&gt;un-&lt;/em&gt; 開頭的反義形容詞（不可為動詞），則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;a hurried meeting&lt;/em&gt; 中的 &lt;em&gt;hurried&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;an unhurried meeting&lt;/em&gt; 中的 &lt;em&gt;unhurried&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Your shoelace has been untied ever since we started.&lt;/em&gt; 中的 &lt;em&gt;untied&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;It got untied by accident.&lt;/em&gt; 中的 &lt;em&gt;untied&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;We need an armed guard.&lt;/em&gt; 中的 &lt;em&gt;armed&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;We need an unarmed guard.&lt;/em&gt; 中的 &lt;em&gt;unarmed&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Armed with only a knife.&lt;/em&gt; 中的 &lt;em&gt;Armed&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 形式出現的詞如果與 &lt;em&gt;be&lt;/em&gt; 動詞一起出現，且 &lt;em&gt;be&lt;/em&gt; 動詞可以替換成 &lt;em&gt;become&lt;/em&gt;、&lt;em&gt;feel&lt;/em&gt;、&lt;em&gt;look&lt;/em&gt;、&lt;em&gt;remain&lt;/em&gt;、&lt;em&gt;seem&lt;/em&gt;、&lt;em&gt;sound&lt;/em&gt;，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;He is interested.&lt;/em&gt; 中的 &lt;em&gt;interested&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He became interested.&lt;/em&gt; 中的 &lt;em&gt;interested&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He felt interested.&lt;/em&gt; 中的 &lt;em&gt;interested&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He looked surprised.&lt;/em&gt; 中的 &lt;em&gt;surprised&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He remained surprised.&lt;/em&gt; 中的 &lt;em&gt;surprised&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He seemed surprised.&lt;/em&gt; 中的 &lt;em&gt;surprised&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He sounded surprised.&lt;/em&gt; 中的 &lt;em&gt;surprised&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 形式出現的詞如果與 &lt;em&gt;be&lt;/em&gt; 動詞一起出現，且 &lt;em&gt;be&lt;/em&gt; 動詞可以替換成 &lt;em&gt;become&lt;/em&gt;、&lt;em&gt;feel&lt;/em&gt;、&lt;em&gt;look&lt;/em&gt;、&lt;em&gt;remain&lt;/em&gt;、&lt;em&gt;seem&lt;/em&gt;、&lt;em&gt;sound&lt;/em&gt;，且後續有 &lt;em&gt;by&lt;/em&gt; 的出現則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;He remains guided by these principles.&lt;/em&gt; 中的 &lt;em&gt;guided&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 形式出現的詞與 &lt;em&gt;keep&lt;/em&gt; 一起出現時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;They should be kept well watered.&lt;/em&gt; 中的 &lt;em&gt;watered&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 形式出現的詞表達結果或狀態（state or resultant state）而不是事件（event）時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;At the time, I was married.&lt;/em&gt; 中的 &lt;em&gt;married&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I was mistaken the other day.&lt;/em&gt; 中的 &lt;em&gt;mistaken&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a mistaken decision.&lt;/em&gt; 中的 &lt;em&gt;mistaken&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;如果 &lt;em&gt;X-ed N&lt;/em&gt; 不能被改寫成 &lt;em&gt;N that has been X-ed&lt;/em&gt;，則 &lt;em&gt;X-ed&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;a decided advantage&lt;/em&gt; 中的 &lt;em&gt;decided&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a grown woman&lt;/em&gt; 中的 &lt;em&gt;grown&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a married life&lt;/em&gt; 中的 &lt;em&gt;married&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a worried faces&lt;/em&gt; 中的 &lt;em&gt;worried&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;形容詞比較級（adjective，comparative）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJR&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含以 &lt;em&gt;-er&lt;/em&gt; 結尾的形容詞&lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;more&lt;/em&gt;、&lt;em&gt;less&lt;/em&gt; 作為形容詞使用時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJR&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;more or less mail&lt;/em&gt; 中的 &lt;em&gt;more&lt;/em&gt; 與 &lt;em&gt;less&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJR&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;具有比較意義的形容詞但並不是以 &lt;em&gt;-er&lt;/em&gt; 結尾被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;superior&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-er&lt;/em&gt; 結尾但嚴格上來說沒有進行比較標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;further details&lt;/em&gt; 中的 &lt;em&gt;further&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;形容詞最高級（adjective，superlative）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJS&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含以 &lt;em&gt;-est&lt;/em&gt; 結尾的形容詞&lt;/li&gt;
      &lt;li&gt;包含 &lt;em&gt;worst&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;most&lt;/em&gt;、&lt;em&gt;least&lt;/em&gt; 作為形容詞使用時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJS&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;具有最高級意義的形容詞但並不是以 &lt;em&gt;-est&lt;/em&gt; 結尾則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;first&lt;/em&gt;、&lt;em&gt;last&lt;/em&gt;、&lt;em&gt;unsurpassed&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;列表項目符號（list item marker）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LS&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含作為項目符號的字母（letter）&lt;/li&gt;
      &lt;li&gt;包含作為項目符號的數字（numerals）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;情態動詞（modal verb）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MD&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含所有在第三人稱單數型現在式（3rd person singular present）不加上 &lt;em&gt;-s&lt;/em&gt; 的動詞
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;can&lt;/em&gt;、&lt;em&gt;could&lt;/em&gt;、&lt;em&gt;dare&lt;/em&gt;、&lt;em&gt;may&lt;/em&gt;、&lt;em&gt;might&lt;/em&gt;、&lt;em&gt;must&lt;/em&gt;、&lt;em&gt;ought&lt;/em&gt;、&lt;em&gt;shall&lt;/em&gt;、&lt;em&gt;should&lt;/em&gt;、&lt;em&gt;will&lt;/em&gt;、&lt;em&gt;would&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MD&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;名詞單數型 / 不可數名詞（noun，singular or mass）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;顏色（color）作為名詞使用時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;That's a nice red.&lt;/em&gt; 中的 &lt;em&gt;red&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Not too many reds go with that purple.&lt;/em&gt; 中的 &lt;em&gt;reds&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;若名詞結尾為 &lt;em&gt;-s&lt;/em&gt;，但觸發動詞單數型（singular agreement on a verb），則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Linguistics is a difficult field.&lt;/em&gt; 中的 &lt;em&gt;Linguistics&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;語意上為複數，但觸發動詞單數型，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;The group has disbanded.&lt;/em&gt; 中的 &lt;em&gt;group&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;The jury is deliberating.&lt;/em&gt; 中的 &lt;em&gt;group&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;大寫引用詞應標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Chapter&lt;/em&gt;、&lt;em&gt;Exhibit&lt;/em&gt;、&lt;em&gt;Figure&lt;/em&gt;、&lt;em&gt;Table&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;不定代詞（indefinite pronoun）應該標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;naught&lt;/em&gt;、&lt;em&gt;none&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：開頭為 &lt;em&gt;any-&lt;/em&gt;、&lt;em&gt;every-&lt;/em&gt;、&lt;em&gt;no-&lt;/em&gt;、&lt;em&gt;some-&lt;/em&gt; 的詞都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：結尾為 &lt;em&gt;-one&lt;/em&gt;、&lt;em&gt;-thing&lt;/em&gt; 的詞都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;no one&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;no-one&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;作為副詞使用的名詞標記為名詞 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;He comes by Sundays and holidays.&lt;/em&gt; 中 &lt;em&gt;Sundays&lt;/em&gt; 與 &lt;em&gt;holidays&lt;/em&gt; 分別被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS&lt;/code&gt; 與 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;方位可以被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 或 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;，判斷方法為前面有沒有冠詞出現
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;The nearest shopping center is two miles to the north of here.&lt;/em&gt; 中 &lt;em&gt;north&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;The nearest shopping center is two miles north of here.&lt;/em&gt; 中 &lt;em&gt;north&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;yesterday&lt;/em&gt;、&lt;em&gt;today&lt;/em&gt;、&lt;em&gt;tomorrow&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;，理由是他們都可以補上所有格結尾&lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞如果允許複數型則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;The reading for this class is difficult.&lt;/em&gt; 中的 &lt;em&gt;reading&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;The readings for this class is difficult.&lt;/em&gt; 中的 &lt;em&gt;readings&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞作為名詞使用，且可由形容詞修飾，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Good cooking is something to enjoy.&lt;/em&gt; 中的 &lt;em&gt;Good cooking&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Cooking well is a useful skill.&lt;/em&gt; 中的 &lt;em&gt;Cooking well&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞作為名詞使用，且由 &lt;em&gt;of&lt;/em&gt; 片語進行修飾，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;GM's closing of the plant&lt;/em&gt; 中的 &lt;em&gt;closing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;GM's closing the plant&lt;/em&gt; 中的 &lt;em&gt;closing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;em&gt;-ing&lt;/em&gt; 結尾的詞出現在名詞之後時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the plant closing&lt;/em&gt; 中的 &lt;em&gt;plant closing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;unsavory plant closing tactics&lt;/em&gt; 中的 &lt;em&gt;plant closing tactics&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN NN NNS&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;X-ing N&lt;/em&gt; 不含有 &lt;em&gt;N X-es&lt;/em&gt; 的語意時，&lt;em&gt;X-ing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;spending reductions&lt;/em&gt; 中的 &lt;em&gt;spending&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the mating season&lt;/em&gt; 中的 &lt;em&gt;mating&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a holding pattern&lt;/em&gt; 中的 &lt;em&gt;holding&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;專有名詞單數型（proper noun，singular）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;原本標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;，但為了避免與語法標記中的名詞片語（noun phrase）混淆改為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;縮寫應該標記成如同沒有縮寫的內容
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;U.S.&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 hyphenated compound proper noun 作為修飾詞時應標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Gramm-Rudman Act&lt;/em&gt; 中的 &lt;em&gt;Gramm-Rudman&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 hyphenated compound 中第二個位置為專有名詞時應標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;mid-March&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;non-NATO&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;專有名詞複數型（proper noun，plural）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNPS&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;原本標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS&lt;/code&gt;，但為了避免與語法標記中的名詞片語（noun phrase）混淆改為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNPS&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;名詞複數型（noun，plural）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;如果名詞結尾不為 &lt;em&gt;-s&lt;/em&gt;，但觸發動詞雙數型（plural agreement on a verb），則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;The faculty are on strike.&lt;/em&gt; 中的 &lt;em&gt;faculty&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;The police have arrived on the scene.&lt;/em&gt; 中的 &lt;em&gt;police&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當量詞結尾為 &lt;em&gt;-s&lt;/em&gt;，但觸發動詞單數型（singular agreement on a verb），則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Three years is a long time.&lt;/em&gt; 中的 &lt;em&gt;years&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Twelve inches is a foot.&lt;/em&gt; 中的 &lt;em&gt;inches&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;限定詞前置詞（predeterminer）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含所有出現在冠詞（article）之前類似於限定詞的詞
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;both the girls&lt;/em&gt; 中的 &lt;em&gt;both&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;all his marbles&lt;/em&gt; 中的 &lt;em&gt;all&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;many a moon&lt;/em&gt; 中的 &lt;em&gt;many&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;nary a soul&lt;/em&gt; 中的 &lt;em&gt;nary&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;quite a mess&lt;/em&gt; 中的 &lt;em&gt;quite&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;rather a nuisance&lt;/em&gt; 中的 &lt;em&gt;rather&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;such a good time&lt;/em&gt; 中的 &lt;em&gt;such&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含所有出現在代名詞所有格（possessive pronoun）之前類似於限定詞的詞
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;all his marbles&lt;/em&gt; 中的 &lt;em&gt;all&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;half his time&lt;/em&gt; 中的 &lt;em&gt;half&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PDT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;所有格結尾（possessive ending）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POS&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含單數名詞所有格結尾 &lt;em&gt;'s&lt;/em&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;John 's idea&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNP POS NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含複數名詞所有格結尾 &lt;em&gt;'&lt;/em&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the parents ' distress&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT NNS POS NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;代名詞所有格（possessive pronoun）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP$&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含物主形容詞（adjectival possessive pronoun）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;my&lt;/em&gt;、&lt;em&gt;your&lt;/em&gt;、&lt;em&gt;his&lt;/em&gt;、&lt;em&gt;her&lt;/em&gt;、&lt;em&gt;its&lt;/em&gt;、&lt;em&gt;one's&lt;/em&gt;、&lt;em&gt;our&lt;/em&gt;、&lt;em&gt;their&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP$&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;名詞性物主代詞（nominal possessive pronouns）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRP&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;人稱代名詞（personal pronoun）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRP&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;原本標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP&lt;/code&gt;，但為了避免與語法標記中的介系詞片語（prepositional phrase）混淆改為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRP&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;包含主格（subject）代名詞與受格（object）代名詞
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;I&lt;/em&gt;、&lt;em&gt;me&lt;/em&gt;、&lt;em&gt;you&lt;/em&gt;、&lt;em&gt;he&lt;/em&gt;、&lt;em&gt;him&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRP&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含以 &lt;em&gt;-self&lt;/em&gt; 或 &lt;em&gt;-selves&lt;/em&gt; 結尾的反身代名詞（reflexive pronoun）&lt;/li&gt;
      &lt;li&gt;包含名詞性物主代詞（nominal possessive pronouns）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;mine&lt;/em&gt;、&lt;em&gt;yours&lt;/em&gt;、&lt;em&gt;his&lt;/em&gt;、&lt;em&gt;hers&lt;/em&gt;、&lt;em&gt;ours&lt;/em&gt;、&lt;em&gt;theirs&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRP&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;物主形容詞（adjectival possessive pronoun）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP$&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;副詞（adverb）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含 &lt;em&gt;-ly&lt;/em&gt; 結尾的副詞&lt;/li&gt;
      &lt;li&gt;包含程度詞（degree word）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;quite&lt;/em&gt;、&lt;em&gt;too&lt;/em&gt;、&lt;em&gt;very&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含中心詞後置修飾詞（posthead modifiers）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;good enough&lt;/em&gt; 中的 &lt;em&gt;enough&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;very well indeed&lt;/em&gt; 中的 &lt;em&gt;indeed&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含反義詞（negative markers）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;not&lt;/em&gt;、&lt;em&gt;n't&lt;/em&gt;、&lt;em&gt;never&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;有些形容詞會當成副詞使用且沒有 &lt;em&gt;-ly&lt;/em&gt; 結尾，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;rapid growing plants&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB VBG NNS&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;rapid growth&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;地點單獨出現時應標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Call me when you get home.&lt;/em&gt; 中的 &lt;em&gt;home&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Call me when you are at home.&lt;/em&gt; 中的 &lt;em&gt;home&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當動詞與介系詞之間能夠插入方式副詞（manner adverb）時，標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RP&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;to sit calmly by&lt;/em&gt; 中的 &lt;em&gt;calmly by&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;副詞比較級（adverb，comparative）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBR&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;副詞但結尾為 &lt;em&gt;-er&lt;/em&gt;，且嚴格上來說沒有進行比較則被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;come by later&lt;/em&gt; 中的 &lt;em&gt;later&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;副詞最高級（adverb，superlative）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBS&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;most&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBS&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;most every-&lt;/em&gt; 中的 &lt;em&gt;most&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBS&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;助詞（particle）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RP&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;主要由單音節（monosyllabic）的詞組成，有可能會作為方向副詞（directional adverb）或介系詞（preposition）使用&lt;/li&gt;
      &lt;li&gt;如果一個介系詞能夠任意出現在名詞片語（noun phrase）之前或之後，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;She told off her friends.&lt;/em&gt; 中的 &lt;em&gt;off&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;She told her friends off.&lt;/em&gt; 中的 &lt;em&gt;off&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;將介系詞後出現的名詞片語替換成代名詞後，代名詞必須改為出現在介系詞之前，則該介系詞標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;She told them off.&lt;/em&gt; 中的 &lt;em&gt;off&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He peeled it off.&lt;/em&gt; 中的 &lt;em&gt;off&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;註：此規則與前一規則衝突時以此規則為主&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;如果動詞與介系詞合併後能夠成為名詞，則介系詞標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;to break down&lt;/em&gt; 中的 &lt;em&gt;down&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;，因為 &lt;em&gt;breakdown&lt;/em&gt; 是名詞&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;to break through&lt;/em&gt; 中的 &lt;em&gt;through&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;，因為 &lt;em&gt;breakthrough&lt;/em&gt; 是名詞&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;to be left over&lt;/em&gt; 中的 &lt;em&gt;over&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;，因為 &lt;em&gt;leftover&lt;/em&gt; 是名詞&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;to push over&lt;/em&gt; 中的 &lt;em&gt;over&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;，因為 &lt;em&gt;pushover&lt;/em&gt; 是名詞&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;to put down&lt;/em&gt; 中的 &lt;em&gt;down&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;，因為 &lt;em&gt;putdown&lt;/em&gt; 是名詞&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當介系詞為單音節（monosyllabic）、出現在句尾且表重音（stress）則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Why don't you come by?&lt;/em&gt; 中的 &lt;em&gt;by&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Real bargains are hard to come by.&lt;/em&gt; 中的 &lt;em&gt;by&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;雖然助詞與動詞經常一起處現，但也可以與動詞的變形一起出現
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the cutting off of the top&lt;/em&gt; 中 &lt;em&gt;cutting off&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the setting up of the problem&lt;/em&gt; 中 &lt;em&gt;setting up&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN RB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He looks worn out&lt;/em&gt; 中 &lt;em&gt;worn out&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ RP&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;about&lt;/em&gt; 與 &lt;em&gt;around&lt;/em&gt; 表達 &lt;em&gt;approximately&lt;/em&gt; 的語意時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;close to&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB TO&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;closer to&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB TO&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;near to&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB TO&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;nearer to&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB TO&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;badly off&lt;/em&gt;、&lt;em&gt;better off&lt;/em&gt;、&lt;em&gt;well off&lt;/em&gt;、&lt;em&gt;worse off&lt;/em&gt; 中的 &lt;em&gt;off&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RP&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;通用科學符號（symbols）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SYM&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含數學符號、科學記號、科技符號、非英語公式等&lt;/li&gt;
      &lt;li&gt;化學名稱應該被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;測量單位應該被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;to&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TO&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;不論是作為不定式（infinitive）或介系詞（prepositional）都採用相同標記&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;感嘆詞（interjection）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UH&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含驚嘆詞（Exclamation）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;oh&lt;/em&gt;、&lt;em&gt;please&lt;/em&gt;、&lt;em&gt;uh&lt;/em&gt;、&lt;em&gt;well&lt;/em&gt;、&lt;em&gt;yes&lt;/em&gt; 都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UH&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;my&lt;/em&gt; 作為感嘆詞時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UH&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;My, what a gorgeous day&lt;/em&gt; 中的 &lt;em&gt;My&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UH&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;see&lt;/em&gt; 作為感嘆詞時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UH&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;See, it's like this&lt;/em&gt; 中的 &lt;em&gt;See&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UH&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;動詞原型（verb，base form）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含命令句（imperative）的動詞
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Do it.&lt;/em&gt; 中的 &lt;em&gt;Do&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含接在不定式（infinitive）後的動詞
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;You should do it.&lt;/em&gt; 中的 &lt;em&gt;do&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;We want them to do it.&lt;/em&gt; 中的 &lt;em&gt;do&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;We made them do it.&lt;/em&gt; 中的 &lt;em&gt;do&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含現在式假設語氣（subjunctive）中的動詞
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;We suggest that he do it.&lt;/em&gt; 中的 &lt;em&gt;do&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;動詞過去式（verb，past tense）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBD&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含過去式假設語氣（be verb，conditional form）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;If I were rich, …&lt;/em&gt; 中的 &lt;em&gt;were&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBD&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;If I were to win the lottery, …&lt;/em&gt; 中的 &lt;em&gt;were&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBD&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;現在分詞（gerund）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;當 &lt;em&gt;X-ing N&lt;/em&gt; 含有 &lt;em&gt;N X-es&lt;/em&gt; 的語意時，&lt;em&gt;X-ing&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt; 而不是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;the declining productivity of U.S. industry&lt;/em&gt; 中的 &lt;em&gt;declining&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;the acting vice president&lt;/em&gt; 中的 &lt;em&gt;acting&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;過去分詞（past participle）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;如果以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 形式出現的詞可以接著 &lt;em&gt;by&lt;/em&gt;，且不能區分程度（gradable），則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;He was invited by some friends of hers.&lt;/em&gt; 中的 &lt;em&gt;invited&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;He was very surprised by her remarks.&lt;/em&gt; 中的 &lt;em&gt;surprised&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 形式出現的詞表達事件（event）而不是結果或狀態（state or resultant state）時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;I was married on a Sunday.&lt;/em&gt; 中的 &lt;em&gt;married&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I was mistaken for you the other day.&lt;/em&gt; 中的 &lt;em&gt;mistaken&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;a case of mistaken identity.&lt;/em&gt; 中的 &lt;em&gt;mistaken&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 形式出現的詞如果與 &lt;em&gt;be&lt;/em&gt; 動詞一起出現，且 &lt;em&gt;be&lt;/em&gt; 動詞可以替換成 &lt;em&gt;get&lt;/em&gt;，但 &lt;em&gt;be&lt;/em&gt; 動詞不能替換成 &lt;em&gt;become&lt;/em&gt;，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt; 而非 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;I was married on a Sunday.&lt;/em&gt; 中的 &lt;em&gt;married&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I got married.&lt;/em&gt; 中的 &lt;em&gt;married&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I became married.&lt;/em&gt; 中的 &lt;em&gt;married&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;動詞非第三人稱單數現在式（verb，present tense，other than 3rd person singular）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBP&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;動詞第三人稱單數現在式（verb，present tense，3rd person singular）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBZ&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;wh-&lt;/em&gt; 限定詞（&lt;em&gt;wh-&lt;/em&gt; determiner）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;出現在名詞中心語（head noun）之前的 &lt;em&gt;wh-&lt;/em&gt; 詞標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;What kind do you want?&lt;/em&gt; 中的 &lt;em&gt;What&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I don't know what kind do you want.&lt;/em&gt; 中的 &lt;em&gt;what&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Be sure to wash whatever fruit you buy.&lt;/em&gt; 中的 &lt;em&gt;whatever&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Which book do you like better?&lt;/em&gt; 中的 &lt;em&gt;Which&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I don't know which book you like better.&lt;/em&gt; 中的 &lt;em&gt;which&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;Which one do you like better?&lt;/em&gt; 中的 &lt;em&gt;Which&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I don't know which one you like better.&lt;/em&gt; 中的 &lt;em&gt;which&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;which&lt;/em&gt; 或 &lt;em&gt;whichever&lt;/em&gt; 沒有出現在名詞中心語之前時仍然標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Which do you like better?&lt;/em&gt; 中的 &lt;em&gt;Which&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I don't know which you like better.&lt;/em&gt; 中的 &lt;em&gt;which&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I'll get you whichever you want.&lt;/em&gt; 中的 &lt;em&gt;whichever&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;that&lt;/em&gt; 作為關係從句（relative pronoun）使用時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;wh-&lt;/em&gt; 代名詞（&lt;em&gt;wh-&lt;/em&gt; pronoun）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WP&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;當 &lt;em&gt;what&lt;/em&gt; 或 &lt;em&gt;whatever&lt;/em&gt; 沒有出現在名詞中心語（head noun）之前時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WP&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;Tell me what you would like to eat.&lt;/em&gt; 中的 &lt;em&gt;what&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;例如：&lt;em&gt;I'll get you whatever you want.&lt;/em&gt; 中的 &lt;em&gt;whatever&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WP&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;包含 &lt;em&gt;who&lt;/em&gt;、&lt;em&gt;whom&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;wh-&lt;/em&gt; 代名詞所有格（possessive &lt;em&gt;wh-&lt;/em&gt; pronoun）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WP$&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含 &lt;em&gt;whose&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;wh-&lt;/em&gt; 副詞（&lt;em&gt;wh-&lt;/em&gt; adverb）標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WRB&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;包含 &lt;em&gt;how&lt;/em&gt;、&lt;em&gt;where&lt;/em&gt;、&lt;em&gt;why&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;when&lt;/em&gt; 描述時間時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WRB&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;When he finally arrived, I was on my way out.&lt;/em&gt; 中的 &lt;em&gt;When&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WRB&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;當 &lt;em&gt;when&lt;/em&gt; 作為 &lt;em&gt;if&lt;/em&gt; 使用時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;I like it when you make dinner for me.&lt;/em&gt; 中的 &lt;em&gt;when&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;標記流程&lt;/h3&gt;

&lt;p&gt;標記詞性總共分兩個階段，分別為自動化詞性標記（automatic POS assignment）與人工校正（manual correction）。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;採用 PARTS 進行自動化詞性標記
    &lt;ul&gt;
      &lt;li&gt;由 AT &amp;amp; T Bell Labs 開發&lt;/li&gt;
      &lt;li&gt;是一種 stochastic algorithm，error rate 為 $3–5\%$&lt;/li&gt;
      &lt;li&gt;基於 Brown Corpus 的標記做點修改，與 Penn Treebank 的標記風格類似&lt;/li&gt;
      &lt;li&gt;輸出結果進行 tokenization 且轉換成 Penn Treebank 標記，此過程約產生 4% 誤差
        &lt;ul&gt;
          &lt;li&gt;誤差來源包含 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBP&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;誤差來源包含 adverb 與 particle&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;撰寫規則進行自動化詞性標記
    &lt;ul&gt;
      &lt;li&gt;全靠作者的經驗進行規則編寫（沒錯就是大量的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if-else&lt;/code&gt;）&lt;/li&gt;
      &lt;li&gt;將 PARTS 自動化標記過程產生的 $4\%$ 誤差修正為 $2–6\%$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;採用圖形化介面幫助人工校正
    &lt;ul&gt;
      &lt;li&gt;圖形化介面是由 GNU Emacs Lisp 撰寫而成，內嵌在 GNU Emacs editor&lt;/li&gt;
      &lt;li&gt;使用者透過滑鼠點擊不正確的標記，並輸入正確版本（一或多個標籤）&lt;/li&gt;
      &lt;li&gt;使用者的更正結果會與&lt;a href=&quot;#paper-fig-2&quot;&gt;圖 2&lt;/a&gt; 中的標籤進行確認，避免使用者輸入錯誤&lt;/li&gt;
      &lt;li&gt;使用者的更正結果會補在原本的註記上，並標記 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; 幫助作者分析 error rate&lt;/li&gt;
      &lt;li&gt;釋出的版本會以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt; 標記答案為主，並移除所有錯誤答案&lt;/li&gt;
      &lt;li&gt;校正人員經過一個月（15 hrs per week）的時間熟悉校正流程，之後每個月的工作效率約為 3000 words per hr&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;在不使用自動化標記進行輔助時，人工標記時間比校正時間多了 2 倍，標記結果的不一致多了 2 倍，錯誤率多了 50%
    &lt;ul&gt;
      &lt;li&gt;此實驗的參與人員共有 4 位語言學研究生，校正受訓 15 hrs，標記受訓 6 hrs&lt;/li&gt;
      &lt;li&gt;所有標記人員皆熟悉 GNU Emacs&lt;/li&gt;
      &lt;li&gt;標記 Brown Corpus 中 8 個 samples，每個 sample 有 2000 words&lt;/li&gt;
      &lt;li&gt;從 4 個 genres（2 fictions、2 nonfictions）中各抽兩個 samples，samples 必須為標記人員沒在受試過程中接觸過&lt;/li&gt;
      &lt;li&gt;標記人員首先標記 4 個 samples（順序隨機），再修正 4 個自動標記的 samples（順序隨機）&lt;/li&gt;
      &lt;li&gt;不同標記人員的速度沒有顯著差異&lt;/li&gt;
      &lt;li&gt;不同 genres 的標記速度沒有顯著差異&lt;/li&gt;
      &lt;li&gt;標記與校正速度上有顯著差異（$\alpha = 0.05$）：校正速度中位數為 22 mins per 1k words、平均為 20 mins per 1k words；標記中位數為 42 mins per 1k words、平均為 44 mins per 1k words&lt;/li&gt;
      &lt;li&gt;分析標記不一致的細節如下
        &lt;ul&gt;
          &lt;li&gt;假設有 $k$ 個標記詞，有 $n$ 個標記者，任取 $2$ 個標記者比較所有標記結果，總共有 $\binom{n}{2}$ 種取法&lt;/li&gt;
          &lt;li&gt;將 $k$ 個數取平均得到標記不一致的比例，總共有 $\binom{n}{2}$ 個比例&lt;/li&gt;
          &lt;li&gt;標記任務不一致的比例平均值為 $7.2\%$，中位數為 $7.2\%$；校正任務上不一致的比例平均值為 $4.1\%$，中位數為 $3.6\%$&lt;/li&gt;
          &lt;li&gt;檢視結果發現不一致的主因為化學符號，在缺乏明確的指示下標記人員隨意進行標記&lt;/li&gt;
          &lt;li&gt;將化學符號造成的不一致去除後，校正任務不一致的比例平均值降為 $3.5\%$，中位數仍為 $3.6\%$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;分析標記答案與官方答案的差異
        &lt;ul&gt;
          &lt;li&gt;套用分析標記不一致的方法，只是比較的對象改為官方答案&lt;/li&gt;
          &lt;li&gt;標記任務不一致的比例平均值為 $5.4\%$，中位數為 $5.7\%$；校正任務上不一致的比例平均值為 $4.0\%$，中位數為 $3.4\%$&lt;/li&gt;
          &lt;li&gt;將化學符號造成的不一致去除後，校正任務不一致的比例平均值降為 $3.4\%$，中位數仍為 $3.4\%$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;分析自動化標記與官方答案的差異
        &lt;ul&gt;
          &lt;li&gt;套用分析標記答案與官方答案的方法，只是標記答案是自動化產生&lt;/li&gt;
          &lt;li&gt;標記不一致的比例平均值為 $9.6\%$&lt;/li&gt;
          &lt;li&gt;人工校正能夠減少 $4.2\%$ 的標記錯誤，說明人工校正是必要的&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;語法成份分析標記&lt;/h2&gt;

&lt;p&gt;與詞性標記平行進行的語法成份分析（constituent parse）標記流程為 Fidditch 自動化標記加上人工校正而得。&lt;/p&gt;

&lt;h3 id=&quot;fidditch&quot;&gt;Fidditch&lt;/h3&gt;

&lt;p&gt;首先使用 Donald Hindle 在 University of Pennsylvania 與 AT &amp;amp; T Bell Labs 開發的 Fidditch parser 進行自動化標記。
Fidditch 的特性如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fidditch 永遠只會給出一種答案
    &lt;ul&gt;
      &lt;li&gt;因此校正過程不需要分析多個答案&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;當 Fidditch 無法在非常確定的前提下，判斷句子中的部份 constituent 在更大的 constituent 中的定位時，不會給出標記
    &lt;ul&gt;
      &lt;li&gt;這代表 Fidditch 的輸出可能不是完整 constituent&lt;/li&gt;
      &lt;li&gt;校正人員需要把已經分析完的 constituent 組成更大更完整的 constituent，作者說這就好像在把constituent 「黏」（glue）在一起的感覺&lt;/li&gt;
      &lt;li&gt;Fidditch 的表現不錯，因此只要是在非常確定的前提下產出的 constituent 幾乎都是正確的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;無法判斷的結果 Fidditch 會以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;?&lt;/code&gt; 標記
    &lt;ul&gt;
      &lt;li&gt;Fidditch 為 syntatic parser，沒有考慮 semantic or pragmatic information&lt;/li&gt;
      &lt;li&gt;由於判斷 prepositional phrases、relative clauses、adverbial modifiers 時需要考慮資訊遠超過語法（extrasyntatic information），因此 Fidditch 會將這些 constituent 不連結至任何上層 constituent（leaving such constituents unattached），而校正工作就是正確連結未標記的 constituent&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-6&quot;&gt;標記總表&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 3：Penn Treebank 語法成份分析標記總表。
圖片來源：&lt;a href=&quot;https://alliance.seas.upenn.edu/~nlp/publications/pdf/marcus1993.pdf&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/czVasbA.png&quot; alt=&quot;圖 3&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;與 Lancaster Treebank Project 標記種類相似，但 Penn Treebank 允許 null element&lt;/li&gt;
  &lt;li&gt;Null elements 包含了 Fidditch 產出的標記&lt;/li&gt;
  &lt;li&gt;作者認為保留 null element 能夠分析 predicate-argument structure 與 verb transitivity&lt;/li&gt;
  &lt;li&gt;作者認為 Penn Treebank 是以 &lt;strong&gt;context-free grammar&lt;/strong&gt; 作為前提進行標記，因此必須保留 null element&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-7&quot;&gt;標記流程&lt;/h3&gt;

&lt;p&gt;根據觀察發現&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;語法成份分析標記所需的時間遠大於詞性標記時間&lt;/li&gt;
  &lt;li&gt;進行 Fidditch 輸出校正所需時間為
    &lt;ul&gt;
      &lt;li&gt;每小時約 375 個詞（受訓 3 星期）&lt;/li&gt;
      &lt;li&gt;每小時約 475 個詞（受訓 6 星期）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;將 Fidditch 的輸出進一步化簡成類似於 Lancaster UCREL Treebank Project 的風格讓校正速度變快（每小時校正增加約 100-200 個詞）
    &lt;ul&gt;
      &lt;li&gt;移除詞性標記&lt;/li&gt;
      &lt;li&gt;移除 nonbranching lexical nodes&lt;/li&gt;
      &lt;li&gt;移除特定 phrase nodes（主要為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NBAR&lt;/code&gt;）&lt;/li&gt;
      &lt;li&gt;當 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ADJP&lt;/code&gt; 出現在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt; 內，且 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ADJP&lt;/code&gt; 不是 coordinate structure 的一部份時，移除 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ADJP&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;由於 verb's arguments 與 verb's adjuncts 很難區分，當允許校正人員忽略差異時讓校正速度變快（每小時校正增加約 150-200 個詞）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此作者決定標記採用&lt;strong&gt;化簡風格&lt;/strong&gt;（&lt;strong&gt;skeletal syntactic structure&lt;/strong&gt;），且不強制區分 verb's arguments 與 verb's adjuncts。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;共 5 位兼職標記人員，平均每人每小時校正約 750 個詞&lt;/li&gt;
  &lt;li&gt;最有效率的標記人員每小時校正約 1500 個詞，且中間還穿插短暫休息時間&lt;/li&gt;
  &lt;li&gt;每天工作 3 小時，估算一年能標記約 2.5M 個詞&lt;/li&gt;
  &lt;li&gt;有經驗的標記者能夠非常快速的驗證標記結果，每人每小時約可驗證 4000 個詞&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;為了增加標記效率，Penn Treebank 額外使用了兩種標記：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt; constituent
    &lt;ul&gt;
      &lt;li&gt;當標記者認為某個片段為 constituent，但不確定應該標記成什麼，則可以暫時寫為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt; constituent&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pseudo-attachment
    &lt;ul&gt;
      &lt;li&gt;在給予前後文的情況下仍然無法確定一個 constituent 應該被連結到哪一個上層的結構時稱為 permanent predictable ambiguities，並標記為 pseudo-attachment&lt;/li&gt;
      &lt;li&gt;有些情況中一個 constituent 修飾的對象不只一個，此情況出現時可以使用 pseudo-attachment 標記，但絕大多數情況標記都是以 context-free grammar 出發，即一個 constituent 只能連接一個上層 constituent&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-8&quot;&gt;標記結果&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 4：Penn Treebank 標記結果。
圖片來源：&lt;a href=&quot;https://alliance.seas.upenn.edu/~nlp/publications/pdf/marcus1993.pdf&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/JVj3svh.png&quot; alt=&quot;圖 4&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;所有資料都可由 &lt;a href=&quot;https://catalog.ldc.upenn.edu/LDC93T1&quot;&gt;LDC&lt;/a&gt; 取得&lt;/li&gt;
  &lt;li&gt;Department of Energy abstracts 是科學研究的摘要&lt;/li&gt;
  &lt;li&gt;The Department of Agriculture materials 包含種花的時機、如何將蔬菜水果做成罐頭等的文章&lt;/li&gt;
  &lt;li&gt;The Library of America texts 的文章段落約為 5000-10000 詞，主要都是書本的章節內容&lt;/li&gt;
  &lt;li&gt;MUC-3 都是 Federal News Service 的新聞內容，主題為南美的恐怖份子行動，包含部份西班牙文新聞翻譯&lt;/li&gt;
  &lt;li&gt;IBM Manual sentences 是 IBM 電腦的參考手冊，字典大小約為 3000&lt;/li&gt;
  &lt;li&gt;ATIS sentences 是由 DARPA Air Travel Information System project 轉錄而得&lt;/li&gt;
  &lt;li&gt;Brown Corpus 完全依照 Penn Treebank 的規則重新進行標記&lt;/li&gt;
  &lt;li&gt;作者估算 POS 標記誤差為 $3\%$&lt;/li&gt;
&lt;/ul&gt;</content><author><name>[&quot;Mitchell Marcus&quot;, &quot;Beatrice Santorini&quot;, &quot;Mary Ann Marcinkiewicz&quot;]</name></author><category term="Dataset" /><category term="Penn Treebank" /><category term="part of speech" /><category term="constituent parse" /><summary type="html">目標 建立大型文字標記資料集，基於 Brown Corpus 的詞性標記作修改 作者 Mitchell Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz 隸屬單位 University of Pennsylvania 期刊/會議名稱 Computational Linguistics 發表時間 1993 論文連結 https://alliance.seas.upenn.edu/~nlp/publications/pdf/marcus1993.pdf 詞性標記參考手冊 https://repository.upenn.edu/cgi/viewcontent.cgi?article=1603&amp;amp;context=cis_reports 語法成份分析標記參考手冊 http://languagelog.ldc.upenn.edu/myl/PennTreebank1995.pdf（找不到原版）</summary></entry><entry><title type="html">A Standard Corpus of Edited Present-Day American English</title><link href="/dataset/2022/08/06/a-standard-corpus-of-edited-present-day-american-english.html" rel="alternate" type="text/html" title="A Standard Corpus of Edited Present-Day American English" /><published>2022-08-06T02:07:00+08:00</published><updated>2022-08-06T02:07:00+08:00</updated><id>/dataset/2022/08/06/a-standard-corpus-of-edited-present-day-american-english</id><content type="html" xml:base="/dataset/2022/08/06/a-standard-corpus-of-edited-present-day-american-english.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;建立大型文字標記資料集&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;W. Nelson Francis&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隸屬單位&lt;/td&gt;
      &lt;td&gt;University of Brown&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;College English&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;1964&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.jstor.org/stable/373638&quot;&gt;https://www.jstor.org/stable/373638&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;參考手冊&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM&quot;&gt;http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section&quot;&gt;重點&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;建立大型文字標記資料集，後續標記都以 Brown Corpus 為參考基準&lt;/li&gt;
  &lt;li&gt;蒐集文字對象為 edited American English，prepared for print&lt;/li&gt;
  &lt;li&gt;所有詞都被標記詞性與語法結構&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;蒐集文字&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 1：蒐集來源與編碼
圖片來源：&lt;a href=&quot;https://www.jstor.org/stable/373638&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/FkZo2e9.png&quot; alt=&quot;圖 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;起始時間：1961&lt;/li&gt;
  &lt;li&gt;蒐集對象：編輯後準備發表的美語文字（edited American English，prepared for print）
    &lt;ul&gt;
      &lt;li&gt;這代表蒐集文字中不包含口語（spoken English）、信件（personal letters）、student themes、油印稿（ephemeral mimeographed material）&lt;/li&gt;
      &lt;li&gt;所有蒐集資料都是來自 USA 出版物，假設都是由美國人寫的，無法保證假設為真&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;特定文字會被濾除，包含
    &lt;ul&gt;
      &lt;li&gt;詩（verse），但如果是教材則納入蒐集&lt;/li&gt;
      &lt;li&gt;戲劇（drama）&lt;/li&gt;
      &lt;li&gt;包含大量數學與科學符號的文字（mathematical and scientific writing so laden with formulas as to be hardly linguistic discourse at all）&lt;/li&gt;
      &lt;li&gt;對話數量超過 50% 以上的小說（fiction that contains more than 50 per cent dialogue）&lt;/li&gt;
      &lt;li&gt;色情文學（outright pornography）&lt;/li&gt;
      &lt;li&gt;備註（footnotes）&lt;/li&gt;
      &lt;li&gt;參考書目（bibliographies）&lt;/li&gt;
      &lt;li&gt;圖片標題（picture captions）&lt;/li&gt;
      &lt;li&gt;表格（tables）&lt;/li&gt;
      &lt;li&gt;意會型與偶發性用語（illustrative and incidental linguistic material）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;目標蒐集總詞數為 1M，拆成 500 個範例，每個範例中包含 2000 詞&lt;/li&gt;
  &lt;li&gt;蒐集來源共分 15 個種類，主要種類會給予一個編碼（code letter），種類與細節見&lt;a href=&quot;#paper-fig-1&quot;&gt;圖 1&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;絕大部分樣本母體來自兩個圖書館 Brown Univarsity Library 與 the Providence Athenaeum 館中的收藏&lt;/li&gt;
      &lt;li&gt;針對 daily press 樣本母體來自於 New York Public Library&lt;/li&gt;
      &lt;li&gt;針對類別 E. Skills and Hobbies 樣本母體來自於紐約數一數二大的二手雜誌書店&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;每個子類別的抽樣方法為
    &lt;ol&gt;
      &lt;li&gt;對子類別的總文章數作 uniform sampling 選出文章&lt;/li&gt;
      &lt;li&gt;對選出文章的總頁碼進行 uniform sampling 選出起始頁面&lt;/li&gt;
      &lt;li&gt;從起始頁面中的第一個完整句子開始紀錄連續文字，紀錄總詞數控制在約為 2000 次，當滿足 2000 詞後看到完整的句子結尾便停止紀錄&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;詞的定義為前後有空白出現的連續字母序列
    &lt;ul&gt;
      &lt;li&gt;數學與化學公式會用特殊符號代替，並當成一個詞&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;句子的定義為
    &lt;ul&gt;
      &lt;li&gt;由一連串的詞與結尾所組成&lt;/li&gt;
      &lt;li&gt;第一個詞中的第一個字母為大寫&lt;/li&gt;
      &lt;li&gt;結尾的定義為符號 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.!?&lt;/code&gt; 加上空白加上大寫字母&lt;/li&gt;
      &lt;li&gt;縮寫不會被當成結尾&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;版權（copyright）問題
    &lt;ul&gt;
      &lt;li&gt;除了政府資料外，所有資料都有版權所屬者&lt;/li&gt;
      &lt;li&gt;作者寄信給所有版權擁有者，絕大部分的人都迅速表示同意授權甚至對研究很有興趣&lt;/li&gt;
      &lt;li&gt;由於希望不需給付版權費，因此部份樣本有重新抽樣&lt;/li&gt;
      &lt;li&gt;少部份人表示 computer tape 的版權狀態當前無法評估，作者與版權專家都表示無奈（哇塞超級版權砲）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;針對資料的編碼採用 U.S. Patent Office 提出的標準 “Notation System for Transliterating Technical and Scientific Texts for Use in Data Processing Systems”
    &lt;ul&gt;
      &lt;li&gt;由於該標準是針對專利文件，因此作者提出了一些修改方便使用&lt;/li&gt;
      &lt;li&gt;所有修改細節請見&lt;a href=&quot;http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM&quot;&gt;參考手冊&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;所有複製資料都會附贈參考手冊&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;作者有額外撰寫移除標點符號的程式，目的是方便計算詞頻
    &lt;ul&gt;
      &lt;li&gt;移除的標點符號不包含 internal hyphen、apostrophes、diacritics 等&lt;/li&gt;
      &lt;li&gt;提供有包含程式碼與不包含程式碼的磁帶&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;標記&lt;/h2&gt;

&lt;p&gt;原始版本並不包含標記，後續版本（版本 C）才額外補上標記。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;專有名詞的大寫保留&lt;/li&gt;
  &lt;li&gt;包有語法結構的標點符號保留，其他一律捨棄&lt;/li&gt;
  &lt;li&gt;標記類別數為 81&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;標記分成 6 大類別：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mayor form-classes（主要詞彙）
    &lt;ul&gt;
      &lt;li&gt;具體上就是指 open lexcial classes（開放式詞彙），open 的意思是指常有新的詞彙加入&lt;/li&gt;
      &lt;li&gt;此類別包含
        &lt;ul&gt;
          &lt;li&gt;Noun（名詞），可細分為 common noun（一般名詞）或 proper noun（專有名詞）&lt;/li&gt;
          &lt;li&gt;Verb（動詞）&lt;/li&gt;
          &lt;li&gt;Adjective（形容詞）&lt;/li&gt;
          &lt;li&gt;Adverb（副詞）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Function words（功能詞）
    &lt;ul&gt;
      &lt;li&gt;範圍包含 closed lexcial classes（封閉式詞彙）與 grammatical classes（語法詞彙），closed 的意思是指不常有新的詞彙加入&lt;/li&gt;
      &lt;li&gt;此類別包含
        &lt;ul&gt;
          &lt;li&gt;Determiners（限定詞）&lt;/li&gt;
          &lt;li&gt;Prepositions（介系詞）&lt;/li&gt;
          &lt;li&gt;Conjunctions（連接詞）&lt;/li&gt;
          &lt;li&gt;Pronouns（代名詞）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Certain important individual words（特殊詞）
    &lt;ul&gt;
      &lt;li&gt;（反義副詞）&lt;em&gt;not&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Existential（存在副詞） &lt;em&gt;there&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Infinitival（不定式）&lt;em&gt;to&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;do&lt;/em&gt;、&lt;em&gt;be&lt;/em&gt;、&lt;em&gt;have&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Punctuation marks of syntactic significance（含有重要語法結構標點符號）&lt;/li&gt;
  &lt;li&gt;Inflectional morphemes（詞根不變的變形）
    &lt;ul&gt;
      &lt;li&gt;屬於此類的 noun 包含 noun plural（複數名詞）與 possessive（所有格）&lt;/li&gt;
      &lt;li&gt;屬於此類的 verb 包含 past tense（過去式）、present participle（現在分詞）、past participle（過去分詞）與 3rd person singular concord marker（第三人稱單數型）&lt;/li&gt;
      &lt;li&gt;屬於此類的 adjective 包含 comparative adjective（比較級）與 superlative adjective（最高級）&lt;/li&gt;
      &lt;li&gt;屬於此類的 adverb 包含 adverb suffix（副詞後綴）&lt;/li&gt;
      &lt;li&gt;Brown Corpus 對此類別進行以下編碼
        &lt;ul&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$&lt;/code&gt; 代表 possessive（所有格）&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;D&lt;/code&gt; 代表 past tense（過去式）&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;G&lt;/code&gt; 代表 present participle or gerund（現在分詞）&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S&lt;/code&gt; 代表 plural（複數）&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;N&lt;/code&gt; 代表 past participle（過去分詞）&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O&lt;/code&gt; 代表 objective case of pronoun（代名詞受格）&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; 代表 comparative（比較級）&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T&lt;/code&gt; 代表 superlative（最高級）&lt;/li&gt;
          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Z&lt;/code&gt; 代表 3rd singular verb（動詞第三人稱單數型）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;外語及引用詞
    &lt;ul&gt;
      &lt;li&gt;用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FM&lt;/code&gt; 代表 foreign word（外語）&lt;/li&gt;
      &lt;li&gt;用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NC&lt;/code&gt; 代表 cited word（引用詞）&lt;/li&gt;
      &lt;li&gt;兩者會用 hyphen 與其他標籤結合&lt;/li&gt;
      &lt;li&gt;當詞出現在 headline（標題）時會補上 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-HL&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;當詞出現在 title（頭銜）時會補上 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-TL&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;完整標記流程需參考 Automatic Grammatical Tagging of English, by Barbara B. Greene and Gerald M. Rubin (Providence: Brown Univ., 1971.)。
我找不到該份文件的連結，可能沒被數位化。&lt;/p&gt;

&lt;h3 id=&quot;noun-phrase&quot;&gt;Noun Phrase&lt;/h3&gt;

&lt;p&gt;Noun pharse（名詞片語）由 determiner sector（限定詞片段）+ modifier sector（修飾詞片段）+ head（頭）所組成。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The (noun phrase) model for this consists of a head preceded by a determiner sector and a modifier sector.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;determiner-sector&quot;&gt;Determiner Sector&lt;/h4&gt;

&lt;p&gt;Determiner sector 的中心為 determiner，主要包含三個種類：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Article（冠詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;a&lt;/em&gt;、&lt;em&gt;an&lt;/em&gt;、&lt;em&gt;the&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AT&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Deictics（指事詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;this&lt;/em&gt;、&lt;em&gt;that&lt;/em&gt;、&lt;em&gt;another&lt;/em&gt;、&lt;em&gt;each&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Deictics with plurals（複數指事詞）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;these&lt;/em&gt;、&lt;em&gt;those&lt;/em&gt;&lt;/li&gt;
          &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTS&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Dual deictics（雙數指事詞）
        &lt;ul&gt;
          &lt;li&gt;例如：&lt;em&gt;either&lt;/em&gt;、&lt;em&gt;neither&lt;/em&gt;&lt;/li&gt;
          &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTX&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Dual detictics 也常常作為 correlative conjunctions（相關連接詞）使用&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Quantifiers not marked for number（不具體描述數量的量詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;some&lt;/em&gt;、&lt;em&gt;any&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTI&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;出現在 determiner 之前的詞包含：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pre-quantifiers（限定詞前置量詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;all&lt;/em&gt;、&lt;em&gt;half&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABN&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Both（描述雙數）
    &lt;ul&gt;
      &lt;li&gt;此類別只有 &lt;em&gt;both&lt;/em&gt; 一個詞&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABX&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;有時作為 correlative conjunctions 使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;出現在 determinier 之後，modifier 之前的詞包含：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Post-determinier（限定詞後置詞）
    &lt;ul&gt;
      &lt;li&gt;主要都是 quantifier（量詞），例如：&lt;em&gt;many&lt;/em&gt;、&lt;em&gt;more&lt;/em&gt;、&lt;em&gt;most&lt;/em&gt;、&lt;em&gt;several&lt;/em&gt;、&lt;em&gt;single&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;也包含 particularizers，例如：&lt;em&gt;past&lt;/em&gt;、&lt;em&gt;next&lt;/em&gt;、&lt;em&gt;some&lt;/em&gt;、&lt;em&gt;only&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AP&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cardinal numerals（基數）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;one&lt;/em&gt;、&lt;em&gt;two&lt;/em&gt;、&lt;em&gt;three&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CD&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ordinal numerals（序數）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;first&lt;/em&gt;、&lt;em&gt;second&lt;/em&gt;、&lt;em&gt;third&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OD&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Possessive nouns and pronouns（所有格名詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;cat's&lt;/em&gt;、&lt;em&gt;his&lt;/em&gt;、&lt;em&gt;mine&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記結尾加上 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;modifier-sector&quot;&gt;Modifier Sector&lt;/h4&gt;

&lt;p&gt;Modifier sector 最簡單的形式是以三種類別構成：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Adjectives（由形容詞組成修飾詞），包含
    &lt;ul&gt;
      &lt;li&gt;Positive adjectives（一般形容詞），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Comparative adjectives（比較級形容詞），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJR&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Superlative adjectives（最高級容詞），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJT&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Participles（由分詞組成修飾詞），包含
    &lt;ul&gt;
      &lt;li&gt;Present participles（現在分詞），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Past participles（過去分詞），標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Nominals（由名稱組成修飾詞），標記與 head 規則相同&lt;/li&gt;
  &lt;li&gt;Compounding（由多個詞複合組成修飾詞），標記規則複雜&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由 Adjective 組成的 modifier 前後可以加上以下內容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Adjective may be modified by qualifiers（在形容詞前面加上程度詞修飾）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;rather&lt;/em&gt;、&lt;em&gt;very&lt;/em&gt;、&lt;em&gt;too&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QL&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Adjective may be followd by the post-qualifiers（在形容詞後面加上程度詞修飾）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;enough&lt;/em&gt;、&lt;em&gt;indeed&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QLP&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In general, adverbs in &lt;em&gt;-ly&lt;/em&gt; immediately preceding and clearly qualifying an adjective or adverb are commonly tagged &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QL&lt;/code&gt; rather than the general adverb tag &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;.
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;exceedingly&lt;/em&gt;、&lt;em&gt;sufficiently&lt;/em&gt;、&lt;em&gt;terribly&lt;/em&gt;、&lt;em&gt;unusually&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Certain adjectives which are semantically superlative and thus never compared are given the tag &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJS&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;chief&lt;/em&gt;、&lt;em&gt;head&lt;/em&gt;、&lt;em&gt;main&lt;/em&gt;、&lt;em&gt;prime&lt;/em&gt;、&lt;em&gt;principal&lt;/em&gt;、&lt;em&gt;single&lt;/em&gt;、&lt;em&gt;top&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Compound words（複合詞）作為修飾詞的規則超級複雜，原因是複合詞的結構多變，因此規則也伴隨結構進行探討。&lt;/p&gt;

&lt;p&gt;首先區分複合詞的組成方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open compound（多個詞以空格相隔組成一個片語）&lt;/li&gt;
  &lt;li&gt;Hyphened compound（ hyphen 串接多個詞組成一個片語）&lt;/li&gt;
  &lt;li&gt;Closed compound（多個詞去除空格組成一個片語）&lt;/li&gt;
  &lt;li&gt;Adjunction（多個詞以空格相隔組成一個片語，但彼此之間沒有關聯）&lt;/li&gt;
  &lt;li&gt;Affixation（加上綴字）
    &lt;ul&gt;
      &lt;li&gt;英文包含 Preffixation（加上前綴）與 Suffixation（加上後綴）&lt;/li&gt;
      &lt;li&gt;英文沒有 Inffixation（任意位置加上綴字）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以下列出複合詞作為修飾詞的標記方法：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;如果複合詞是 hyphened compound，且在去除 hyphen 後剩餘的所有詞是一個合法 noun phrase，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;long-range&lt;/em&gt;、&lt;em&gt;high-energy&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如果複合詞是由單詞 + &lt;em&gt;-ed&lt;/em&gt; 組成，且去除 &lt;em&gt;-ed&lt;/em&gt; 後為 verb，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;united&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如果複合詞是由多個詞 + &lt;em&gt;-ed&lt;/em&gt; 組成，且去除 &lt;em&gt;-ed&lt;/em&gt; 後為 verb，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;downgraded&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如果複合詞是由單詞 + &lt;em&gt;-ing&lt;/em&gt; 組成，且去除 &lt;em&gt;-ing&lt;/em&gt; 後為 verb，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;outdistancing&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如果複合詞是由多個詞 + &lt;em&gt;-ing&lt;/em&gt; 組成，且去除 &lt;em&gt;-ing&lt;/em&gt; 後為 verb，則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;double-crossing&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;在 2345 的規則下有例外，當詞由 qualifier 修飾，則被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;very tired&lt;/em&gt;、&lt;em&gt;rather entertaining&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Words normally nouns appearing in the immediate prenomial position（名稱之前的詞）會被當成 noun-adjunct，每個詞都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;army officer&lt;/em&gt;、&lt;em&gt;weather report&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;不符合上述規則的都被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;，因此 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt; 類別會包含超大量且規則複雜的內容，以下舉例
    &lt;ul&gt;
      &lt;li&gt;Words ending in &lt;em&gt;-type&lt;/em&gt;：&lt;em&gt;sandwich-type&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Noun-Adjective combinations：&lt;em&gt;fancy-free&lt;/em&gt;、&lt;em&gt;screw-loose&lt;/em&gt;、&lt;em&gt;shoulder-high&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Noun-Present Participle constructions：&lt;em&gt;run-scoring&lt;/em&gt;、&lt;em&gt;sales-building&lt;/em&gt;、&lt;em&gt;law-abiding&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Noun-Past Participle constructions：&lt;em&gt;home-made&lt;/em&gt;、&lt;em&gt;rock-strewn&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Noun-Noun+&lt;em&gt;-ed&lt;/em&gt; combinations：&lt;em&gt;shirt-sleeeved&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Adjective-Noun+&lt;em&gt;-ed&lt;/em&gt; combinations：&lt;em&gt;short-skirted&lt;/em&gt;、&lt;em&gt;slim-waisted&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Miscellaneous combinations：&lt;em&gt;show-offy&lt;/em&gt;、&lt;em&gt;signal-to-noise&lt;/em&gt;、&lt;em&gt;smash-'em-down&lt;/em&gt;、&lt;em&gt;snob-clannish&lt;/em&gt;、&lt;em&gt;topsy-turvy&lt;/em&gt;、&lt;em&gt;to-the-death&lt;/em&gt;、&lt;em&gt;tongue-in-cheek&lt;/em&gt;、&lt;em&gt;too-simple-to-be-true&lt;/em&gt;、&lt;em&gt;unique-ingrown-screwedup&lt;/em&gt;、&lt;em&gt;round-the-clock&lt;/em&gt;、&lt;em&gt;day-after-day&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;head&quot;&gt;Head&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Singular noun（名詞單數型）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Plural noun（名詞複數型）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Possessive noun（名詞所有格）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Possessive plural noun（名詞所有格複數型）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Proper noun（專有名詞）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Plural proper noun（專有名詞複數型）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Possessive proper noun（專有名詞所有格）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP$&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Possessive plural proper noun（專有名詞所有格複數型）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS$&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;verbal-phrase&quot;&gt;Verbal Phrase&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Verbs in the base form（動詞原型）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Verbs in the 3rd person singular inflected form（動詞第三人稱單數型）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBZ&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Verbs in the past tense（動詞過去式）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBD&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Verbs in the past participle（動詞過去分詞）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Verbs in the present participle（動詞現在分詞）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Modal auxiliary verbs（情態助動詞）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MD&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;不論時態都標記成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MD&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;例如：&lt;em&gt;can&lt;/em&gt;、&lt;em&gt;could&lt;/em&gt;、&lt;em&gt;may&lt;/em&gt;、&lt;em&gt;might&lt;/em&gt;、&lt;em&gt;shall&lt;/em&gt;、&lt;em&gt;should&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;be&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BE&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;不論是作為 auxiliary verb（助動詞）或 full verb（一般動詞）都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BE&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;were&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BED&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;was&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEDZ&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;being&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEG&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;am&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEM&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;been&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEN&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;are&lt;/em&gt;、&lt;em&gt;art&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BER&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;is&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEZ&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;have&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HV&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;不論是作為 auxiliary verb（助動詞）或 full verb（一般動詞）都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HV&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;had&lt;/em&gt;（past tense）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVD&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;having&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVG&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;had&lt;/em&gt;（past participle）被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVN&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;has&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVZ&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;do&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DO&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;不論是作為 auxiliary verb（助動詞）或 full verb（一般動詞）都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DO&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;did&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DOD&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;does&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DOZ&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;doing&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;done&lt;/em&gt; 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Contracted forms of auxiliaries（助動詞縮寫）會與 subject（主詞）一起標記，標記的方式為 subject + auxiliary
    &lt;ul&gt;
      &lt;li&gt;例如：I'm 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPSS+BEM&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;例如：you've 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPSS+HV&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;例如：he'd 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPS+MD&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Contracted negatives（反義縮寫）標記方式為 auxiliary + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;例如：can't 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MD*&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Condensed forms in dialogue（對話縮寫）會以原始結構進行標記
    &lt;ul&gt;
      &lt;li&gt;例如：gonna 被標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG+TO&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pronoun&quot;&gt;Pronoun&lt;/h3&gt;

&lt;p&gt;Personal pronouns（人身代名詞）的標記皆由 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP&lt;/code&gt; 開頭，額外接上一個字母代表不同的情況。
額外的字母可以是 case（主格、所有格、受格）、concord（與動詞結合的變化）或 number（第一、二、三人稱）。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3rd person singular nominative pronoun（第三人稱單數代名詞主格）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;he&lt;/em&gt;、&lt;em&gt;she&lt;/em&gt;、&lt;em&gt;it&lt;/em&gt;、&lt;em&gt;one&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPS&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Nominative personal pronoun other than 3rd person singular（非第三人稱單數代名詞主格）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;I&lt;/em&gt;、&lt;em&gt;we&lt;/em&gt;、&lt;em&gt;they&lt;/em&gt;、&lt;em&gt;you&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPSS&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Objective personal pronoun（任意人稱代名詞受格）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;me&lt;/em&gt;、&lt;em&gt;him&lt;/em&gt;、&lt;em&gt;it&lt;/em&gt;、&lt;em&gt;them&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPO&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Possessive personal pronoun（第一人稱所有格）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;my&lt;/em&gt;、&lt;em&gt;our&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP$&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Second (nominal) possessive pronoun
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;mine&lt;/em&gt;、&lt;em&gt;ours&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP$$&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Singular reflexive / intensive personal pronoun（單數反身代名詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;myself&lt;/em&gt;、&lt;em&gt;yourself&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPL&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Plural reflexive / intensive personal pronoun（複數反身代名詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;ourselves&lt;/em&gt;、&lt;em&gt;themselves&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPLS&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Interrogative pronoun（疑問代詞）與 relative pronoun（相對代詞）都由 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WP&lt;/code&gt; 作為標記開頭
    &lt;ul&gt;
      &lt;li&gt;作為主詞則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WPS&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;作為受詞則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WPO&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Indefinite pronouns（不定代名詞）：由 &lt;em&gt;any-&lt;/em&gt;、&lt;em&gt;every-&lt;/em&gt;、&lt;em&gt;no-&lt;/em&gt;、&lt;em&gt;some-&lt;/em&gt; 組合而成的複合詞
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;anyone&lt;/em&gt;、&lt;em&gt;everyone's&lt;/em&gt;、&lt;em&gt;nobody&lt;/em&gt;、&lt;em&gt;somebody's&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;沒有 &lt;em&gt;'s&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PN&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;有 &lt;em&gt;'s&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PN$&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Demonstrative pronouns（指事代詞）被當成 determiners（限定詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;this&lt;/em&gt;、&lt;em&gt;that&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Singular determiner 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Singular or plural determiner 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTI&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Plural determiner 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTS&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adverbials&quot;&gt;Adverbials&lt;/h3&gt;

&lt;p&gt;Adverbials（副詞片語）可以由單個 adverb（副詞）組成或是由多個詞組成，用以描述或修飾 verbs、adjectives or clauses。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Adverb（副詞）
    &lt;ul&gt;
      &lt;li&gt;例如：he swam &lt;em&gt;fast&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Inflectional comparative adverb
    &lt;ul&gt;
      &lt;li&gt;例如：he swam &lt;em&gt;faster&lt;/em&gt; than another swimmer&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBR&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Inflectional superlative adverb
    &lt;ul&gt;
      &lt;li&gt;例如：he swim &lt;em&gt;fastest&lt;/em&gt; among others&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBT&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Nominal adverb（名詞性副詞），主要與時間或地點相關，本身為 adverb 卻常常作為 nominals 使用
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;here&lt;/em&gt;、&lt;em&gt;then&lt;/em&gt;、&lt;em&gt;indoors&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RN&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Advervial nouns（副詞性名詞），主要與時間或地點相關，本身為 noun 卻常常作為 adverbial 使用
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;home&lt;/em&gt;、&lt;em&gt;east&lt;/em&gt;、&lt;em&gt;Tuesday&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NR&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在區分 article 與 particle（助詞）時，作者認為需要同時考量語法與語意才有辦法正確標記，因此以合成詞（portmanteau）的方式創造標記 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RP&lt;/code&gt;，含意為 adverb or particle，用來標記可為兩者的詞。
此類別包含十個詞 &lt;em&gt;about&lt;/em&gt;、&lt;em&gt;across&lt;/em&gt;、&lt;em&gt;down&lt;/em&gt;、&lt;em&gt;in&lt;/em&gt;、&lt;em&gt;off&lt;/em&gt;、&lt;em&gt;on&lt;/em&gt;、&lt;em&gt;out&lt;/em&gt;、&lt;em&gt;over&lt;/em&gt;、&lt;em&gt;through&lt;/em&gt;、&lt;em&gt;up&lt;/em&gt;。
當這些詞作為 preposition 使用時則標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;connectives&quot;&gt;Connectives&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Coordinating conjunction（對等連接詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;and&lt;/em&gt;、&lt;em&gt;or&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Subordinators（從屬子句連接詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;since&lt;/em&gt;、&lt;em&gt;because&lt;/em&gt;、&lt;em&gt;if&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CS&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Prepositions（介系詞）
    &lt;ul&gt;
      &lt;li&gt;例如：&lt;em&gt;in&lt;/em&gt;、&lt;em&gt;on&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;to&lt;/em&gt; 當成 infinitive marker 時標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TO&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;miscellaneous-items&quot;&gt;Miscellaneous Items&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The existential subject &lt;em&gt;there&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EX&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;與作為副詞時使用進行區分&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Exclamations（驚嘆詞）
    &lt;ul&gt;
      &lt;li&gt;大部份只出現在對話&lt;/li&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UH&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;not&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;在與動詞合成時標記也會合成&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;capitalized-words-titles-and-proper-nouns&quot;&gt;Capitalized Words, Titles, and Proper Nouns&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;所有句子開頭的大寫都轉換成小寫，除了本來就是大寫的詞保留&lt;/li&gt;
  &lt;li&gt;在句子中的某些詞有可能因為作者的強調或其他因素而採用大寫，出現位置為隨機，因此保留大寫但不改變標記結果&lt;/li&gt;
  &lt;li&gt;Proper nouns（專有名詞）的大小寫保留
    &lt;ul&gt;
      &lt;li&gt;單數標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;複數標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;單數所有格標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP$&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;複數所有格標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS$&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;外語會以 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FW-&lt;/code&gt; 作為標記開頭&lt;/li&gt;
  &lt;li&gt;大部份出現在頭銜的詞都會在原始標記上加入 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-TL&lt;/code&gt; 的標記
    &lt;ul&gt;
      &lt;li&gt;出現在頭銜的詞幾乎都是大寫開頭，除了 prepositions、conjunctions 與 pronouns&lt;/li&gt;
      &lt;li&gt;有些外語（例如法文）在頭銜不會大寫&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;專有名詞可以由多個詞合成，標記的規則如下
    &lt;ol&gt;
      &lt;li&gt;只要是人名都標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;組成地理詞彙的名詞在給予基本標記後補上 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-TL&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;組成地理詞彙的專有名詞標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP-TL&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;頭銜如 &lt;em&gt;Mr.&lt;/em&gt;、&lt;em&gt;Mrs.&lt;/em&gt;、&lt;em&gt;Ms.&lt;/em&gt;、&lt;em&gt;Miss&lt;/em&gt;、&lt;em&gt;Sir&lt;/em&gt; 標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;同時擁有作為名詞、形容詞等功能的其他人類頭銜會在標記加上 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-TL&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;不常見的外語頭銜標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;標記總表&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Tag&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Examples&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;sentence closer&lt;/td&gt;
      &lt;td&gt;. ; ? !&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;left parenthesis&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;)&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;right parenthesis&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;not&lt;/em&gt;, &lt;em&gt;n't&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;dash&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;,&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;comma&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;colon&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABL&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;pre-qualifier&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;quite&lt;/em&gt;, &lt;em&gt;rather&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABN&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;pre-quantifier&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;half&lt;/em&gt;, &lt;em&gt;all&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ABX&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;pre-quantifier&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;both&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AP&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;post-determiner&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;many&lt;/em&gt;, &lt;em&gt;several&lt;/em&gt;, &lt;em&gt;next&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AT&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;article&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;a&lt;/em&gt;, &lt;em&gt;the&lt;/em&gt;, &lt;em&gt;no&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BE&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;be&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BED&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;were&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEDZ&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;was&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEG&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;being&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEM&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;am&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEN&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;been&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BER&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;are&lt;/em&gt;, &lt;em&gt;art&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BEZ&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;is&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CC&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;coordinating conjunction&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;and&lt;/em&gt;, &lt;em&gt;or&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CD&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;cardinal numeral&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;one&lt;/em&gt;, &lt;em&gt;two&lt;/em&gt;, &lt;em&gt;2&lt;/em&gt;, etc.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;subordinating conjunction&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;if&lt;/em&gt;, &lt;em&gt;although&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DO&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;do&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DOD&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;did&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DOZ&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;does&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DT&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;singular determiner&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;this&lt;/em&gt;, &lt;em&gt;that&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTI&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;singular or plural determiner / quantifier&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;some&lt;/em&gt;, &lt;em&gt;any&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;plural determiner&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;these&lt;/em&gt;, &lt;em&gt;those&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DTX&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;determiner / double conjunction&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;either&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EX&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;existentil &lt;em&gt;there&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FW&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;foreign word (hyphenated before regular tag)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HL&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;word occurring in headline (hyphenated after regular tag)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HV&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;have&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVD&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;had&lt;/em&gt; (past tense)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVG&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;having&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVN&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;had&lt;/em&gt; (past participle)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HVZ&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;has&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IN&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;preposition&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJ&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;adjective&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJR&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;comparative adjective&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;semantically superlative adjective&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;chief&lt;/em&gt;, &lt;em&gt;top&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JJT&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;morphologically superlative adjective&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;biggest&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MD&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;modal auxiliary&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;can&lt;/em&gt;, &lt;em&gt;should&lt;/em&gt;, &lt;em&gt;will&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NC&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;cited word (hyphenated after regular tag)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;singular or mass noun&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NN$&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;possessive singular noun&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;plural noun&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NNS$&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;possessive plural noun&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;proper noun or part of name phrase&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NP$&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;possessive proper noun&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;plural proper noun&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NPS$&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;possessive plural proper noun&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NR&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;adverbial noun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;home&lt;/em&gt;, &lt;em&gt;today&lt;/em&gt;, &lt;em&gt;west&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NRS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;plural adverbial noun&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OD&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;ordinal numeral&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;first&lt;/em&gt;, &lt;em&gt;2nd&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PN&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;nominal pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;everybody&lt;/em&gt;, &lt;em&gt;nothing&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PN$&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;possessive nominal pronoun&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP$&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;possessive personal pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;my&lt;/em&gt;, &lt;em&gt;our&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PP$$&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;second (nominal) possessive pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;mine&lt;/em&gt;, &lt;em&gt;ours&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPL&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;singular reflexive/intensive personal pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;myself&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPLS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;plural reflexive/intensive personal pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;ourselves&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPO&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;objective personal pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;me&lt;/em&gt;, &lt;em&gt;him&lt;/em&gt;, &lt;em&gt;it&lt;/em&gt;, &lt;em&gt;them&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;3rd. singular nominative pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;he&lt;/em&gt;, &lt;em&gt;she&lt;/em&gt;, &lt;em&gt;it&lt;/em&gt;, &lt;em&gt;one&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PPSS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;other nominative personal pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;I&lt;/em&gt;, &lt;em&gt;we&lt;/em&gt;, &lt;em&gt;they&lt;/em&gt;, &lt;em&gt;you&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QL&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;qualifier&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;very&lt;/em&gt;, &lt;em&gt;fairly&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QLP&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;post-qualifier&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;enough&lt;/em&gt;, &lt;em&gt;indeed&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RB&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;adverb&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBR&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;comparative adverb&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RBT&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;superlative adverb&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RN&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;nominal adverb&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;here&lt;/em&gt;, &lt;em&gt;then&lt;/em&gt;, &lt;em&gt;indoors&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RP&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;adverb / particle&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;about&lt;/em&gt;, &lt;em&gt;off&lt;/em&gt;, &lt;em&gt;up&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TL&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;word occurring in title (hyphenated after regular tag)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TO&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;infinitive marker &lt;em&gt;to&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UH&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;interjection, exclamation&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VB&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;verb, base form&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBD&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;verb, past tense&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBG&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;verb, present participle / gerund&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBN&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;verb, past participle&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VBZ&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;verb, 3rd. singular present&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WDT&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;wh-&lt;/em&gt; determiner&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;what&lt;/em&gt;, &lt;em&gt;which&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WP$&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;possessive &lt;em&gt;wh-&lt;/em&gt; pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;whose&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WPO&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;objective &lt;em&gt;wh-&lt;/em&gt; pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;whom&lt;/em&gt;, &lt;em&gt;which&lt;/em&gt;, &lt;em&gt;that&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WPS&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;nominative &lt;em&gt;wh-&lt;/em&gt; pronoun&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;who&lt;/em&gt;, &lt;em&gt;which&lt;/em&gt;, &lt;em&gt;that&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WQL&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;wh-&lt;/em&gt; qualifier&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;how&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WRB&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;wh-&lt;/em&gt; adverb&lt;/td&gt;
      &lt;td&gt;&lt;em&gt;how&lt;/em&gt;, &lt;em&gt;where&lt;/em&gt;, &lt;em&gt;when&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>[&quot;W. Nelson Francis&quot;]</name></author><category term="Dataset" /><category term="Brown Corpus" /><category term="part of speech" /><summary type="html">目標 建立大型文字標記資料集 作者 W. Nelson Francis 隸屬單位 University of Brown 期刊/會議名稱 College English 發表時間 1964 論文連結 https://www.jstor.org/stable/373638 參考手冊 http://korpus.uib.no/icame/manuals/BROWN/INDEX.HTM</summary></entry><entry><title type="html">Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling</title><link href="/acoustic%20modeling/2022/03/01/long-short-term-memory-recurrent-neural-network-architectures-for-large-scale-acoustic-modeling.html" rel="alternate" type="text/html" title="Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling" /><published>2022-03-01T19:42:00+08:00</published><updated>2022-03-01T19:42:00+08:00</updated><id>/acoustic%20modeling/2022/03/01/long-short-term-memory-recurrent-neural-network-architectures-for-large-scale-acoustic-modeling</id><content type="html" xml:base="/acoustic%20modeling/2022/03/01/long-short-term-memory-recurrent-neural-network-architectures-for-large-scale-acoustic-modeling.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;嘗試分散式平型化訓練 LSTM 進行字典範圍較大的語音辨識&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;Hasim Sak, Andrew W. Senior, Françoise Beaufays&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隸屬單位&lt;/td&gt;
      &lt;td&gt;Google&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;Interspeech&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;2014&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://research.google/pubs/pub43905/&quot;&gt;https://research.google/pubs/pub43905/&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section&quot;&gt;重點&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;這篇論文是 Google &lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;前一篇&lt;/a&gt;論文的續作，補了更多實驗後終於投稿上 Interspeech
    &lt;ul&gt;
      &lt;li&gt;所有實驗採用的架構都與&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;前一篇&lt;/a&gt;論文相同&lt;/li&gt;
      &lt;li&gt;在這篇論文中幫提出的架構取名為 LSTMP（Long Short-Term Memory Projected）&lt;/li&gt;
      &lt;li&gt;不再使用額外的 non-recurrent projection layer，因此&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;前一篇&lt;/a&gt;論文中的 $n_p = 0$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;第一篇論文嘗試以大量叢集節點 + asynchronous stochastic gradient descent（ASGD）訓練 LSTM 進行語音辨識
    &lt;ul&gt;
      &lt;li&gt;人家有錢&lt;/li&gt;
      &lt;li&gt;兩層 LSTM 可以達到語音辨識的 SOTA&lt;/li&gt;
      &lt;li&gt;比 RNN + feed-forward 架構表現還好&lt;/li&gt;
      &lt;li&gt;比單純使用 feed-forward 架構的參數數量少快 10 倍&lt;/li&gt;
      &lt;li&gt;比 &lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;LSTM-2002&lt;/a&gt; 架構表現更好，雖然兩者在層數增加時表現接近，但 &lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;LSTM-2002&lt;/a&gt; 更難訓練且訓練時間更長&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;架構&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 1：LSTMP 架構。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43905/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Thd51gv.png&quot; alt=&quot;圖 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 2：多層 LSTM 與 LSTMP 架構。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43905/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/eeTLPV3.png&quot; alt=&quot;圖 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;參數定義與運算架構與&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;前一篇&lt;/a&gt;論文完全相同&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$n_i$：輸入單元個數&lt;/li&gt;
  &lt;li&gt;$n_o$：輸出單元個數&lt;/li&gt;
  &lt;li&gt;$n_c$：記憶單元區塊個數&lt;/li&gt;
  &lt;li&gt;$n_r$：記憶單元輸出降維後的維度&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;符號&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;維度&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$T$&lt;/td&gt;
      &lt;td&gt;輸入序列的總長度&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;$T \in \N$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$t$&lt;/td&gt;
      &lt;td&gt;輸入序列的時間點&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;$t = 1, \dots, T$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$x_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸入&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_i$&lt;/td&gt;
      &lt;td&gt;$x = (x_1, \dots, x_T)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_o$&lt;/td&gt;
      &lt;td&gt;$y = (y_1, \dots, y_T)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$f_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;遺忘閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$f_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$i_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸入閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$i_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$o_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸出閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$o_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$c_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點&lt;strong&gt;記憶單元內部狀態&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$c_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$m_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點&lt;strong&gt;記憶單元輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$r_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點&lt;strong&gt;記憶單元輸出&lt;/strong&gt;經過降維後的結果&lt;/td&gt;
      &lt;td&gt;$n_r$&lt;/td&gt;
      &lt;td&gt;$r_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{g x}$&lt;/td&gt;
      &lt;td&gt;連接外部輸入與閘門 $g$ 的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_i$&lt;/td&gt;
      &lt;td&gt;全連接，$g \in \set{i, f, o}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{g r}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元輸出降維結果與閘門 $g$ 的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_r$&lt;/td&gt;
      &lt;td&gt;全連接，$g \in \set{i, f, o}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{g c}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元內部狀態與閘門 $g$ 的參數&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;peephole connections，$g \in \set{i, f, o}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_g$&lt;/td&gt;
      &lt;td&gt;閘門 $g$ 的偏差項&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$g \in \set{i, f, o}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{c x}$&lt;/td&gt;
      &lt;td&gt;連接外部輸入與記憶單元輸入的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_i$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{c r}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元輸出降維結果與記憶單元輸入的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_r$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_c$&lt;/td&gt;
      &lt;td&gt;記憶單元輸入的偏差項&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{y r}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元輸出降維結果與總輸出的參數&lt;/td&gt;
      &lt;td&gt;$n_o \times n_r$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_y$&lt;/td&gt;
      &lt;td&gt;總輸出的偏差項&lt;/td&gt;
      &lt;td&gt;$n_o$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;sigmoid 函數&lt;/td&gt;
      &lt;td&gt;$\sigma(x) = \frac{1}{1 + e^{-x}}$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;計算公式定義如下&lt;/p&gt;

\[\begin{align*}
i_t &amp;amp; = \sigma(W_{i x} \cdot x_t + W_{i r} \cdot r_{t - 1} + W_{i c} \odot c_{t - 1} + b_i) \\
f_t &amp;amp; = \sigma(W_{f x} \cdot x_t + W_{f r} \cdot r_{t - 1} + W_{f c} \odot c_{t - 1} + b_f) \\
c_t &amp;amp; = f_t \odot c_{t - 1} + i_t \odot \tanh(W_{c x} \cdot x_t + W_{c m} \cdot r_{t - 1} + b_c) \\
o_t &amp;amp; = \sigma(W_{o x} \cdot x_t + W_{o r} \cdot r_{t - 1} + W_{o c} \odot c_t + b_o) \\
m_t &amp;amp; = o_t \odot \tanh(c_t) \\
r_t &amp;amp; = W_{r m} \cdot m_t \\
y_t &amp;amp; = \operatorname{softmax}(W_{y r} r_t + b_y)
\end{align*} \tag{1}\label{1}\]

&lt;h2 id=&quot;section-2&quot;&gt;最佳化&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;使用 CPU 叢集進行訓練
    &lt;ul&gt;
      &lt;li&gt;共有 $500$ 個計算節點&lt;/li&gt;
      &lt;li&gt;每個節點使用 $3$ 個 threads&lt;/li&gt;
      &lt;li&gt;每個 thread 計算 $4$ 個訊號序列（batch size per thread = $4$）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;使用 &lt;a href=&quot;http://eigen.tuxfamily.org&quot;&gt;Eigen&lt;/a&gt; 函式庫進行矩陣計算
    &lt;ul&gt;
      &lt;li&gt;版本為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v3&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;支援 C++&lt;/li&gt;
      &lt;li&gt;支援 SIMD 平行化指令&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;採用 truncated BPTT 進行最佳化，truncated window size 為 $20$&lt;/li&gt;
  &lt;li&gt;使用 cross entropy loss 作為最佳化目標&lt;/li&gt;
  &lt;li&gt;使用非同步梯度下降（Asynchronous Stochastic Gradient Descent，ASGD）演算法進行最佳化
    &lt;ul&gt;
      &lt;li&gt;擁有一個中央伺服器負責儲存參數&lt;/li&gt;
      &lt;li&gt;單一計算節點完成 $3 \times 4 \times 20$ 的梯度計算後將梯度傳給中央伺服器&lt;/li&gt;
      &lt;li&gt;中央伺服器收到梯度後進行更新，並回傳更新後的參數給計算節點&lt;/li&gt;
      &lt;li&gt;由於 batch size 理論上變大了，作者將 learning rate 設定成較小的數值（這句話似乎完全是經驗談）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;實驗設計&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;實驗資料集為 Google English Voice Search task，非公開資料集&lt;/li&gt;
  &lt;li&gt;實驗的比較對象為 DNN 與 RNN
    &lt;ul&gt;
      &lt;li&gt;所有模型都訓練在 $3$ 百萬筆的語音資料集上，長度共 $1900$ 小時&lt;/li&gt;
      &lt;li&gt;所有資料都有去識別化&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;資料前處理
    &lt;ul&gt;
      &lt;li&gt;每筆資料共有 $25$ 毫秒&lt;/li&gt;
      &lt;li&gt;每筆資料一幀為 $10$ 毫秒&lt;/li&gt;
      &lt;li&gt;每幀都使用 log-filterbank 將頻率進行特徵提取（phonemes），取 40 個維度當作特徵&lt;/li&gt;
      &lt;li&gt;額外訓練了一個共有 $90$ M 參數的 feed-forward neural network（FFNN）進行輸入特徵與狀態對齊（states alignment），總共定義了 $14247$ 個前後文相依狀態（context dependent states，CD）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;標記資料為每個 $40$ 維的 feature 對應到的 phoneme state
    &lt;ul&gt;
      &lt;li&gt;模型每個時間點的輸入至少為 $40$ 維（代表 $n_i = 40$）&lt;/li&gt;
      &lt;li&gt;模型每個時間點的輸出為對應到的狀態（代表 $n_o = 14247$）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;所有參數初始化範圍為 $(-0.02, 0.02)$&lt;/li&gt;
  &lt;li&gt;每個實驗設置都採用各自最適合的 learning rate（hyperparameter tuning），並對 learning rate 使用 expenentially decay
    &lt;ul&gt;
      &lt;li&gt;Learning rate 範圍大約落在 $[5 \times 10^{-6}, 1 \times 10^{-5}]$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;額外限制 LSTM 中 $c_t$ 的數值範圍，落在 $[-50, 50]$
    &lt;ul&gt;
      &lt;li&gt;概念如同 gradient clipping&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;評估方法
    &lt;ul&gt;
      &lt;li&gt;驗證資料（development set）有 $200000$ 幀，針對每一幀中所有的 state 進行準確率（accuracy）的計算，稱為 frame accuracy&lt;/li&gt;
      &lt;li&gt;測試資料（test set）有 $22500$ 幀，計算文字辨識錯誤率（word error rates）&lt;/li&gt;
      &lt;li&gt;所有實驗共用相同的 $5$-gram language model
        &lt;ul&gt;
          &lt;li&gt;這裡的假設為：當模型能夠將輸入特徵與狀態對齊成功時，後續的 language model 就會自然產出正確的辨識文字結果&lt;/li&gt;
          &lt;li&gt;共有兩種不同的字典大小，分別為 $23$ M 與 $1$ B&lt;/li&gt;
          &lt;li&gt;Language model decoding 採用 beam search，beam width 設成較大的數字&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;由於未來時間的資訊有助於提升預測的準確度，因此模型預測會在延遲 $5$ 幀後開始輸出
    &lt;ul&gt;
      &lt;li&gt;ex: 第 $0$ 幀到第 $4$ 幀輸入完後，當第 $5$ 幀輸入時預測第 $0$ 幀的 $40$ 維特徵所對應到的狀態&lt;/li&gt;
      &lt;li&gt;前 $5$ 幀不計算誤差，最後 $5$ 幀重複輸入讓模型可以預測 $T - 4$ 到 $T$ 的狀態&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;實驗結果&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 3：LSTM 與 LSTMP 的表現對照。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43905/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/NlKdg0R.png&quot; alt=&quot;圖 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 4：LSTM 與 LSTMP 的收斂速度對照。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43905/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/hB3iGDJ.png&quot; alt=&quot;圖 4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 5：LSTMP 不同參數組合實驗結果。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43905/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/cIPrLTD.png&quot; alt=&quot;圖 5&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;對 &lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;LSTM-2002&lt;/a&gt; 架構進行分析（見&lt;a href=&quot;#paper-fig-3&quot;&gt;圖 3&lt;/a&gt; 上半部）
    &lt;ul&gt;
      &lt;li&gt;在只使用 $1$ 層時表現不好&lt;/li&gt;
      &lt;li&gt;改用 $2$ 層時表現有進步但仍然不夠好&lt;/li&gt;
      &lt;li&gt;採用 $5$ 層時表現最佳&lt;/li&gt;
      &lt;li&gt;採用 $7$ 層時很難收斂（作者 train 了一天以上才看到收斂），而且表現沒有比較好&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;對 LSTMP 進行分析（見&lt;a href=&quot;#paper-fig-3&quot;&gt;圖 3&lt;/a&gt; 下半部）
    &lt;ul&gt;
      &lt;li&gt;只使用 $1$ 層且使用大量的 memory cell blocks（$n_c$ 較大）時容易導致 overfitting&lt;/li&gt;
      &lt;li&gt;單純的將層數增加似乎就減少 overfitting 的現象&lt;/li&gt;
      &lt;li&gt;多層 LSTMP 表現只比多層 LSTM 好一點點，與&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;前一篇&lt;/a&gt;論文的實驗結果差異蠻大的（理由是前一篇論文都只用一層進行實驗）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;對收斂狀況進行分析（見&lt;a href=&quot;#paper-fig-4&quot;&gt;圖 4&lt;/a&gt;）
    &lt;ul&gt;
      &lt;li&gt;LSTMP 收斂速度比 &lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;LSTM-2002&lt;/a&gt; 還要快&lt;/li&gt;
      &lt;li&gt;層數愈多表現愈好，但愈難收斂&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;對 LSTMP 的參數數量進行分析（見&lt;a href=&quot;#paper-fig-5&quot;&gt;圖 5&lt;/a&gt;）
    &lt;ul&gt;
      &lt;li&gt;參數數量大於 $13$ M 時並不會讓表現進步更多&lt;/li&gt;
      &lt;li&gt;只有兩層時表現可以達到最佳
        &lt;ul&gt;
          &lt;li&gt;訓練 $48$ 小時可以讓 WER 達到 $10.9\%$&lt;/li&gt;
          &lt;li&gt;訓練 $100$ 小時可以讓 WER 達到 $10.7\%$&lt;/li&gt;
          &lt;li&gt;訓練 $200$ 小時可以讓 WER 達到 $10.5\%$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;參數數量為 $85$ M 的 DNN 模型，最佳表現只能達到 $11.3\%$，並且需要訓練好幾個星期&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>[&quot;Hasim Sak&quot;, &quot;Andrew W. Senior&quot;, &quot;Françoise Beaufays&quot;]</name></author><category term="Acoustic Modeling" /><category term="LSTM" /><category term="LSTMP" /><category term="model architecture" /><category term="neural network" /><summary type="html">目標 嘗試分散式平型化訓練 LSTM 進行字典範圍較大的語音辨識 作者 Hasim Sak, Andrew W. Senior, Françoise Beaufays 隸屬單位 Google 期刊/會議名稱 Interspeech 發表時間 2014 論文連結 https://research.google/pubs/pub43905/</summary></entry><entry><title type="html">Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Scale Vocabulary Speech Recognition</title><link href="/acoustic%20modeling/2022/03/01/long-short-term-memory-based-recurrent-neural-network-architectures-for-large-scale-vocabulary-speech-recognition.html" rel="alternate" type="text/html" title="Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Scale Vocabulary Speech Recognition" /><published>2022-03-01T16:01:00+08:00</published><updated>2022-03-01T16:01:00+08:00</updated><id>/acoustic%20modeling/2022/03/01/long-short-term-memory-based-recurrent-neural-network-architectures-for-large-scale-vocabulary-speech-recognition</id><content type="html" xml:base="/acoustic%20modeling/2022/03/01/long-short-term-memory-based-recurrent-neural-network-architectures-for-large-scale-vocabulary-speech-recognition.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;嘗試以 LSTM 進行字典範圍較大的語音辨識&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;Hasim Sak, Andrew W. Senior, Françoise Beaufays&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隸屬單位&lt;/td&gt;
      &lt;td&gt;Google&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;arXiv&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;2014&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;https://research.google/pubs/pub43895/&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section&quot;&gt;重點&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;此篇論文被 ICASSP reject，因為頁數太少（含 reference 只有 5 頁）
    &lt;ul&gt;
      &lt;li&gt;這篇論文真的就只跑兩個實驗&lt;/li&gt;
      &lt;li&gt;後續作品為&lt;a href=&quot;https://research.google/pubs/pub43905/&quot;&gt;這篇&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;此論文實驗結果說明 LSTM 可以套用到字典量大的語音辨識
    &lt;ul&gt;
      &lt;li&gt;字典量大代表對應的 phoneme states 變多，真正的難題是如何將輸入特徵對應到 phoneme states&lt;/li&gt;
      &lt;li&gt;過去使用傳統 RNN 模型的論文只能在字典量小的語音辨識資料集上表現不錯&lt;/li&gt;
      &lt;li&gt;使用作者提出的 LSTM 架構可以達到語音辨識的 SOTA，原本的 LSTM 架構（這裡指的是 &lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;LSTM-2002&lt;/a&gt;）則無法超越單純使用 DNN 的表現&lt;/li&gt;
      &lt;li&gt;作者提出的 LSTM 架構主要在減少參數數量，參數數量比單純使用 feed-forward 架構的模型少 $2$ 到 $3$ 倍&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;為了使用 LSTM 進行大規模的平行化訓練，修改了 LSTM 架構讓訓練更有效率
    &lt;ul&gt;
      &lt;li&gt;不需要使用 Connectionist Temporal Classifier （CTC） 或 RNN transducer 等架構&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html&quot;&gt;PyTorch 實作的 LSTM&lt;/a&gt; 宣稱是參考此篇論文，但實際上實作的卻是 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 的架構&lt;/li&gt;
      &lt;li&gt;此篇論文是基於 &lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;LSTM-2002&lt;/a&gt; 的架構進行改良&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;模型架構&lt;/h2&gt;

&lt;h3 id=&quot;lstm&quot;&gt;原版 LSTM&lt;/h3&gt;

&lt;p&gt;假設 LSTM 的記憶單元（memory block）維度為 $1$（one cell in each memory block），共有 $n_c$ 個記憶單元，$n_i$ 個輸入單元，$n_o$ 個輸出單元，則 LSTM 總參數量（不含 bias）為（細節可見&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;我的筆記&lt;/a&gt;）&lt;/p&gt;

\[W = n_c \times n_c \times 4 + n_i \times n_c \times 4 + n_c \times n_o + n_c \times 3 \tag{1}\label{1}\]

&lt;ul&gt;
  &lt;li&gt;$n_c \times n_c \times 4$：記憶單元輸出以全連接的形式連接到記憶單元輸入、遺忘閘門、輸入閘門與輸出閘門&lt;/li&gt;
  &lt;li&gt;$n_i \times n_c \times 4$：外部輸入以全連接的形式連接到記憶單元輸入、遺忘閘門、輸入閘門與輸出閘門&lt;/li&gt;
  &lt;li&gt;$n_c \times n_o$：記憶單元輸出以全連接的形式連接到總輸出&lt;/li&gt;
  &lt;li&gt;$n_c \times 3$：peephole connections&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由於 LSTM 使用 truncated RTRL，因此每個時間點以隨機梯度下降法（stochastic gradient descent，SGD）進行參數最佳化的時間複雜度為 $O(W)$。&lt;/p&gt;

&lt;p&gt;當輸入維度 $n_i$ 較小時，時間複雜度的主要貢獻來自於 $n_c \times (n_c + n_o)$。
在輸出預測範圍較大（字典範圍較大）或需要大量記憶容量（$n_c$ 較大時）的狀況下，模型的最佳化時間複雜度變高，計算成本大幅提升。
因此此論文提出想要將 LSTM 的複雜度降低成 $n_r \times (n_c + n_o)$，其中 $n_r \ll n_c$，$n_r$ 的定義在後面的文章段落中進行描述。&lt;/p&gt;

&lt;p&gt;首先我們定義這篇論文使用的符號&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;符號&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;維度&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$T$&lt;/td&gt;
      &lt;td&gt;輸入序列的總長度&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;$T \in \N$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$t$&lt;/td&gt;
      &lt;td&gt;輸入序列的時間點&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;$t = 1, \dots, T$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$x_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸入&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_i$&lt;/td&gt;
      &lt;td&gt;$x = (x_1, \dots, x_T)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$f_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;遺忘閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$f_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$i_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸入閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$i_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$o_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸出閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$o_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$c_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點&lt;strong&gt;記憶單元內部狀態&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$c_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$m_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點&lt;strong&gt;記憶單元輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;$m_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_t$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$n_o$&lt;/td&gt;
      &lt;td&gt;$y = (y_1, \dots, y_T)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{i x}$&lt;/td&gt;
      &lt;td&gt;連接外部輸入與輸入閘門的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_i$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{i m}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元輸出與輸入閘門的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_c$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{i c}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元內部狀態與輸入閘門的參數&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;peephole connections&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_i$&lt;/td&gt;
      &lt;td&gt;輸入閘門的偏差項&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{f x}$&lt;/td&gt;
      &lt;td&gt;連接外部輸入與遺忘閘門的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_i$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{f m}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元輸出與遺忘閘門的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_c$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{f c}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元內部狀態與遺忘閘門的參數&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;peephole connections&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_f$&lt;/td&gt;
      &lt;td&gt;遺忘閘門的偏差項&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{o x}$&lt;/td&gt;
      &lt;td&gt;連接外部輸入與輸出閘門的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_i$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{o m}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元輸出與輸出閘門的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_c$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{o c}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元內部狀態與輸出閘門的參數&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt;peephole connections&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_o$&lt;/td&gt;
      &lt;td&gt;輸出閘門的偏差項&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{c x}$&lt;/td&gt;
      &lt;td&gt;連接外部輸入與記憶單元輸入的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_i$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{c m}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元輸出與記憶單元輸入的參數&lt;/td&gt;
      &lt;td&gt;$n_c \times n_c$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_c$&lt;/td&gt;
      &lt;td&gt;記憶單元輸入的偏差項&lt;/td&gt;
      &lt;td&gt;$n_c$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$W_{y m}$&lt;/td&gt;
      &lt;td&gt;連接記憶單元輸出與總輸出的參數&lt;/td&gt;
      &lt;td&gt;$n_o \times n_c$&lt;/td&gt;
      &lt;td&gt;全連接&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$b_y$&lt;/td&gt;
      &lt;td&gt;總輸出的偏差項&lt;/td&gt;
      &lt;td&gt;$n_o$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;sigmoid 函數&lt;/td&gt;
      &lt;td&gt;$\sigma(x) = \frac{1}{1 + e^{-x}}$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;得到 $t$ 時間點的外部輸入時可以計算 $t$ 時間點的遺忘閘門 $f_t$ 與輸入閘門 $i_t$&lt;/p&gt;

\[\begin{align*}
i_t &amp;amp; = \sigma(W_{i x} \cdot x_t + W_{i m} \cdot m_{t - 1} + W_{i c} \odot c_{t - 1} + b_i) \\
f_t &amp;amp; = \sigma(W_{f x} \cdot x_t + W_{f m} \cdot m_{t - 1} + W_{f c} \odot c_{t - 1} + b_f)
\end{align*} \tag{2}\label{2}\]

&lt;p&gt;注意：論文不小心把 peephole connections 寫成全連接，因此 $W_{i c} \cdot c_{t - 1}$ 要改成 $W_{i c} \odot c_{t - 1}$，同理 $W_{f c} \cdot c_{t - 1}$ 要改成 $W_{f c} \odot c_{t - 1}$。&lt;/p&gt;

&lt;p&gt;接著產生 $t$ 時間點的記憶單元內部狀態 $c_t$&lt;/p&gt;

\[c_t = f_t \odot c_{t - 1} + i_t \odot \tanh(W_{c x} \cdot x_t + W_{c m} \cdot m_{t - 1} + b_c) \tag{3}\label{3}\]

&lt;p&gt;利用 $t - 1$ 時間點的記憶單元輸出 $m_{t - 1}$ 加上 $t$ 時間點的外部輸入 $x_t$ 與記憶單元內部狀態 $c_t$ 更新 $t$ 時間點的輸出閘門&lt;/p&gt;

\[o_t = \sigma(W_{o x} \cdot x_t + W_{o m} \cdot m_{t - 1} + W_{o c} \odot c_t + b_o) \tag{4}\label{4}\]

&lt;p&gt;注意：論文不小心把 peephole connections 寫成全連接，因此 $W_{o c} \cdot c_t$ 要改成 $W_{o c} \odot c_t$。&lt;/p&gt;

&lt;p&gt;接著可以計算 $t$ 時間點的記憶單元輸出 $m_t$&lt;/p&gt;

\[m_t = o_t \odot \tanh(c_t) \tag{5}\label{5}\]

&lt;p&gt;最後利用 $t$ 時間點的記憶單元輸出 $m_t$ 計算 $t$ 時間點的總輸出 $y_t$&lt;/p&gt;

\[y_t = W_{y m} \cdot m_t + b_y \tag{6}\label{6}\]

&lt;p&gt;注意 LSTM 的總輸出沒有使用啟發函數。&lt;/p&gt;

&lt;h3 id=&quot;lstm-1&quot;&gt;改版 LSTM&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 1：改版 LSTM 架構。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Oz7AHYQ.png&quot; alt=&quot;圖 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;為了降低計算的時間複雜度，作者提出了對 $m_t$ 進行降低維度的概念。&lt;/p&gt;

&lt;p&gt;以 $r_t$ 代表降維後的 $m_t$，$r_t$ 的維度為 $n_r$，協助降維的參數為 $W_{r m}$，將 $\eqref{2} \eqref{3} \eqref{4}$ 中的 $m_t$ 改為 $r_t$&lt;/p&gt;

\[\begin{align*}
i_t &amp;amp; = \sigma(W_{i x} \cdot x_t + W_{i r} \cdot r_{t - 1} + W_{i c} \odot c_{t - 1} + b_i) \\
f_t &amp;amp; = \sigma(W_{f x} \cdot x_t + W_{f r} \cdot r_{t - 1} + W_{f c} \odot c_{t - 1} + b_f) \\
c_t &amp;amp; = f_t \odot c_{t - 1} + i_t \odot \tanh(W_{c x} \cdot x_t + W_{c r} \cdot r_{t - 1} + b_c) \\
o_t &amp;amp; = \sigma(W_{o x} \cdot x_t + W_{o r} \cdot r_{t - 1} + W_{o c} \odot c_t + b_o)
\end{align*} \tag{7}\label{7}\]

&lt;p&gt;而 $\eqref{5}$ 的計算方法不變，得到 $\eqref{5}$ 我們使用 $W_{r m}$ 進行降維的動作&lt;/p&gt;

\[r_t = W_{r m} \cdot m_t \tag{8}\label{8}\]

&lt;p&gt;最後計算總輸出的式子 $\eqref{6}$ 改為&lt;/p&gt;

\[y_t = W_{y r} \cdot r_t + b_y \tag{9}\label{9}\]

&lt;p&gt;由於 $n_r &amp;lt; n_c$，將 $W_{\star m}$ 改成 $W_{\star r}$ 之後維度從 $n_c \times n_c$ 降維 $n_c \times n_r$，模型的總參數量（不含 bias）變成&lt;/p&gt;

\[W = n_c \times n_i \times 4 + n_c \times n_r \times 4 + n_c \times 3 + n_r \times n_c + n_o \times n_r \tag{10}\label{10}\]

&lt;p&gt;當輸入維度 $n_i$ 較小時，時間複雜度的主要貢獻來自於 $n_r \times (n_c + n_o)$。&lt;/p&gt;

&lt;p&gt;作者認為可以額外加上一些非遞迴單元 $p_t$，在不增加遞迴計算的維度下讓與輸出層相接的隱藏層維度稍微增加一些。&lt;/p&gt;

&lt;p&gt;令 $p_t$ 的維度為 $n_p$，我們額外定義新的參數 $W_{p m}$，並使用記憶單元輸出 $m_t$ 計算 $p_t$&lt;/p&gt;

\[p_t = W_{p m} \cdot m_t \tag{11}\label{11}\]

&lt;p&gt;最後將 $\eqref{9}$ 修改為&lt;/p&gt;

\[y_t = W_{y r} \cdot r_t + W_{y p} \cdot p_t + b_y \tag{12}\label{12}\]

&lt;p&gt;注意 $r_t$ 與 $p_t$ 不同，$r_t$ 有參與遞迴的過程，$p_t$ 並沒有參與遞迴的過程。&lt;/p&gt;

&lt;p&gt;在加入 $p_t$ 後參數的數量變成&lt;/p&gt;

\[W = n_c \times n_i \times 4 + n_c \times n_r \times 4 + n_c \times 3 + (n_r + n_p) \times n_c + n_o \times (n_r + n_p) \tag{13}\label{13}\]

&lt;h3 id=&quot;section-2&quot;&gt;實作&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;使用 CPU 而不是 GPU
    &lt;ul&gt;
      &lt;li&gt;使用 CPU 方便 debug&lt;/li&gt;
      &lt;li&gt;當時的環境是 Google 有大量 CPU 叢集節點（clustering node），但沒有 GPU 叢集節點&lt;/li&gt;
      &lt;li&gt;這是 2014 年的論文，還沒有 tensorflow 可以用，所以這個選擇可以理解&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;使用 &lt;a href=&quot;http://eigen.tuxfamily.org&quot;&gt;Eigen&lt;/a&gt; 函式庫進行矩陣計算
    &lt;ul&gt;
      &lt;li&gt;版本為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v3&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;支援 C++&lt;/li&gt;
      &lt;li&gt;支援 SIMD 平行化指令&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;使用非同步梯度下降（Asynchronous Stochastic Gradient Descent，ASGD）演算法進行最佳化&lt;/li&gt;
  &lt;li&gt;因為有多層 LSTM，使用 truncated BPTT 進行最佳化
    &lt;ul&gt;
      &lt;li&gt;注意不是採用&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM&lt;/a&gt; 論文中的 truncated RTRL&lt;/li&gt;
      &lt;li&gt;每 $20$ 個時間點進行一次 BPTT&lt;/li&gt;
      &lt;li&gt;每 $20$ 個時間點的計算狀態會保留給下一次 $20$ 個時間點當成計算初始狀態&lt;/li&gt;
      &lt;li&gt;一個 batch 會由長度為 $20$ 個時間點的序列組成&lt;/li&gt;
      &lt;li&gt;一個 batch 中比較短的序列會以 padding 補齊，並且在下一個 batch 中替換成其他輸入序列，對應的計算狀態都會初始化&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;最佳化目標為 cross entropy&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;實驗設計&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;實驗資料集為 Google English Voice Search task，非公開資料集&lt;/li&gt;
  &lt;li&gt;實驗的比較對象為 DNN 與 RNN
    &lt;ul&gt;
      &lt;li&gt;所有模型都訓練在 $3$ 百萬筆的語音資料集上，長度共 $1900$ 小時&lt;/li&gt;
      &lt;li&gt;所有資料都有去識別化&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;資料前處理
    &lt;ul&gt;
      &lt;li&gt;每筆資料共有 $25$ 毫秒&lt;/li&gt;
      &lt;li&gt;每筆資料一幀為 $10$ 毫秒&lt;/li&gt;
      &lt;li&gt;每幀都使用 log-filterbank 將頻率進行特徵提取（phonemes），取 40 個維度當作特徵&lt;/li&gt;
      &lt;li&gt;額外訓練了一個共有 $90$ M 參數的 feed-forward neural network（FFNN）進行輸入特徵與狀態對齊（states alignment），總共定義了 $14247$ 個前後文相依狀態（context dependent states，CD）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;將總共 $14247$ 個狀態減少成三種不同數量的狀態進行實驗
    &lt;ul&gt;
      &lt;li&gt;前後文無關狀態（context independent states，CI）：共有 $126$ 個狀態，每 $3$ 個 phonemes 組成一個狀態，共有 $42$ 種不同的 phonemes&lt;/li&gt;
      &lt;li&gt;使用事先定義好的 phonemes 等價關係將狀態分別減少至 $8000$ 與 $2000$，仍為 CD 狀態&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;標記資料為每個 $40$ 維的 feature 對應到的 phoneme state
    &lt;ul&gt;
      &lt;li&gt;模型每個時間點的輸入至少為 $40$ 維（對應到 $n_i$）&lt;/li&gt;
      &lt;li&gt;模型每個時間點的輸出為對應到的狀態（對應到 $n_o$）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;每個實驗設置都採用各自最適合的 learning rate（hyperparameter tuning），並對 learning rate 使用 expenentially decay&lt;/li&gt;
  &lt;li&gt;評估方法
    &lt;ul&gt;
      &lt;li&gt;驗證資料（development set）有 $200000$ 幀，針對每一幀中所有的 state 進行準確率（accuracy）的計算，稱為 frame accuracy&lt;/li&gt;
      &lt;li&gt;測試資料（test set）有 $23000$ 幀，計算文字辨識錯誤率（word error rates）
        &lt;ul&gt;
          &lt;li&gt;需要額外擁有一個 language model 進行狀態到文字的轉換&lt;/li&gt;
          &lt;li&gt;所有實驗共用相同的 language model，字典大小為 $2.6$ M&lt;/li&gt;
          &lt;li&gt;這裡的假設為：當模型能夠將輸入特徵與狀態對齊成功時，後續的 language model 就會自然產出正確的辨識文字結果&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dnn-&quot;&gt;DNN 實驗設計&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;batch size = $200$ 幀&lt;/li&gt;
  &lt;li&gt;使用 GPU 進行訓練&lt;/li&gt;
  &lt;li&gt;模型採用全連接架構，隱藏層都使用 sigmoid 作為 activation function，輸出使用 softmax 進行 normalization&lt;/li&gt;
  &lt;li&gt;輸入共包含 $3$ 個部份
    &lt;ul&gt;
      &lt;li&gt;當前幀數：$1$&lt;/li&gt;
      &lt;li&gt;未來幀數：$5$&lt;/li&gt;
      &lt;li&gt;過去幀數：$10$ 或 $16$，分別標記為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10w5&lt;/code&gt; 與 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;16w5&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-&quot;&gt;RNN 實驗設計&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;使用 ASGD 進行最佳化&lt;/li&gt;
  &lt;li&gt;使用 CPU 進行訓練，一個 CPU 使用 $24$ 的 threads，只使用一個 CPU
    &lt;ul&gt;
      &lt;li&gt;使用 data parallel 的概念，每個 thread 計算 $4$ 到 $8$ 筆序列資料&lt;/li&gt;
      &lt;li&gt;使用 truncated BPTT，一次只計算 $20$ 個 time steps&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN 的非遞迴隱藏層使用 sigmoid activation，遞迴的隱藏層不使用 activation function&lt;/li&gt;
  &lt;li&gt;LSTM 架構請參考 $\eqref{7} \eqref{8} \eqref{11} \eqref{12}$&lt;/li&gt;
  &lt;li&gt;由於未來時間的資訊有助於提升預測的準確度，因此模型預測會在延遲 $5$ 幀後開始輸出
    &lt;ul&gt;
      &lt;li&gt;ex: 第 $0$ 幀到第 $4$ 幀輸入完後預測第 $0$ 幀的 $40$ 維特徵所對應到的狀態&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;實驗 1：驗證資料的表現結果&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 2：在 $n_o = 126$ 時的驗證資料的表現結果。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/qjEOPv9.png&quot; alt=&quot;圖 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 3：在 $n_o = 2000$ 時的驗證資料的表現結果。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/II30qkv.png&quot; alt=&quot;圖 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 4：在 $n_o = 8000$ 時的驗證資料的表現結果。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Fq8vYNQ.png&quot; alt=&quot;圖 4&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;圖中的實驗名稱包含架構資訊
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c2048&lt;/code&gt; 代表 $n_c = 2048$&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r512&lt;/code&gt; 代表 $n_r = 512$&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p256&lt;/code&gt; 代表 $n_p = 256$&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10w5_6_704&lt;/code&gt; 代表輸入包含過去 $10$ 幀與未來 $5$ 幀，隱藏層有 $6$ 層，每個隱藏層維度為 $704$&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lr_256&lt;/code&gt; 代表故意降維成 $256$ 維的全連接層，目的是為了和 LSTM 公平的比較&lt;/li&gt;
      &lt;li&gt;括號中的數字代表總參數量&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN 在 $n_o = 126$ 時表現已經比 DNN 與 LSTM 差，因此後續實驗不討論 RNN
    &lt;ul&gt;
      &lt;li&gt;在訓練過程作者發現 RNN 非常不穩定，必須要額外進行 gradients clipping 確保不會產生 gradient explosion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LSTM 表現最好而且收斂速度最快
    &lt;ul&gt;
      &lt;li&gt;在採用作者提出的架構下，使用 projection 的 LSTM 比原本 LSTM 表現還要好，使用的參數也比較少&lt;/li&gt;
      &lt;li&gt;在採用 $n_p &amp;gt; 0$ 的架構下，大部份實驗都比 $n_p = 0$ 的架構表現還要好，唯一的例外是&lt;a href=&quot;#paper-fig-2&quot;&gt;圖 3&lt;/a&gt; 的實驗，作者認為是 learning rate 不小心調的太小&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;實驗 2：測試資料的表現結果&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 5：在 $n_o = 126$ 時的測試資料的表現結果。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/H4omLt0.png&quot; alt=&quot;圖 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-6&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 6：在 $n_o = 2000$ 時的測試資料的表現結果。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/xJpYoZY.png&quot; alt=&quot;圖 6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-7&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 7：在 $n_o = 8000$ 時的測試資料的表現結果。
圖片來源：&lt;a href=&quot;https://research.google/pubs/pub43895/&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/3rG7mzq.png&quot; alt=&quot;圖 7&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;作者說有些模型還沒完全收斂，他會更新實驗結果，很顯然他忘記了&lt;/li&gt;
  &lt;li&gt;簡單來說作者提出的 LSTM 架構就是表現比較好
    &lt;ul&gt;
      &lt;li&gt;如果使用 &lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;LSTM-2002&lt;/a&gt; 而不是作者的架構，則表現會比 DNN 還差&lt;/li&gt;
      &lt;li&gt;單純的增加 DNN 的層數也可以讓表現變好&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>[&quot;Hasim Sak&quot;, &quot;Andrew W. Senior&quot;, &quot;Françoise Beaufays&quot;]</name></author><category term="Acoustic Modeling" /><category term="LSTM" /><category term="LSTMP" /><category term="model architecture" /><category term="neural network" /><summary type="html">目標 嘗試以 LSTM 進行字典範圍較大的語音辨識 作者 Hasim Sak, Andrew W. Senior, Françoise Beaufays 隸屬單位 Google 期刊/會議名稱 arXiv 發表時間 2014 論文連結 https://research.google/pubs/pub43895/</summary></entry><entry><title type="html">Learning Precise Timing with LSTM Recurrent Networks</title><link href="/general%20sequence%20modeling/2021/12/29/learning-precise-timing-with-lstm-recurrent-networks.html" rel="alternate" type="text/html" title="Learning Precise Timing with LSTM Recurrent Networks" /><published>2021-12-29T16:28:00+08:00</published><updated>2021-12-29T16:28:00+08:00</updated><id>/general%20sequence%20modeling/2021/12/29/learning-precise-timing-with-lstm-recurrent-networks</id><content type="html" xml:base="/general%20sequence%20modeling/2021/12/29/learning-precise-timing-with-lstm-recurrent-networks.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;在 LSTM 上加入 peephole connections&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;Felix A. Gers, Nicol N. Schraudolph, Jürgen Schmidhuber&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隸屬單位&lt;/td&gt;
      &lt;td&gt;IDSIA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;JMLR, Volume 3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;2002&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;https://www.jmlr.org/papers/v3/gers02a.html&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--
  Define LaTeX command which will be used through out the writing.

  Each command must be wrapped with $ signs.
  We use &quot;display: none;&quot; to avoid redudant whitespaces.
 --&gt;

&lt;p style=&quot;display: none;&quot;&gt;

  &lt;!-- Operator in. --&gt;
  $\providecommand{\opnet}{}$
  $\renewcommand{\opnet}{\operatorname{net}}$
  &lt;!-- Operator in. --&gt;
  $\providecommand{\opin}{}$
  $\renewcommand{\opin}{\operatorname{in}}$
  &lt;!-- Operator out. --&gt;
  $\providecommand{\opout}{}$
  $\renewcommand{\opout}{\operatorname{out}}$
  &lt;!-- Operator cell block. --&gt;
  $\providecommand{\opblk}{}$
  $\renewcommand{\opblk}{\operatorname{block}}$
  &lt;!-- Operator cell multiplicative forget gate. --&gt;
  $\providecommand{\opfg}{}$
  $\renewcommand{\opfg}{\operatorname{fg}}$
  &lt;!-- Operator cell multiplicative input gate. --&gt;
  $\providecommand{\opig}{}$
  $\renewcommand{\opig}{\operatorname{ig}}$
  &lt;!-- Operator cell multiplicative output gate. --&gt;
  $\providecommand{\opog}{}$
  $\renewcommand{\opog}{\operatorname{og}}$
  &lt;!-- Operator sequence. --&gt;
  $\providecommand{\opseq}{}$
  $\renewcommand{\opseq}{\operatorname{seq}}$
  &lt;!-- Operator loss. --&gt;
  $\providecommand{\oploss}{}$
  $\renewcommand{\oploss}{\operatorname{loss}}$
  &lt;!-- Operator tri. --&gt;
  $\providecommand{\optri}{}$
  $\renewcommand{\optri}{\operatorname{tri}}$
  &lt;!-- Operator rect. --&gt;
  $\providecommand{\oprect}{}$
  $\renewcommand{\oprect}{\operatorname{rect}}$
  &lt;!-- Operator mod. --&gt;
  $\providecommand{\opmod}{}$
  $\renewcommand{\opmod}{\operatorname{mod}}$

  &lt;!-- Net input. --&gt;
  $\providecommand{\net}{}$
  $\renewcommand{\net}[2]{\opnet_{#1}(#2)}$
  &lt;!-- Net input with activatiton f. --&gt;
  $\providecommand{\fnet}{}$
  $\renewcommand{\fnet}[2]{f_{#1}\big(\net{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input. --&gt;
  $\providecommand{\dfnet}{}$
  $\renewcommand{\dfnet}[2]{f_{#1}'\big(\net{#1}{#2}\big)}$

  &lt;!-- Input dimension. --&gt;
  $\providecommand{\din}{}$
  $\renewcommand{\din}{d_{\opin}}$
  &lt;!-- Output dimension. --&gt;
  $\providecommand{\dout}{}$
  $\renewcommand{\dout}{d_{\opout}}$
  &lt;!-- Cell block dimension. --&gt;
  $\providecommand{\dblk}{}$
  $\renewcommand{\dblk}{d_{\opblk}}$

  &lt;!-- Number of cell blocks. --&gt;
  $\providecommand{\nblk}{}$
  $\renewcommand{\nblk}{n_{\opblk}}$

  &lt;!-- Cell block k. --&gt;
  $\providecommand{\blk}{}$
  $\renewcommand{\blk}[1]{\opblk^{#1}}$

  &lt;!-- Weight of multiplicative forget gate. --&gt;
  $\providecommand{\wfg}{}$
  $\renewcommand{\wfg}{w^{\opfg}}$
  $\providecommand{\ufg}{}$
  $\renewcommand{\ufg}{u^{\opfg}}$
  &lt;!-- Weight of multiplicative input gate. --&gt;
  $\providecommand{\wig}{}$
  $\renewcommand{\wig}{w^{\opig}}$
  $\providecommand{\uig}{}$
  $\renewcommand{\uig}{u^{\opig}}$
  &lt;!-- Weight of multiplicative output gate. --&gt;
  $\providecommand{\wog}{}$
  $\renewcommand{\wog}{w^{\opog}}$
  $\providecommand{\uog}{}$
  $\renewcommand{\uog}{u^{\opog}}$
  &lt;!-- Weight of cell units. --&gt;
  $\providecommand{\wblk}{}$
  $\renewcommand{\wblk}[1]{w^{\blk{#1}}}$
  &lt;!-- Weight of output units. --&gt;
  $\providecommand{\wout}{}$
  $\renewcommand{\wout}{w^{\opout}}$

  &lt;!-- Net input of multiplicative forget gate. --&gt;
  $\providecommand{\netfg}{}$
  $\renewcommand{\netfg}[2]{\opnet_{#1}^{\opfg}(#2)}$
  &lt;!-- Net input of multiplicative forget gate with activatiton f. --&gt;
  $\providecommand{\fnetfg}{}$
  $\renewcommand{\fnetfg}[2]{f_{#1}^{\opfg}\big(\netfg{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input of forget gate. --&gt;
  $\providecommand{\dfnetfg}{}$
  $\renewcommand{\dfnetfg}[2]{f_{#1}^{\opfg}{'}\big(\netfg{#1}{#2}\big)}$
  &lt;!-- Net input of multiplicative input gate. --&gt;
  $\providecommand{\netig}{}$
  $\renewcommand{\netig}[2]{\opnet_{#1}^{\opig}(#2)}$
  &lt;!-- Net input of multiplicative input gate with activatiton f. --&gt;
  $\providecommand{\fnetig}{}$
  $\renewcommand{\fnetig}[2]{f_{#1}^{\opig}\big(\netig{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input of input gate. --&gt;
  $\providecommand{\dfnetig}{}$
  $\renewcommand{\dfnetig}[2]{f_{#1}^{\opig}{'}\big(\netig{#1}{#2}\big)}$
  &lt;!-- Net input of multiplicative output gate. --&gt;
  $\providecommand{\netog}{}$
  $\renewcommand{\netog}[2]{\opnet_{#1}^{\opog}(#2)}$
  &lt;!-- Net input of multiplicative output gate with activatiton f. --&gt;
  $\providecommand{\fnetog}{}$
  $\renewcommand{\fnetog}[2]{f_{#1}^{\opog}\big(\netog{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input of output gate. --&gt;
  $\providecommand{\dfnetog}{}$
  $\renewcommand{\dfnetog}[2]{f_{#1}^{\opog}{'}\big(\netog{#1}{#2}\big)}$
  &lt;!-- Net input of output units. --&gt;
  $\providecommand{\netout}{}$
  $\renewcommand{\netout}[2]{\opnet_{#1}^{\opout}(#2)}$
  &lt;!-- Net input of output units with activatiton f. --&gt;
  $\providecommand{\fnetout}{}$
  $\renewcommand{\fnetout}[2]{f_{#1}^{\opout}\big(\netout{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input of output units. --&gt;
  $\providecommand{\dfnetout}{}$
  $\renewcommand{\dfnetout}[2]{f_{#1}^{\opout}{'}\big(\netout{#1}{#2}\big)}$

  &lt;!-- Net input of cell unit. --&gt;
  $\providecommand{\netblk}{}$
  $\renewcommand{\netblk}[3]{\opnet_{#1}^{\blk{#2}}(#3)}$
  &lt;!-- Net input of cell unit with activatiton g. --&gt;
  $\providecommand{\gnetblk}{}$
  $\renewcommand{\gnetblk}[3]{g_{#1}\big(\netblk{#1}{#2}{#3}\big)}$
  &lt;!-- Derivative of g with respect to net input of cell unit. --&gt;
  $\providecommand{\dgnetblk}{}$
  $\renewcommand{\dgnetblk}[3]{g_{#1}'\big(\netblk{#1}{#2}{#3}\big)}$
  &lt;!-- Cell unit with activatiton h. --&gt;
  $\providecommand{\hblk}{}$
  $\renewcommand{\hblk}[3]{h_{#1}\big(s_{#1}^{\blk{#2}}(#3)\big)}$

  &lt;!-- Gradient approximation by truncating gradient. --&gt;
  $\providecommand{\aptr}{}$
  $\renewcommand{\aptr}{\approx_{\operatorname{tr}}}$
&lt;/p&gt;

&lt;!-- End LaTeX command define section. --&gt;

&lt;h2 id=&quot;section&quot;&gt;重點&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 與 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 都沒有 peephole connections
    &lt;ul&gt;
      &lt;li&gt;論文提議的 peephole connections 是只連接到相同的記憶單元&lt;/li&gt;
      &lt;li&gt;現今常用的 LSTM 使用 peephole connections 的方法是全連接，例如 &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html&quot;&gt;PyTorch 實作的 LSTM&lt;/a&gt; 就是一個例子&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 細節可以看&lt;a href=&quot;/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html&quot;&gt;我的筆記&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 細節可以看&lt;a href=&quot;/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html&quot;&gt;我的筆記&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;這篇論文終於把過去兩篇論文寫錯的數學式改對了&lt;/li&gt;
  &lt;li&gt;作者認為在不給予 LSTM 模型任何的輸入時， LSTM 必須要能夠觀察記憶單元內部狀態的變化才能模擬週期函數
    &lt;ul&gt;
      &lt;li&gt;例如音樂節奏辨識&lt;/li&gt;
      &lt;li&gt;LSTM + peephole connections 在實驗中能夠成功解決模擬週期函數的任務&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;從&lt;a href=&quot;#paper-fig-15&quot;&gt;圖 15&lt;/a&gt; 可以發現模型的初始計算狀態為 $0$，但開始計算後模型計算狀態再也不為 $0$
    &lt;ul&gt;
      &lt;li&gt;這表示模型&lt;strong&gt;初始計算狀態&lt;/strong&gt;應該也被當成&lt;strong&gt;參數&lt;/strong&gt;一起訓練&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;作者認為 RNN 模型在記憶上仍然有問題，即使使用 LSTM 記憶的容量仍然被記憶單元的個數限制，並且無法只靠簡單增加記憶單元個數解決
    &lt;ul&gt;
      &lt;li&gt;與現今的 transformers 想法不同，大家都在搞大型 pre-trained model&lt;/li&gt;
      &lt;li&gt;作者認為有效解決記憶容量問題的模型架構仍然未被發現&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LSTM 所採用的 truncated BPTT 最佳化演算法會導致模型沒辦法有效的學習遞迴的資訊
    &lt;ul&gt;
      &lt;li&gt;根據作者實驗，當序列資料有大量雜訊時不做特殊的前處理就無法進行訓練&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lstm-&quot;&gt;原始 LSTM 架構&lt;/h2&gt;

&lt;h3 id=&quot;section-1&quot;&gt;模型架構&lt;/h3&gt;

&lt;p&gt;根據 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 提出的架構如下（這篇論文不使用額外的&lt;strong&gt;隱藏單元&lt;/strong&gt;，因此我們也完全不列出隱藏單元相關的公式）（細節可以參考&lt;a href=&quot;/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html&quot;&gt;我的筆記&lt;/a&gt;）&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;符號&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\din$&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;輸入層&lt;/strong&gt;的維度&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;記憶單元&lt;/strong&gt;的維度&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;記憶單元&lt;/strong&gt;的個數&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dout$&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;輸出層&lt;/strong&gt;的維度&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$T$&lt;/td&gt;
      &lt;td&gt;輸入序列的長度&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;以下所有符號的時間 $t$ 範圍為 $t \in \set{1, \dots, T}$&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;符號&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;維度&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$x(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸入&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\din$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y^{\opfg}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點的&lt;strong&gt;遺忘閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$y^{\opfg}(0) = 0$，同一個記憶單元&lt;strong&gt;共享遺忘閘門&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y^{\opig}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點的&lt;strong&gt;輸入閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$y^{\opig}(0) = 0$，同一個記憶單元&lt;strong&gt;共享輸入閘門&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y^{\opog}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點的&lt;strong&gt;輸出閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$y^{\opog}(0) = 0$，同一個記憶單元&lt;strong&gt;共享輸出閘門&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$s^{\blk{k}}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點的第 $k$ 個&lt;strong&gt;記憶單元區塊內部狀態&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$s^{\blk{k}}(0) = 0$ 且 $k \in \set{1, \dots, \nblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y^{\blk{k}}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點的第 $k$ 個&lt;strong&gt;記憶單元區塊輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$y^{\blk{k}}(0) = 0$ 且 $k \in \set{1, \dots, \nblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\dout$&lt;/td&gt;
      &lt;td&gt;由 $t$ 時間點的&lt;strong&gt;輸入&lt;/strong&gt;與&lt;strong&gt;記憶單元輸出&lt;/strong&gt;透過&lt;strong&gt;全連接&lt;/strong&gt;產生，因此沒有 $y(0)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\hat{y}(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;預測目標&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\dout$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;符號&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;下標範圍&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$x_j(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的第 $j$ 個&lt;strong&gt;輸入&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$j \in \set{1, \dots, \din}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_k^{\opfg}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點第 $k$ 個記憶單元區塊的&lt;strong&gt;遺忘閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$k \in \set{1, \dots, \nblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_k^{\opig}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點第 $k$ 個記憶單元區塊的&lt;strong&gt;輸入閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$k \in \set{1, \dots, \nblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_k^{\opog}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點第 $k$ 個記憶單元區塊的&lt;strong&gt;輸出閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$k \in \set{1, \dots, \nblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$s_i^{\blk{k}}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點的第 $k$ 個&lt;strong&gt;記憶單元區塊&lt;/strong&gt;的第 $i$ 個&lt;strong&gt;記憶單元內部狀態&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$i \in \set{1, \dots, \dblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_i^{\blk{k}}(t - 1)$&lt;/td&gt;
      &lt;td&gt;第 $t - 1$ 個時間點的第 $k$ 個&lt;strong&gt;記憶單元區塊&lt;/strong&gt;的第 $i$ 個&lt;strong&gt;記憶單元輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$i \in \set{1, \dots, \dblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_i(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的第 $i$ 個&lt;strong&gt;輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$i \in \set{1, \dots, \dout}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\hat{y}_i(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的第 $i$ 個&lt;strong&gt;預測目標&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$i \in \set{1, \dots, \dout}$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;輸出維度&lt;/th&gt;
      &lt;th&gt;輸入維度&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\wfg$&lt;/td&gt;
      &lt;td&gt;產生&lt;strong&gt;遺忘閘門&lt;/strong&gt;的全連接參數&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$\din + \nblk \cdot (3 + \dblk)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\wig$&lt;/td&gt;
      &lt;td&gt;產生&lt;strong&gt;輸入閘門&lt;/strong&gt;的全連接參數&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$\din + \nblk \cdot (3 + \dblk)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\wog$&lt;/td&gt;
      &lt;td&gt;產生&lt;strong&gt;輸出閘門&lt;/strong&gt;的全連接參數&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$\din + \nblk \cdot (3 + \dblk)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\wblk{k}$&lt;/td&gt;
      &lt;td&gt;產生第 $k$ 個&lt;strong&gt;記憶單元淨輸入&lt;/strong&gt;的全連接參數&lt;/td&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$\din + \nblk \cdot (3 + \dblk)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\wout$&lt;/td&gt;
      &lt;td&gt;產生&lt;strong&gt;輸出&lt;/strong&gt;的全連接參數&lt;/td&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$\din + \nblk \cdot \dblk$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;定義 $\sigma$ 為 sigmoid 函數 $\sigma(x) = \frac{1}{1 + e^{-x}}$&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;函數&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;公式&lt;/th&gt;
      &lt;th&gt;range&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$f_k^{\opfg}$&lt;/td&gt;
      &lt;td&gt;第 $k$ 個&lt;strong&gt;遺忘閘門&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;$[0, 1]$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$f_k^{\opig}$&lt;/td&gt;
      &lt;td&gt;第 $k$ 個&lt;strong&gt;輸入閘門&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;$[0, 1]$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$f_k^{\opog}$&lt;/td&gt;
      &lt;td&gt;第 $k$ 個&lt;strong&gt;輸出閘門&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;$[0, 1]$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$g_i^{\blk{k}}$&lt;/td&gt;
      &lt;td&gt;第 $k$ 個&lt;strong&gt;記憶單元&lt;/strong&gt;第 $i$ 個&lt;strong&gt;內部狀態&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$4\sigma - 2$&lt;/td&gt;
      &lt;td&gt;$[-2, 2]$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$h_i^{\blk{k}}$&lt;/td&gt;
      &lt;td&gt;第 $k$ 個&lt;strong&gt;記憶單元&lt;/strong&gt;第 $i$ 個&lt;strong&gt;輸出&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$2\sigma - 1$&lt;/td&gt;
      &lt;td&gt;$[-1, 1]$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$f_i^{\opout}$&lt;/td&gt;
      &lt;td&gt;第 $i$ 個&lt;strong&gt;輸出&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;$[0, 1]$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在 $t$ 時間點時得到&lt;strong&gt;輸入&lt;/strong&gt; $x(t)$，產生 $t$ 時間點&lt;strong&gt;遺忘閘門&lt;/strong&gt; $y^{\opfg}(t)$、&lt;strong&gt;輸入閘門&lt;/strong&gt; $y^{\opig}(t)$ 與&lt;strong&gt;輸出閘門&lt;/strong&gt; $y^{\opog}(t)$ 的方法如下&lt;/p&gt;

\[\begin{align*}
g &amp;amp; \in \set{\opfg, \opig, \opog} \\
\opnet^g(t) &amp;amp; = w^g \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix} \\
y^g(t) &amp;amp; = f^g(\opnet^g(t))
\end{align*} \tag{1}\label{1}\]

&lt;ul&gt;
  &lt;li&gt;注意與&lt;a href=&quot;/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html&quot;&gt;以前的筆記&lt;/a&gt;不同，這裡是產生 $t$ 時間點的資訊而不是 $t + 1$&lt;/li&gt;
  &lt;li&gt;注意是以 $t$ 時間點的輸入（不是 $t - 1$）與 $t - 1$ 時間點的計算狀態產生 $t$ 時間點的計算狀態&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;利用 $\eqref{1}$ 產生 $t$ 時間點的&lt;strong&gt;記憶單元內部狀態&lt;/strong&gt; $s^{\blk{k}}(t)$ 方法如下&lt;/p&gt;

\[\begin{align*}
k &amp;amp; \in \set{1, \dots, \nblk} \\
\opnet^{\blk{k}}(t) &amp;amp; = \wblk{k} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix} \\
s^{\blk{k}}(t) &amp;amp; = y_k^{\opfg}(t) \cdot s^{\blk{k}}(t - 1) + y_k^{\opig}(t) \cdot g^{\blk{k}}(\opnet^{\blk{k}}(t))
\end{align*} \tag{2}\label{2}\]

&lt;p&gt;注意第 $k$ 個記憶單元內部狀態&lt;strong&gt;共享遺忘閘門&lt;/strong&gt; $y_k^{\opfg}(t)$ 與&lt;strong&gt;輸入閘門&lt;/strong&gt; $y_k^{\opig}(t)$。&lt;/p&gt;

&lt;p&gt;利用 $\eqref{1}\eqref{2}$ 產生 $t$ 時間點的&lt;strong&gt;記憶單元輸出&lt;/strong&gt; $y^{\blk{k}}(t)$ 方法如下&lt;/p&gt;

\[\begin{align*}
k &amp;amp; \in \set{1, \dots, \nblk} \\
y^{\blk{k}}(t) &amp;amp; = y_k^{\opog}(t) \cdot h^{\blk{k}}(s^{\blk{k}}(t))
\end{align*} \tag{3}\label{3}\]

&lt;p&gt;注意第 $k$ 個記憶單元輸出&lt;strong&gt;共享輸出閘門&lt;/strong&gt; $y_k^{\opog}(t)$。
由於實驗結果作者認為 $h^{\blk{k}}$ 不是很重要，因此 $\eqref{3}$ 中的式子改為&lt;/p&gt;

\[y^{\blk{k}}(t) = y_k^{\opog}(t) \cdot s^{\blk{k}}(t) \quad k = 1, \dots, \nblk \tag{4}\label{4}\]

&lt;p&gt;產生 $t$ 時間點的&lt;strong&gt;輸出&lt;/strong&gt;是透過 $t$ 時間點的&lt;strong&gt;輸入&lt;/strong&gt;與&lt;strong&gt;記憶單元輸出&lt;/strong&gt;（見 $\eqref{4}$）而得（注意是 $t$ 時間點不是 $t - 1$，代表&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 與 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 都寫錯了）&lt;/p&gt;

\[\begin{align*}
\opnet^{\opout}(t) &amp;amp; = \wout \cdot \begin{pmatrix}
x(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix} \\
y(t) &amp;amp; = f^{\opout}(\opnet^{\opout}(t))
\end{align*} \tag{5}\label{5}\]

&lt;h3 id=&quot;section-2&quot;&gt;最佳化&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM&lt;/a&gt; 提出與 truncated BPTT 相似的概念，透過 RTRL 進行參數更新，並故意&lt;strong&gt;丟棄流出記憶單元的所有梯度&lt;/strong&gt;，避免梯度爆炸或梯度消失的問題，同時節省更新所需的空間與時間（local in time and space）。（細節可見&lt;a href=&quot;/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html&quot;&gt;我的筆記&lt;/a&gt;）&lt;/p&gt;

&lt;p&gt;令 $t = 1, \dots, T$，最佳化的目標為每個時間點 $t$ 所產生的&lt;strong&gt;平方誤差總和最小化&lt;/strong&gt;&lt;/p&gt;

\[\begin{align*}
\oploss(t) &amp;amp; = \sum_{i = 1}^{\dout} \oploss_i(t) \\
&amp;amp; = \sum_{i = 1}^{\dout} \frac{1}{2} \big(y_i(t) - \hat{y}_i(t)\big)^2
\end{align*} \tag{6}\label{6}\]

&lt;p&gt;以下我們使用 $\aptr$ 代表&lt;strong&gt;丟棄部份梯度後的剩餘梯度&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;注意：論文中的式子 7 與 8 互相矛盾，式子 8 應改為 $\triangle w_{k m}(t) = \alpha \delta_k(t) y_m(t)$&lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;輸出參數的剩餘梯度&lt;/h4&gt;

\[\begin{align*}
\pd{\oploss(t)}{\wout_{i, j}} &amp;amp; = \pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pd{\netout{i}{t}}{\wout_{i, j}} \\
&amp;amp; = \big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_j
\end{align*} \tag{7}\label{7}\]

&lt;p&gt;其中 $1 \leq i \leq \dout$ 且 $1 \leq j \leq \din + \nblk \cdot \dblk$。&lt;/p&gt;

&lt;h4 id=&quot;section-4&quot;&gt;輸出閘門參數的剩餘梯度&lt;/h4&gt;

\[\begin{align*}
\pd{\oploss(t)}{\wog_{k, q}} &amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \\
&amp;amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{y_k^{\opog}(t)}} \cdot \pd{y_k^{\opog}(t)}{\netog{k}{t}} \cdot \pd{\netog{k}{t}}{\wog_{k, q}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \\
&amp;amp; \quad \pa{\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot s_j^{\blk{k}}(t)} \cdot \dfnetog{k}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q\Bigg]
\end{align*} \tag{8}\label{8}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。&lt;/p&gt;

&lt;h4 id=&quot;section-5&quot;&gt;輸入閘門參數的剩餘梯度&lt;/h4&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t)}{\wig_{k, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \pd{s_j^{\blk{k}}(t)}{\wig_{k, q}}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \\
&amp;amp; \quad \quad \br{y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\wig_{k, q}} + \pd{s_j^{\blk{k}}(t)}{y_k^{\opig}(t)} \cdot \pd{y_k^{\opig}(t)}{\netig{k}{t}} \cdot \pd{\netig{k}{t}}{\wig_{k, q}}}\Bigg)\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp;amp; \quad \quad \br{y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\wig_{k, q}} + \gnetblk{j}{k}{t} \cdot \dfnetig{k}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q}\Bigg)\Bigg]
\end{align*} \tag{9}\label{9}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。&lt;/p&gt;

&lt;h4 id=&quot;section-6&quot;&gt;遺忘閘門參數的剩餘梯度&lt;/h4&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t)}{\wfg_{k, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \pd{s_j^{\blk{k}}(t)}{\wfg_{k, q}}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \\
&amp;amp; \quad \quad \br{\pd{y_k^{\opfg}(t)}{\netfg{k}{t}} \cdot \pd{\netfg{k}{t}}{\wfg_{k, q}} \cdot s_j^{\blk{k}}(t - 1) + y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\wfg_{k, q}}}\Bigg)\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp;amp; \quad \quad \br{\dfnetfg{k}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q \cdot s_j^{\blk{k}}(t - 1) + y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\wfg_{k, q}}}\Bigg)\Bigg]
\end{align*} \tag{10}\label{10}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。&lt;/p&gt;

&lt;h4 id=&quot;section-7&quot;&gt;記憶單元淨輸入參數的剩餘梯度&lt;/h4&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t)}{\wblk{k}_{p, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \br{\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pd{\netout{i}{t}}{y_p^{\blk{k}}(t)} \cdot \pd{y_p^{\blk{k}}(t)}{s_p^{\blk{k}}(t)} \cdot \pd{s_p^{\blk{k}}(t)}{\wblk{k}_{p, q}}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pd{\netout{i}{t}}{y_p^{\blk{k}}(t)} \cdot \pd{y_p^{\blk{k}}(t)}{s_p^{\blk{k}}(t)} \cdot \\
&amp;amp; \quad \quad \pa{f_k^{\opfg}(t) \cdot \pd{s_p^{\blk{k}}(t - 1)}{\wblk{k}_{p, q}} + \pd{s_p^{\blk{k}}(t)}{\gnetblk{j}{k}{t}} \cdot \pd{\gnetblk{j}{k}{t}}{\netblk{j}{k}{t}} \cdot \pd{\netblk{j}{k}{t}}{\wblk{k}_{p, q}}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp;amp; \quad \br{f_k^{\opfg}(t) \cdot \pd{s_p^{\blk{k}}(t - 1)}{\wblk{k}_{p, q}} + y_k^{\opig}(t) \cdot \dgnetblk{p}{k}{t} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q}\Bigg]
\end{align*} \tag{11}\label{11}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$， $1 \leq p \leq \dblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。&lt;/p&gt;

&lt;h4 id=&quot;section-8&quot;&gt;梯度下降&lt;/h4&gt;

&lt;p&gt;計算完上述所有參數後使用&lt;strong&gt;梯度下降&lt;/strong&gt;（gradient descent）進行參數更新&lt;/p&gt;

\[\begin{align*}
\wout_{i, j} &amp;amp; \leftarrow \wout_{i, j} - \alpha \cdot \pd{\oploss(t)}{\wout_{i, j}} \\
\wog_{k, q} &amp;amp; \leftarrow \wog_{k, q} - \alpha \cdot \pd{\oploss(t)}{\wog_{k, q}} \\
\wig_{k, q} &amp;amp; \leftarrow \wig_{k, q} - \alpha \cdot \pd{\oploss(t)}{\wig_{k, q}} \\
\wfg_{k, q} &amp;amp; \leftarrow \wig_{k, q} - \alpha \cdot \pd{\oploss(t)}{\wfg_{k, q}} \\
\wblk{k}_{p, q} &amp;amp; \leftarrow \wblk{k}_{p, q} - \alpha \cdot \pd{\oploss(t)}{\wblk{k}_{p, q}}
\end{align*} \tag{12}\label{12}\]

&lt;p&gt;其中 $\alpha$ 為&lt;strong&gt;學習率&lt;/strong&gt;（&lt;strong&gt;learning rate&lt;/strong&gt;）。&lt;/p&gt;

&lt;p&gt;由於使用基於 RTRL 的最佳化演算法，因此每個時間點 $t$ 計算完誤差後就可以更新參數。&lt;/p&gt;

&lt;h3 id=&quot;section-9&quot;&gt;問題&lt;/h3&gt;

&lt;p&gt;由於&lt;strong&gt;輸出閘門&lt;/strong&gt;為 $0$ 時記憶單元的輸出等同於 $0$，導致基於記憶單元輸出計算所得的閘門與記憶單元本身無法觀察到&lt;strong&gt;記憶單元的內部狀態&lt;/strong&gt;，作者認為在後續提出的任務中此現象會影響模型的表現。&lt;/p&gt;

&lt;h2 id=&quot;lstm--peephole-connections&quot;&gt;LSTM + Peephole Connections&lt;/h2&gt;

&lt;h3 id=&quot;section-10&quot;&gt;模型架構&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 1：LSTM 加上 peephole connections。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/G7Pgl3D.png&quot; alt=&quot;圖 1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;針對前述問題提出的解決方法為 peephole connections
    &lt;ul&gt;
      &lt;li&gt;所有閘門與記憶單元內部狀態相接&lt;/li&gt;
      &lt;li&gt;最佳化時梯度不會經由 peephole connections 傳播（手動將梯度設為 $0$）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此 $\eqref{1}$ 中的&lt;strong&gt;遺忘閘門&lt;/strong&gt;與&lt;strong&gt;輸入閘門&lt;/strong&gt;計算方法改成如下：&lt;/p&gt;

\[\begin{align*}
g &amp;amp; \in \set{\opfg, \opig} \\
\opnet_k^g(t) &amp;amp; = \sum_{q = 1}^{\din + \nblk \cdot (3 + \dblk)} w_{k, q}^g \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q + u_k^g \odot s^{\blk{k}}(t - 1) \\
y^g(t) &amp;amp; = f^g(\opnet^g(t))
\end{align*} \tag{13}\label{13}\]

&lt;p&gt;其中 $\ufg_k, \uig_k$ 的維度為 $1 \times \dblk$，$k$ 的範圍為 $1, \dots, \nblk$。&lt;/p&gt;

&lt;p&gt;$\eqref{13}$ 的計算表示 $t$ 時間點的&lt;strong&gt;遺忘閘門&lt;/strong&gt;與&lt;strong&gt;輸入閘門&lt;/strong&gt;會與 $t - 1$ 時間點的&lt;strong&gt;記憶單元內部狀態相連&lt;/strong&gt;，並且閘門只會與對應的記憶單元連接。&lt;/p&gt;

&lt;p&gt;$\eqref{2}$ 的計算方法不變，在完成 $\eqref{2}$ 的計算後以 $t$ 時間點的記憶單元內部狀態計算&lt;strong&gt;輸出閘門&lt;/strong&gt;（注意不是 $t - 1$）：&lt;/p&gt;

\[\begin{align*}
\opnet_k^{\opog}(t) &amp;amp; = \sum_{q = 1}^{\din + \nblk \cdot (3 + \dblk)} \wog_{k, q} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t - 1) \\
y^{\opig}(t - 1) \\
y^{\opog}(t - 1) \\
y^{\blk{1}}(t - 1) \\
\vdots \\
y^{\blk{\nblk}}(t - 1)
\end{pmatrix}_q + \uog_k \odot s^{\blk{k}}(t) \\
y^{\opog}(t) &amp;amp; = f^{\opog}(\opnet^{\opog}(t))
\end{align*} \tag{14}\label{14}\]

&lt;p&gt;其中 $u_k^{\opog}$ 的維度為 $1 \times \dblk$，$k$ 的範圍為 $1, \dots, \nblk$。&lt;/p&gt;

&lt;p&gt;$\eqref{14}$ 的計算表示 $t$ 時間點的&lt;strong&gt;輸出閘門&lt;/strong&gt;會與 $t$ 時間點的&lt;strong&gt;記憶單元內部狀態相連&lt;/strong&gt;，並且閘門只會與對應的記憶單元連接。&lt;/p&gt;

&lt;p&gt;剩餘的計算方法（$\eqref{4}, \eqref{5}$）不變。&lt;/p&gt;

&lt;h3 id=&quot;section-11&quot;&gt;最佳化&lt;/h3&gt;

&lt;p&gt;由於只有閘門的計算方法受到影響，而且梯度不會流出 peephole connections，因此 $\eqref{8} \eqref{9} \eqref{10}$ 都不受影響，只需探討 $\ufg, \uig, \uog$ 的更新方法。&lt;/p&gt;

&lt;h4 id=&quot;section-12&quot;&gt;輸出閘門參數的剩餘梯度&lt;/h4&gt;

\[\begin{align*}
\pd{\oploss(t)}{\uog_{k, q}} &amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \\
&amp;amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{y_k^{\opog}(t)}} \cdot \pd{y_k^{\opog}(t)}{\netog{k}{t}} \cdot \pd{\netog{k}{t}}{\uog_{k, q}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \\
&amp;amp; \quad \pa{\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot s_j^{\blk{k}}(t)} \cdot \dfnetog{k}{t} \cdot s_q^{\blk{k}}(t)\Bigg]
\end{align*} \tag{15}\label{15}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \dblk$，$\eqref{15}$ 式就是論文的 24 式。&lt;/p&gt;

&lt;h4 id=&quot;section-13&quot;&gt;輸入閘門參數的剩餘梯度&lt;/h4&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t)}{\uig_{k, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \pd{s_j^{\blk{k}}(t)}{\uig_{k, q}}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \\
&amp;amp; \quad \quad \br{y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\uig_{k, q}} + \pd{s_j^{\blk{k}}(t)}{y_k^{\opig}(t)} \cdot \pd{y_k^{\opig}(t)}{\netig{k}{t}} \cdot \pd{\netig{k}{t}}{\uig_{k, q}}}\Bigg)\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp;amp; \quad \quad \br{y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\uig_{k, q}} + \gnetblk{j}{k}{t} \cdot \dfnetig{k}{t} \cdot s_q^{\blk{k}}(t - 1)}\Bigg)\Bigg]
\end{align*} \tag{16}\label{16}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \dblk$，$\eqref{16}$ 式就是論文的 22 式。&lt;/p&gt;

&lt;h4 id=&quot;section-14&quot;&gt;遺忘閘門參數的剩餘梯度&lt;/h4&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t)}{\ufg_{k, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \pd{s_j^{\blk{k}}(t)}{\ufg_{k, q}}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t)}{y_i(t)} \cdot \pd{y_i(t)}{\netout{i}{t}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t}}{y_j^{\blk{k}}(t)} \cdot \pd{y_j^{\blk{k}}(t)}{s_j^{\blk{k}}(t)} \cdot \\
&amp;amp; \quad \quad \br{\pd{y_k^{\opfg}(t)}{\netfg{k}{t}} \cdot \pd{\netfg{k}{t}}{\ufg_{k, q}} \cdot s_j^{\blk{k}}(t - 1) + y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\ufg_{k, q}}}\Bigg)\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t) - \hat{y}_i(t)\big) \cdot \dfnetout{i}{t} \cdot \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t) \cdot \\
&amp;amp; \quad \quad \br{\dfnetfg{k}{t} \cdot s_q^{\blk{k}}(t - 1) \cdot s_j^{\blk{k}}(t - 1) + y_k^{\opfg}(t) \cdot \pd{s_j^{\blk{k}}(t - 1)}{\ufg_{k, q}}}\Bigg)\Bigg]
\end{align*} \tag{17}\label{17}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \dblk$，$\eqref{17}$ 式就是論文的 23 式。&lt;/p&gt;

&lt;h4 id=&quot;section-15&quot;&gt;梯度下降&lt;/h4&gt;

&lt;p&gt;計算完上述所有參數後使用&lt;strong&gt;梯度下降&lt;/strong&gt;（gradient descent）進行參數更新&lt;/p&gt;

\[\begin{align*}
\uog_{k, q} &amp;amp; \leftarrow \uog_{k, q} - \alpha \cdot \pd{\oploss(t)}{\uog_{k, q}} \\
\uig_{k, q} &amp;amp; \leftarrow \uig_{k, q} - \alpha \cdot \pd{\oploss(t)}{\uig_{k, q}} \\
\ufg_{k, q} &amp;amp; \leftarrow \uig_{k, q} - \alpha \cdot \pd{\oploss(t)}{\ufg_{k, q}}
\end{align*} \tag{18}\label{18}\]

&lt;p&gt;由於使用基於 RTRL 的最佳化演算法，因此每個時間點 $t$ 計算完誤差後就可以更新參數。&lt;/p&gt;

&lt;h2 id=&quot;section-16&quot;&gt;實驗設計&lt;/h2&gt;

&lt;h3 id=&quot;section-17&quot;&gt;模型架構&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 2：實驗所採用的 LSTM 架構。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/l1IUgTV.png&quot; alt=&quot;圖 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所有實驗都使用相同架構，根據實驗作者發現少量的參數就可以達成所有任務。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;數值（或範圍）&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\din$&lt;/td&gt;
      &lt;td&gt;$1$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$1$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$1$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dout$&lt;/td&gt;
      &lt;td&gt;$1$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wblk{1})$&lt;/td&gt;
      &lt;td&gt;$\dblk \times [\din + \nblk \cdot \dblk + 1]$&lt;/td&gt;
      &lt;td&gt;只與輸入和記憶單元輸出相接，有額外使用偏差項&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wfg)$&lt;/td&gt;
      &lt;td&gt;$\nblk \times [\din + \nblk \cdot \dblk + 1]$&lt;/td&gt;
      &lt;td&gt;只與輸入和記憶單元輸出相接，有額外使用偏差項&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wig)$&lt;/td&gt;
      &lt;td&gt;$\nblk \times [\din + \nblk \cdot \dblk + 1]$&lt;/td&gt;
      &lt;td&gt;只與輸入和記憶單元輸出相接，有額外使用偏差項&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wog)$&lt;/td&gt;
      &lt;td&gt;$\nblk \times [\din + \nblk \cdot \dblk + 1]$&lt;/td&gt;
      &lt;td&gt;只與輸入和記憶單元輸出相接，有額外使用偏差項&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\ufg_k)$&lt;/td&gt;
      &lt;td&gt;$1 \times \dblk$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\uig_k)$&lt;/td&gt;
      &lt;td&gt;$1 \times \dblk$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\uog_k)$&lt;/td&gt;
      &lt;td&gt;$1 \times \dblk$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wout)$&lt;/td&gt;
      &lt;td&gt;$\dout \times [\nblk \cdot \dblk + 1]$&lt;/td&gt;
      &lt;td&gt;外部輸入沒有直接連接到總輸出，有額外使用偏差項&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;遺忘閘門偏差項初始值&lt;/td&gt;
      &lt;td&gt;$-2$&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 採用的初始值為正數，這裡居然用負數&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;輸入閘門偏差項初始值&lt;/td&gt;
      &lt;td&gt;$0$&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 採用的初始值為負數，這裡居然用 $0$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;輸出閘門偏差項初始值&lt;/td&gt;
      &lt;td&gt;$2$&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 採用的初始值為負數，這裡居然用正數&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;參數初始化範圍&lt;/td&gt;
      &lt;td&gt;$[-0.1, 0.1]$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$g^{\blk{k}}$&lt;/td&gt;
      &lt;td&gt;$g^{\blk{k}}(x) = x$&lt;/td&gt;
      &lt;td&gt;identity mapping&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$f^{\opout}$&lt;/td&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;只有在模擬週期函數任務中採用 identity mapping&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Learning rate&lt;/td&gt;
      &lt;td&gt;$10^{-5}$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;總參數量&lt;/td&gt;
      &lt;td&gt;$17$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-18&quot;&gt;實驗細節&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;與預測目標相減的絕對值作為誤差進行評估
    &lt;ul&gt;
      &lt;li&gt;在凸波延遲偵測與生成任務中誤差必須小於 $0.49$&lt;/li&gt;
      &lt;li&gt;在模擬週期函數任務中誤差必須小於 $0.3$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;連續輸入只會在以下其中一個條件發生時停止
    &lt;ul&gt;
      &lt;li&gt;單一時間點模型預測誤差過大&lt;/li&gt;
      &lt;li&gt;在訓練時成功連續預測 $100$ 個凸波延遲&lt;/li&gt;
      &lt;li&gt;在測試時成功連續預測 $1000$ 個凸波延遲&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;一次實驗最多進行 $10^7$ 次訓練，每執行一次訓練就進行一次測試
    &lt;ul&gt;
      &lt;li&gt;每次訓練模型都是接收連續輸入&lt;/li&gt;
      &lt;li&gt;在凸波延遲偵測任務中模型最多訓練 $10^8$ 次&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;總共實驗 $10$ 次，呈現平均實驗結果&lt;/li&gt;
  &lt;li&gt;訓練資料與測試資料皆為隨機產生，產生方法完全相同&lt;/li&gt;
  &lt;li&gt;梯度下降而外使用動量（momentum，細節請看&lt;a href=&quot;/optimization/2021/12/07/learning-representations-by-backpropagating-errors.html&quot;&gt;我的筆記&lt;/a&gt;），動量超參數以 $\eta$ 表示
    &lt;ul&gt;
      &lt;li&gt;在連續凸波延遲偵測中 $\eta = 0.9999$&lt;/li&gt;
      &lt;li&gt;在非連續凸波延遲偵測中 $\eta = 0.99$&lt;/li&gt;
      &lt;li&gt;在凸波生成中 $\eta = 0.999$&lt;/li&gt;
      &lt;li&gt;在模擬週期函數中 $\eta = 0.99$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;論文沒寫但我猜最佳化目標一樣是 MSE&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-19&quot;&gt;實驗 1：凸波延遲偵測&lt;/h2&gt;

&lt;h3 id=&quot;section-20&quot;&gt;任務定義&lt;/h3&gt;

&lt;p&gt;輸入只會是 $0$ 或 $1$，$1$ 代表凸波，輸入序列的產生方法如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第 $1$ 個凸波產生的時間點為 $T(1) = F + I(1)$
    &lt;ul&gt;
      &lt;li&gt;$F \in \N$ 代表凸波週期，是一個常數&lt;/li&gt;
      &lt;li&gt;$I(1) \in \N$ 代表第 $1$ 個週期的延遲時間&lt;/li&gt;
      &lt;li&gt;因此 $T(1) \geq F$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;令 $n \geq 2$，第 $n$ 個凸波產生的時間點為 $T(n) = T(n - 1) + F + I(n)$
    &lt;ul&gt;
      &lt;li&gt;$I(n) \in \N$ 代表第 $n$ 個週期的延遲時間&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型必須要預測每個凸波的延遲時間（Measuring Spike Delays，MSD）
    &lt;ul&gt;
      &lt;li&gt;令 $n \in \N$，任務等同於在第 $T(n)$ 時間點輸出 $I(n)$&lt;/li&gt;
      &lt;li&gt;已知週期 $F$，LSTM 必須在接收 $F - 1$ 個 $0$ 開始紀錄延遲的時間差&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;任務分為連續輸入（MSD）與非連續輸入（non MSD，NMSD）
    &lt;ul&gt;
      &lt;li&gt;NMSD 的版本一次訓練只有一筆資料，即 $n = 1$&lt;/li&gt;
      &lt;li&gt;MSD 的版本一次訓練有多筆資料串接，$n = 100$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-21&quot;&gt;實驗結果&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 3：凸波偵測實驗結果。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/RmIcNfd.png&quot; alt=&quot;圖 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 4：凸波偵測實驗結果，分析週期長度對於表現的影響。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/nEDzkG3.png&quot; alt=&quot;圖 4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 5：凸波偵測實驗結果，增加延遲可能範圍進行實驗。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/JlSE6y7.png&quot; alt=&quot;圖 5&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在 NMSD 任務中根據&lt;a href=&quot;#paper-fig-3&quot;&gt;圖 3&lt;/a&gt; 與&lt;a href=&quot;#paper-fig-4&quot;&gt;圖 4&lt;/a&gt; 實驗結果說明週期愈長（$F$ 愈大）愈不容易偵測
    &lt;ul&gt;
      &lt;li&gt;即使 $I(n) \in \set{0, 1}$ 在週期較長的狀況下偵測延遲仍然很困難&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;雖然 peephole connections 在這個任務中不重要，但仍然比 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 表現還要好&lt;/li&gt;
  &lt;li&gt;作者進一步將 $I(n)$ 的範圍調大，並且將 $f^{\opout}$ 從 sigmoid 函數改成 identity mapping（因為 sigmoid 的數值範圍只能落在 $[0, 1]$）進行實驗（見&lt;a href=&quot;#paper-fig-5&quot;&gt;圖 5&lt;/a&gt;）
    &lt;ul&gt;
      &lt;li&gt;令 $i \in \set{1, \dots, 10}$，$I(n)$ 可以是 $\set{0, i}$ 或 $\set{0, \dots, i}$&lt;/li&gt;
      &lt;li&gt;週期 $F = 10$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;在 NMSD 任務中根據&lt;a href=&quot;#paper-fig-5&quot;&gt;圖 5&lt;/a&gt; 可以得到以下結論
    &lt;ul&gt;
      &lt;li&gt;在此時驗中 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 表現比 peephole connection 好&lt;/li&gt;
      &lt;li&gt;延遲範圍差異愈大 LSTM 收斂愈快，作者認為過大的延遲差異會有明顯的特徵（見&lt;a href=&quot;#paper-fig-5&quot;&gt;圖 5&lt;/a&gt; 下半）&lt;/li&gt;
      &lt;li&gt;當預測範圍可能性變多時，當最大延遲不超過 $5$ 時容易收斂，一旦超過 $5$ 則收斂變慢（見&lt;a href=&quot;#paper-fig-5&quot;&gt;圖 5&lt;/a&gt; 上半）&lt;/li&gt;
      &lt;li&gt;在 $I(n) \in \set{0, 1}$ 時，使用 identity mapping 作為輸出函數表現比使用 sigmoid（見&lt;a href=&quot;#paper-fig-3&quot;&gt;圖 3&lt;/a&gt;）還要好，作者認為 sigmoid 會讓 gradient 變小所以收斂較慢&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-22&quot;&gt;分析&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-6&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 6：凸波偵測實驗中 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 的計算狀態。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ma5loA3.png&quot; alt=&quot;圖 6-1&quot; /&gt;
&lt;img src=&quot;https://i.imgur.com/adTLK96.png&quot; alt=&quot;圖 6-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-7&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 7：凸波偵測實驗中 &lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;LSTM + peephole connections&lt;/a&gt; 的計算狀態。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ZOukPCr.png&quot; alt=&quot;圖 7-1&quot; /&gt;
&lt;img src=&quot;https://i.imgur.com/4GoR9TE.png&quot; alt=&quot;圖 7-2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;透過實驗觀察發現 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 學會兩種不同的方法進行凸波延遲偵測
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 可以在每個時間點都增加記憶單元內部狀態 $s^{\blk{1}}$ 一點點，而預測值可以靠累加結果轉換而得（見&lt;a href=&quot;#paper-fig-6&quot;&gt;圖 6&lt;/a&gt; 左半）&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 可以學會模擬振盪器，並根據振盪的次數進行預測（見&lt;a href=&quot;#paper-fig-6&quot;&gt;圖 6&lt;/a&gt; 右半）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;從&lt;a href=&quot;#paper-fig-6&quot;&gt;圖 6&lt;/a&gt; 的下半可以發現&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 的輸出閘門維持在 $1$ 的狀態
    &lt;ul&gt;
      &lt;li&gt;作者認為由於預測行為很少發生，因此維持輸出並不會影響表現&lt;/li&gt;
      &lt;li&gt;但當任務需要預測的頻率變高時，模型就必須只在適當的時間點開啟輸出閘門，而該行為在沒有 peephole connections 的狀況下無法達成（原始 LSTM 架構的輸出閘門只會獲得 $t - 1$ 時間點的計算狀態，並沒有 $t$ 時間點的記憶單元內部狀態）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;從&lt;a href=&quot;#paper-fig-7&quot;&gt;圖 7&lt;/a&gt; 的下半可以發現加上 peephole connections 的 LSTM 會在大多數時間關閉輸出閘門
    &lt;ul&gt;
      &lt;li&gt;由於新加入的機制比&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 更複雜，因此需要更多的時間才會收斂（見&lt;a href=&quot;#paper-fig-3&quot;&gt;圖 3&lt;/a&gt; 與&lt;a href=&quot;#paper-fig-5&quot;&gt;圖 5&lt;/a&gt;）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-23&quot;&gt;實驗 2：凸波生成&lt;/h2&gt;

&lt;h3 id=&quot;section-24&quot;&gt;任務定義&lt;/h3&gt;

&lt;p&gt;將凸波延遲偵測任務的輸入與輸出互換，稱為凸波生成（Generating Timed Spikes，GTS）&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;論文沒說明確的輸入輸出結構，但我的猜測如下
    &lt;ul&gt;
      &lt;li&gt;輸入是 $T(n) \in \N$ 時，接下來的模型輸入會是 $T(n) - 1$ 個 $0$&lt;/li&gt;
      &lt;li&gt;輸出是 $T(n) - 1$ 個 $0$，尾巴跟著一個 $1$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;由於 LSTM 在不直接觀察記憶單元內部狀態的情況下無法完成 GTS（絕大多數的輸入都是 $0$），因此只顯示 peephole connections 的實驗&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-25&quot;&gt;實驗結果&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-8&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 8：凸波生成實驗結果。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/arXExQj.png&quot; alt=&quot;圖 8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-9&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 9：凸波生成實驗結果。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/HqjloKX.png&quot; alt=&quot;圖 9-1&quot; /&gt;
&lt;img src=&quot;https://i.imgur.com/jMgR93f.png&quot; alt=&quot;圖 9-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-10&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 10：凸波生成實驗分析。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/qruAa3O.png&quot; alt=&quot;圖 10-1&quot; /&gt;
&lt;img src=&quot;https://i.imgur.com/ZnmkEO0.png&quot; alt=&quot;圖 10-2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;根據&lt;a href=&quot;#paper-fig-8&quot;&gt;圖 8&lt;/a&gt; 我們可以發現週期愈長收斂時間愈久，與&lt;a href=&quot;#paper-fig-3&quot;&gt;圖 3&lt;/a&gt; 觀察結果相同
    &lt;ul&gt;
      &lt;li&gt;LSTM + peephole connections 可以解決圖波生成任務&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;根據&lt;a href=&quot;#paper-fig-9&quot;&gt;圖 9&lt;/a&gt; 下半我們可以發現輸出閘門只在需要生成凸波時開啟，生成完畢後馬上關閉
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#paper-fig-9&quot;&gt;圖 9&lt;/a&gt; 左下顯示生成凸波的當下由於遺忘閘門與輸入閘門一起關閉，因此記憶單元內部狀態直接重設為 $0$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;作者嘗試在訓練時將遺忘閘門移除，發現模型無法收斂，證實遺忘閘門的必須性&lt;/li&gt;
  &lt;li&gt;根據&lt;a href=&quot;#paper-fig-10&quot;&gt;圖 10&lt;/a&gt; 可以觀察到模型生成凸波的時間點跟記憶單元內部狀態的增減時間相同&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-26&quot;&gt;實驗 3：模擬週期函數&lt;/h2&gt;

&lt;h3 id=&quot;section-27&quot;&gt;任務定義&lt;/h3&gt;

&lt;p&gt;讓 LSTM 模型模擬週期函數（Periodic Function Generation，PFG），注意訓練過程不需要給模型輸入，只要有輸出能夠模擬誤差即可，在此任務中就不得不使用 peephole connection（因為沒有輸入）。&lt;/p&gt;

&lt;p&gt;令抽樣頻率為 $F$，模擬的週期函數共有三種，分別是三角函數波 $f_{\cos}$、三角波 $f_{\optri}$ 與方波 $f_{\oprect}$&lt;/p&gt;

\[\begin{align*}
f_{\cos}(t) &amp;amp; = \frac{1}{2} \pa{1 - \cos\pa{\frac{2\pi t}{F}}} \\
f_{\optri}(t) &amp;amp; = \begin{dcases}
\frac{2 (t \opmod F)}{F} &amp;amp; \text{if } (t \opmod F) &amp;gt; \frac{F}{2} \\
2 - \frac{2 (t \opmod F)}{F} &amp;amp; \text{otherwise}
\end{dcases} \\
f_{\oprect}(t) &amp;amp; = \begin{dcases}
1 &amp;amp; \text{if } (t \opmod F) &amp;gt; \frac{F}{2} \\
0 &amp;amp; \text{otherwise}
\end{dcases}
\end{align*}\]

&lt;p&gt;模擬週期函數的難度與函數本身的波型（shape）和週期有關，而波型本身可以用一次微分和二次微分進行描述，論文採用一二次微分函數的最大絕對值 $\max_t \abs{f’(t)}$ 與 $\max_t \abs{f’{}’(t)}$ 作為特徵代表。&lt;/p&gt;

&lt;p&gt;由於離散的時間點無法微分，作者將不可微分的函數用以下公式模擬微分&lt;/p&gt;

\[\begin{align*}
f'(t) &amp;amp; \coloneqq f(t + 1) - f(t) \\
f'{}'(t) &amp;amp; \coloneqq f'(t + 1) - f'(t) \\
&amp;amp; \coloneqq f(t + 2) - 2 f(t + 1) + f(t)
\end{align*}\]

&lt;p&gt;因此當 $t^{\star} = \frac{F}{4}$ 時我們可以得到&lt;/p&gt;

\[\begin{align*}
\max_t \abs{f_{\cos}'(t)} &amp;amp; = \max_t \abs{\frac{1}{2} \sin\pa{\frac{2 \pi t}{F}} \frac{2 \pi}{F}} \\
&amp;amp; = \max_t \abs{\frac{\pi}{F} \sin\pa{\frac{2 \pi t}{F}}} \\
&amp;amp; = \abs{\frac{\pi}{F} \sin\pa{\frac{2 \pi t^{\star}}{F}}} \\
&amp;amp; = \frac{\pi}{F}
\end{align*}\]

&lt;p&gt;當 $t^{\star} = 0$ 時我們可以得到&lt;/p&gt;

\[\begin{align*}
\max_t \abs{f'{}_{\cos}'(t)} &amp;amp; = \max_t \abs{\frac{\pi}{F} \cos\pa{\frac{2 \pi t}{F}} \frac{2 \pi}{F}} \\
&amp;amp; = \abs{\frac{\pi}{F} \cos\pa{\frac{2 \pi t^{\star}}{F}} \frac{2 \pi}{F}} \\
&amp;amp; = \frac{2 \pi^2}{F^2}
\end{align*}\]

&lt;p&gt;當 $((t^{\star} + 1) \opmod F) &amp;lt; \frac{F}{2}$ 時我們可以得到&lt;/p&gt;

\[\begin{align*}
\max_t \abs{f_{\optri}'(t)} &amp;amp; = \abs{f_{\optri}(t^{\star} + 1) - f_{\optri}(t^{\star})} \\
&amp;amp; = \abs{2 - \frac{2 ((t^{\star} + 1) \opmod F)}{F} - 2 + \frac{2 (t^{\star} \opmod F)}{F}} \\
&amp;amp; = \abs{-\frac{2 (t^{\star} + 1)}{F} + \frac{2t^{\star}}{F}} \\
&amp;amp; = \frac{2}{F}
\end{align*}\]

&lt;p&gt;當 $((t^{\star} + 1) \opmod F) = \frac{F}{2}$ 時我們可以得到&lt;/p&gt;

\[\begin{align*}
\max_t \abs{f'{}_{\optri}'(t^{\star})} &amp;amp; = \abs{f_{\optri}(t^{\star} + 2) - 2f_{\optri}(t^{\star} + 1) + f_{\optri}(t^{\star})} \\
&amp;amp; = \abs{\frac{2 ((t^{\star} + 2) \opmod F)}{F} - 4 + \frac{4 ((t^{\star} + 1) \opmod F)}{F} + 2 - \frac{2 (t^{\star} \opmod F)}{F}} \\
&amp;amp; = \abs{\frac{2(t^{\star} + 2)}{F} - 4 + \frac{4F}{2F} + 2 - \frac{2t^{\star}}{F}} \\
&amp;amp; = \frac{4}{F}
\end{align*}\]

&lt;p&gt;當 $(t^{\star} \opmod F) = \frac{F}{2}$ 時我們可以得到&lt;/p&gt;

\[\begin{align*}
\max_t \abs{f_{\oprect}'(t)} &amp;amp; = \abs{f_{\oprect}(t^{\star} + 1) - f_{\oprect}(t^{\star})} \\
&amp;amp; = \abs{1 - 0 + 0} \\
&amp;amp; = 1
\end{align*}\]

&lt;p&gt;當 $((t^{\star} + 1) \opmod F) = \frac{F}{2}$ 時我們可以得到&lt;/p&gt;

\[\begin{align*}
\max_t \abs{f'{}_{\oprect}'(t)} &amp;amp; = \abs{f_{\oprect}(t^{\star} + 2) - 2f_{\oprect}(t^{\star} + 1) + f_{\oprect}(t^{\star})} \\
&amp;amp; = \abs{1 - 0} \\
&amp;amp; = 1
\end{align*}\]

&lt;p&gt;一般來說 $\max_t \abs{f’(t)}$ 與 $\max_t \abs{f’{}’(t)}$ 愈大代表波型變化愈大，因此愈難模擬。&lt;/p&gt;

&lt;p&gt;而 $F$ 愈大代表同一個週期內的波型變化較多，因此 $F$ 愈大愈難模擬，此實驗的 $F \in \set{10, 25}$。&lt;/p&gt;

&lt;h3 id=&quot;section-28&quot;&gt;實驗結果&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-11&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 11：模擬週期函數實驗結果。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/zIALWJF.png&quot; alt=&quot;圖 11&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-12&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 12：模擬週期函數實驗結果。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/v8wFmJ2.png&quot; alt=&quot;圖 12-1&quot; /&gt;
&lt;img src=&quot;https://i.imgur.com/ctHi291.png&quot; alt=&quot;圖 12-2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 只能模擬 $F = 10$ 的 $f_{\cos}$，且收斂時間長（見&lt;a href=&quot;#paper-fig-11&quot;&gt;圖 11&lt;/a&gt;）&lt;/li&gt;
  &lt;li&gt;不使用遺忘閘門的&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 無法模擬超過兩個以上的週期&lt;/li&gt;
  &lt;li&gt;將評估標準提生成誤差低於 $0.15$ 時，模型要花更長的時間收斂
    &lt;ul&gt;
      &lt;li&gt;模擬的週期函數為 $f_{\cos}$，$F = 25$&lt;/li&gt;
      &lt;li&gt;RMSE 的表現從 $0.17 \pm 0.019$ （見&lt;a href=&quot;#paper-fig-11&quot;&gt;圖 11&lt;/a&gt;） 降至 $0.086 \pm 0.002$&lt;/li&gt;
      &lt;li&gt;產生完美表現（$100\%$ 預測正確）的時間點為 $(2704 \pm 49) \cdot 10^3$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-29&quot;&gt;分析&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-13&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 13：模擬週期函數實驗分析。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/SJ43cWb.png&quot; alt=&quot;圖 13-1&quot; /&gt;
&lt;img src=&quot;https://i.imgur.com/pRKxTpM.png&quot; alt=&quot;圖 13-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-14&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 14：模擬週期函數實驗分析。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/gW5bmcu.png&quot; alt=&quot;圖 14-1&quot; /&gt;
&lt;img src=&quot;https://i.imgur.com/7TIXIqV.png&quot; alt=&quot;圖 14-2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-15&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 15：模擬週期函數實驗分析。
圖片來源：&lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/biv8smX.png&quot; alt=&quot;圖 15&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;由於模型沒有收到任何輸入，完全只能依賴記憶單元內部狀態進行模擬，因此記憶單元內部狀態的數值變化應該要與模擬目標擁有類似的曲線（見&lt;a href=&quot;#paper-fig-13&quot;&gt;圖 13&lt;/a&gt;）&lt;/li&gt;
  &lt;li&gt;作者認為此任務可以在不使用 peephole connections 的狀態下完成任務，但流經閘門的梯度被手動丟棄，因此 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 的架構很難最佳化，導致實驗表現不佳（見&lt;a href=&quot;#paper-fig-11&quot;&gt;圖 11&lt;/a&gt;）
    &lt;ul&gt;
      &lt;li&gt;LSTM + peephole connections 收斂速度比 &lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;LSTM-2000&lt;/a&gt; 快，見&lt;a href=&quot;#paper-fig-11&quot;&gt;圖 11&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;觀察 peephole connections 的參數數值，作者發現數值與記憶單元輸出連接到閘門的參數數量級相同，說明 peephole connections 真的有被用來協助模擬週期函數&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;從&lt;a href=&quot;#paper-fig-14&quot;&gt;圖 14&lt;/a&gt; 可以觀察到以下現象
    &lt;ul&gt;
      &lt;li&gt;方波值為 $1$ 時
        &lt;ul&gt;
          &lt;li&gt;記憶單元輸出與記憶單元內部狀態的數值相同&lt;/li&gt;
          &lt;li&gt;輸出閘門維持開啟&lt;/li&gt;
          &lt;li&gt;模型內部狀態逐漸遞減（趨向 $0$）&lt;/li&gt;
          &lt;li&gt;由於記憶單元輸出與記憶單元輸入連結的參數數值為負，因此模型有辦法遞減記憶單元內部狀態&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;方波值為 $0$ 時
        &lt;ul&gt;
          &lt;li&gt;記憶單元輸出為 $0$&lt;/li&gt;
          &lt;li&gt;輸出閘門維持關閉&lt;/li&gt;
          &lt;li&gt;模型內部狀態逐漸遞增（趨向 $1$）&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;從&lt;a href=&quot;#paper-fig-15&quot;&gt;圖 15&lt;/a&gt; 可以發現模型的初始計算狀態為 $0$，但開始計算後模型計算狀態再也不為 $0$
    &lt;ul&gt;
      &lt;li&gt;這表示模型&lt;strong&gt;初始計算狀態&lt;/strong&gt;應該也被當成&lt;strong&gt;參數&lt;/strong&gt;一起訓練&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>[&quot;Felix A. Gers&quot;, &quot;Nicol N. Schraudolph&quot;, &quot;Jürgen Schmidhuber&quot;]</name></author><category term="General Sequence Modeling" /><category term="RNN" /><category term="LSTM" /><category term="model architecture" /><category term="neural network" /><summary type="html">目標 在 LSTM 上加入 peephole connections 作者 Felix A. Gers, Nicol N. Schraudolph, Jürgen Schmidhuber 隸屬單位 IDSIA 期刊/會議名稱 JMLR, Volume 3 發表時間 2002 論文連結 https://www.jmlr.org/papers/v3/gers02a.html</summary></entry><entry><title type="html">Finding Structure in Time</title><link href="/text%20modeling/2021/12/21/finding-structure-in-time.html" rel="alternate" type="text/html" title="Finding Structure in Time" /><published>2021-12-21T18:47:00+08:00</published><updated>2021-12-21T18:47:00+08:00</updated><id>/text%20modeling/2021/12/21/finding-structure-in-time</id><content type="html" xml:base="/text%20modeling/2021/12/21/finding-structure-in-time.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;提出 Elman Net&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;Jeffrey L. Elman&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;Cognitive Science&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;1990&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;section&quot;&gt;重點&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;世界上的語言以文字在句中的順序可以區分成兩種
    &lt;ul&gt;
      &lt;li&gt;Fixed word-order：文字的順序是有規則的，例如英文
        &lt;ul&gt;
          &lt;li&gt;需要考慮語法結構（syntactic structure）、謂詞（&lt;a href=&quot;https://en.wikipedia.org/wiki/Predicate_(grammar)&quot;&gt;predicate&lt;/a&gt;）語意限制（&lt;a href=&quot;https://en.wikipedia.org/wiki/Selection_(linguistics)&quot;&gt;selective restrictions&lt;/a&gt;）、次範疇化（&lt;a href=&quot;https://en.wikipedia.org/wiki/Subcategorization&quot;&gt;subcategorization&lt;/a&gt;）、論元（&lt;a href=&quot;https://en.wikipedia.org/wiki/Valency_(linguistics)&quot;&gt;valency&lt;/a&gt;）、對話（discourse）等&lt;/li&gt;
          &lt;li&gt;Chomsky 認為文字順序非線性（linear order），雖然人類只能觀測到線性順序，但大腦會自動轉換成非線性順序進行理解&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Free word-order：文字的順序替換規則較為自由，但不完全隨機&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;在此論文之前的語言學研究認為所有語句都可以轉換成語法樹（syntactic tree），但不特別探討時間順序問題
    &lt;ul&gt;
      &lt;li&gt;作者認為在 parsing 任務仍然表現很差的情況下，此一假設不太適當&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;在此論文之前的研究中，大多數的研究嘗試以 RNN 模型解決時間序列問題通常是隨著時間增加輸入維度&lt;/li&gt;
  &lt;li&gt;作者提出 Elman Net，使用隱藏層的計算結果進行回饋，就是現在常見的 RNN 架構
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html&quot;&gt;PyTorch 的 RNN&lt;/a&gt; 模型就是基於 Elman Net 進行實作&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;作者評估語言模型（language model）的方法不是困惑度（perplexity），而是期望值最大化（expectation maximization）
    &lt;ul&gt;
      &lt;li&gt;也許現在（2021）年的 pre-train model 該考慮一下期望值最大化的評估手段&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;根據實驗結果，作者認為以 &lt;strong&gt;distributed representation&lt;/strong&gt; 表達&lt;strong&gt;語言&lt;/strong&gt;能夠同時學會&lt;strong&gt;字符&lt;/strong&gt;（&lt;strong&gt;token&lt;/strong&gt;）與&lt;strong&gt;種類&lt;/strong&gt;（&lt;strong&gt;type&lt;/strong&gt;）的知識
    &lt;ul&gt;
      &lt;li&gt;傳統 symbolic systems 需要手動定義字符與種類的差異&lt;/li&gt;
      &lt;li&gt;Distributed representation 比 symbolic systems 還要強大&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;以空間代表時間產生的問題&lt;/h2&gt;

&lt;p&gt;若時間序列 $x = x_1, \dots, x_T$，$T$ 為該序列的長度，則常見的時間序列處理方法為將每個時間的 $x_t$ 的輸入以一個維度作為代表，因此輸入維度就會與序列長度 $T$ 相同。&lt;/p&gt;

&lt;p&gt;以上的架構可以想成空間資訊描述時間資訊，但此方法有不少問題&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型一定要在取得所有輸入後才能計算，除了跟人類的行為不太一樣以外，模型&lt;strong&gt;沒有辦法自己知道什麼時候已經取得所有輸入&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;輸入維度固定&lt;/strong&gt;代表訓練的過程必須要選擇&lt;strong&gt;最長&lt;/strong&gt;的輸入序列作為模型輸入維度，除了&lt;strong&gt;浪費計算資源&lt;/strong&gt;以外，在測試時&lt;strong&gt;過長的輸入就無法被處理&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;輸入架構同時隱含&lt;strong&gt;距離關係&lt;/strong&gt;，因此無法辨別&lt;strong&gt;相對時間差&lt;/strong&gt;的觀念
    &lt;ul&gt;
      &lt;li&gt;當兩個輸入序列只有&lt;strong&gt;時間差上的差異&lt;/strong&gt;，則直接將完整輸入序列丟進模型會導致模型無法區分相對時間的差異&lt;/li&gt;
      &lt;li&gt;例如：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;011100000&lt;/code&gt; 與 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;000111000&lt;/code&gt; 兩個序列，時間差為 $2$，但以向量直接表達時兩者的歐式距離為 $2$，此距離差異可能代表完全無關的資料&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;elman-net&quot;&gt;Elman Net&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 1：Elman Net 架構。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/0kJih5k.png&quot; alt=&quot;圖 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;提出的架構概念如下&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型共有三層：輸入層 + 隱藏層 + 輸出層&lt;/li&gt;
  &lt;li&gt;序列資料按照時間依序輸入至模型&lt;/li&gt;
  &lt;li&gt;隱藏層會再作為下個時間點的輸入回饋到隱藏層
    &lt;ul&gt;
      &lt;li&gt;回饋的隱藏層額外稱為 &lt;strong&gt;Context Units&lt;/strong&gt;，由於所有的隱藏單元都會回饋，因此 Context Units 與隱藏單元的個數一樣多&lt;/li&gt;
      &lt;li&gt;回饋的方法是全連接&lt;/li&gt;
      &lt;li&gt;Context Units 初始值設定為 $0.5$，理由是作者採用的啟發函數（activation function）數值範圍落在 $[0, 1]$ 之間（論文沒寫但應該是 sigmoid）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;最佳化的方法就是 BPTT&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;xor&quot;&gt;實驗 1：序列版 XOR&lt;/h2&gt;

&lt;h3 id=&quot;section-2&quot;&gt;任務定義&lt;/h3&gt;

&lt;p&gt;傳統的兩層神經網路（輸入層 + 輸出層）是無法解決 XOR 問題，一定要額外使用隱藏層才有辦法解決。&lt;/p&gt;

&lt;p&gt;輸入只會是由 2 bits 組成的序列 $\set{00, 01, 10, 11}$，當輸入為 $\set{00, 11}$ 時輸出為 $0$，輸入為 $\set{01, 10}$ 時輸出為 $1$。&lt;/p&gt;

&lt;p&gt;序列版的 XOR 任務就是將 $N$ 組 XOR 的輸入輸出串接在一起（三個 bits 為一組），總共長度為 $3N$ bits，目標為在輸入一個 bit 之後預測下個 bit。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;神經網路每個時間點收到的訊號就只有 $1$ 個 bit&lt;/li&gt;
  &lt;li&gt;實驗所採用的 $N = 1000$，即輸入序列由 $3000$ bits 所組成&lt;/li&gt;
  &lt;li&gt;最佳化目標為最小平方差（MSE）&lt;/li&gt;
  &lt;li&gt;以 $N = 3$ 為例，XOR 序列可以是 $110011101$
    &lt;ul&gt;
      &lt;li&gt;第一跟第二個 bits 為 $11$，因此第三個 bit 為 $0$；第四跟第五個 bits 為 $01$，因此第六個 bit 為 $1$；第七跟第八個 bits 為 $10$，因此第三個 bit 為 $1$。&lt;/li&gt;
      &lt;li&gt;第一個 bit 無法預測第二個 bit（是 $0$ 或 $1$ 的機率為 $50\%$），但第二個 bit 必須要透過第一個 bit 的資訊預測第三個 bit&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;elman-net-&quot;&gt;Elman Net 架構&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;數值（或範圍）&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;輸入層維度&lt;/td&gt;
      &lt;td&gt;$1$&lt;/td&gt;
      &lt;td&gt;一次只有 $1$ 個 bit 輸入&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隱藏層維度&lt;/td&gt;
      &lt;td&gt;$2$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;輸出層維度&lt;/td&gt;
      &lt;td&gt;$1$&lt;/td&gt;
      &lt;td&gt;一次只有 $1$ 個 bit 輸出&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-3&quot;&gt;實驗結果&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 2：序列版 XOR 實驗分析。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Fs6mGLJ.png&quot; alt=&quot;圖 2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型在無法預測時誤差較高，可以預測時誤差較低，見&lt;a href=&quot;#paper-fig-2&quot;&gt;圖 2&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;圖中的實驗結果只分析前 $12$ 個 bits，以 $1200$ 次實驗結果平均&lt;/li&gt;
      &lt;li&gt;至少要訓練 $600$ 次才可以達成上述結果&lt;/li&gt;
      &lt;li&gt;論文沒有著明最佳化所採用的學習率（learning rate）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;兩個隱藏單元在看到不同輸入 pattern 時維持開啟（接近 $1$）
    &lt;ul&gt;
      &lt;li&gt;當輸入都是由 $\set{000, 110}$ 組成時，其中一個隱藏單元維持開啟&lt;/li&gt;
      &lt;li&gt;當輸入都是由 $\set{011, 101}$ 組成時，另外一個隱藏單元維持開啟&lt;/li&gt;
      &lt;li&gt;與單純使用全連接層解決 XOR 的作法觀察到的隱藏單元現象不同，說明架構不同時對於類似的任務可以有不同的解法&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;實驗 2：字母序列&lt;/h2&gt;

&lt;h3 id=&quot;section-5&quot;&gt;任務定義&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-table-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;表 1：字母向量表達法與其意義。
表格來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;字母&lt;/th&gt;
      &lt;th&gt;consonant&lt;/th&gt;
      &lt;th&gt;vowel&lt;/th&gt;
      &lt;th&gt;interrupted&lt;/th&gt;
      &lt;th&gt;high&lt;/th&gt;
      &lt;th&gt;back&lt;/th&gt;
      &lt;th&gt;voiced&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;u&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;一個序列由 $6$ 個不同的字母組成，每個字母由 $6$ 個 bits 作為代表，細節請見&lt;a href=&quot;#paper-table-1&quot;&gt;表 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;首先由子音 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bdg&lt;/code&gt; 生成總長度為 $1000$ 的隨機序列，接著將子音依照以下規則進行替換，產生的最終序列作為模型輸入
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt; 換成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ba&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d&lt;/code&gt; 換成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dii&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g&lt;/code&gt; 換成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;guuu&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;每個時間點輸入一個字母（6 bits），預測下一個時間點的字母
    &lt;ul&gt;
      &lt;li&gt;子音無法預測&lt;/li&gt;
      &lt;li&gt;當子音出現時母音可以預測&lt;/li&gt;
      &lt;li&gt;最後一個時間點的輸入預測目標是第一個時間點的輸入&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;最佳化目標為最小平方差（MSE）&lt;/li&gt;
  &lt;li&gt;在同一個輸入序列（長度 $&amp;gt; 1000$）上總共訓練 $200$ 次，測試時使用不同的序列（產生方法相同）進行測試&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;elman-net--1&quot;&gt;Elman Net 架構&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;數值（或範圍）&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;輸入層維度&lt;/td&gt;
      &lt;td&gt;$6$&lt;/td&gt;
      &lt;td&gt;一個字母有 $6$ 個 bits&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隱藏層維度&lt;/td&gt;
      &lt;td&gt;$20$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;輸出層維度&lt;/td&gt;
      &lt;td&gt;$6$&lt;/td&gt;
      &lt;td&gt;一個字母有 $6$ 個 bits&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-6&quot;&gt;實驗結果&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 3：字母序列輸出 $6$ 個 bits 的平均誤差實驗結果。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/yLHWxfr.png&quot; alt=&quot;圖 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 4：字母序列輸出第 $1$ 個 bit 的誤差實驗結果。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/BvnBbEV.png&quot; alt=&quot;圖 4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 5：字母序列輸出第 $4$ 個 bit 的誤差實驗結果。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/U1U1joV.png&quot; alt=&quot;圖 5&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型的平均誤差在無法預測時較高，可以預測時較低，見&lt;a href=&quot;#paper-fig-3&quot;&gt;圖 3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;第 $1$ 個 bit 的規則是完全可以預測的
    &lt;ul&gt;
      &lt;li&gt;模型必須根據子音的類別（與第 $1, 4, 5$ 個 bits 有關）預測母音的個數&lt;/li&gt;
      &lt;li&gt;第 $1$ 個 bit 的預測誤差較低（見&lt;a href=&quot;#paper-fig-4&quot;&gt;圖 4&lt;/a&gt;）說明模型能夠達成任務&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;第 $4$ 個 bit 只能預測母音，無法預測子音
    &lt;ul&gt;
      &lt;li&gt;模型必須根據子音的類別（與第 $1, 4, 5$ 個 bits 有關）預測母音的個數&lt;/li&gt;
      &lt;li&gt;由於子音出現無規則，因此第 $4$ 個 bit 的預測誤差稍微高一點，見&lt;a href=&quot;#paper-fig-5&quot;&gt;圖 5&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-7&quot;&gt;實驗 3：字母層級語言模型&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-table-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;表 2：字母向量表達法，每個字母以 $5$ bits 編碼。
表格來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;字母&lt;/th&gt;
      &lt;th&gt;bit&lt;/th&gt;
      &lt;th&gt;字母&lt;/th&gt;
      &lt;th&gt;bit&lt;/th&gt;
      &lt;th&gt;字母&lt;/th&gt;
      &lt;th&gt;bit&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;00001&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;00010&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;00011&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;00100&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;e&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;00101&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;f&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;00110&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;g&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;00111&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;h&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01000&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01001&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;j&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01010&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01011&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01100&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;m&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01101&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01110&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;o&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01111&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10000&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;q&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10001&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10010&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10011&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;t&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10100&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;u&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10101&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10110&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;w&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10111&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;11000&lt;/code&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;11001&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;11010&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;使用簡單的單字（word）層級語言模型產生文字
    &lt;ul&gt;
      &lt;li&gt;可以產生的單字共有 $15$ 種&lt;/li&gt;
      &lt;li&gt;總共產生 $200$ 個句子，長度不一&lt;/li&gt;
      &lt;li&gt;每個句子包含最少 $4$ 個單字，最多 $9$ 個單字&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;所有產生的句子串接在一起，產生總長為 $1270$ 個單字的序列，共由 $4963$ 個字母（letter）組成
    &lt;ul&gt;
      &lt;li&gt;由於英文只有 $26$ 個字母，因此每個字母以 $5$ bits 表達&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型的任務為根據已經接收到的字母預測下個時間點的字母，即字母層級語言模型（letter-level language model）&lt;/li&gt;
  &lt;li&gt;總共訓練 $10$ 次，最佳化目標為最小平方差（MSE）&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;elman-net--2&quot;&gt;Elman Net 架構&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;數值（或範圍）&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;輸入層維度&lt;/td&gt;
      &lt;td&gt;$5$&lt;/td&gt;
      &lt;td&gt;一個字母有 $5$ 個 bits&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隱藏層維度&lt;/td&gt;
      &lt;td&gt;$20$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;輸出層維度&lt;/td&gt;
      &lt;td&gt;$5$&lt;/td&gt;
      &lt;td&gt;一個字母有 $5$ 個 bits&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-8&quot;&gt;實驗結果&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-6&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 6：字母層級語言模型預測誤差，圖中只顯示一部份字母語言序列（many years ago a boy and girl lived by the sea they played happily m）。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/nVD0jMO.png&quot; alt=&quot;圖 6&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;模型誤差在出現新的單字時較高，在預測單字字母時較低，見&lt;a href=&quot;#paper-fig-6&quot;&gt;圖 6&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;大多數時可以依靠誤差進行斷字（當誤差相對上升時就可以斷字），但仍有部份單字不是用此規則
    &lt;ul&gt;
      &lt;li&gt;例如 they 的 y 誤差上升&lt;/li&gt;
      &lt;li&gt;在部份實驗（不包含在&lt;a href=&quot;#paper-fig-6&quot;&gt;圖 6&lt;/a&gt;）中發現常見的單字序列有可能被當成可以連續預測的片段，導致誤差持續下降而無法斷字，作者認為此概念就像小孩在學習俗諺（idioms）一樣把俗諺當成單字使用&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;作者認為不能只使用此模型進行斷字，必須同時考慮前後文，就如同語音辨識一樣
    &lt;ul&gt;
      &lt;li&gt;Elman Net 只是展示簡單的 RNN 能夠學到部份斷字的知識&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-9&quot;&gt;實驗 4：單字層級語言模型&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-7&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 7：單字層級語言模型所使用的單字種類，共有 $13$ 種，只考慮名詞與動詞。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Dv9knWh.png&quot; alt=&quot;圖 7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-8&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 8：字母層級語言模型訓練資料生成模版。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/dko5W1S.png&quot; alt=&quot;圖 8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-9&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 9：部份訓練資料範例。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ByfVIn0.png&quot; alt=&quot;圖 9&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用&lt;a href=&quot;#paper-fig-7&quot;&gt;圖 7&lt;/a&gt; 的單字與&lt;a href=&quot;#paper-fig-8&quot;&gt;圖 8&lt;/a&gt; 的模版生成訓練資料
    &lt;ul&gt;
      &lt;li&gt;每筆訓練資料只會包含 $2 \sim 3$ 個字&lt;/li&gt;
      &lt;li&gt;共有 $13$ 個不同類別的名詞與動詞，總共有 $29$ 個不同的單字&lt;/li&gt;
      &lt;li&gt;部份動詞會跨類別，例如 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;break&lt;/code&gt; 屬於破壞動詞 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VERB-DESTORY&lt;/code&gt; 但也同時屬於及物動詞 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VERB-TRAN&lt;/code&gt;（transitive）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;總共產生 $10000$ 筆訓練資料，每筆資料必須符合&lt;a href=&quot;#paper-fig-8&quot;&gt;圖 8&lt;/a&gt; 的模版（selective restrictions），最後所有資料串接在一起形成長度為 $27354$ 的文字序列
    &lt;ul&gt;
      &lt;li&gt;每筆資料中的字由 $31$ bits 的 one-hot vector 代表&lt;/li&gt;
      &lt;li&gt;$31$ bits 的理由是後續的實驗會繼續使用相同的架構&lt;/li&gt;
      &lt;li&gt;總 bits 數為 $27354 \times 31 = 853554$&lt;/li&gt;
      &lt;li&gt;部份訓練資料範例請見&lt;a href=&quot;#paper-fig-9&quot;&gt;圖 9&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;模型的任務為根據已經接收到的單字預測下個時間點的單字，即單字層級語言模型（word-level language model）
    &lt;ul&gt;
      &lt;li&gt;句子之間無明顯分割，模型無法預測下個句子的開頭&lt;/li&gt;
      &lt;li&gt;在句子中由於接續的動詞與名詞都有多個選項，模型也無法準確預測&lt;/li&gt;
      &lt;li&gt;但接續的單字仍然有固定的規則，模型必須學會該規則在輸入序列中的&lt;strong&gt;期望值&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;因此訓練目標是 one-hot vector 的最小平方差（MSE），預測目標是期望值的 MSE&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;期望值的計算方法如下
    &lt;ul&gt;
      &lt;li&gt;句子開頭的期望值為：句子開頭的出現次數除以 $10000$（總共有 $10000$ 個句子）&lt;/li&gt;
      &lt;li&gt;中間動詞的期望值為：中間動詞出現次數除以相同開頭名詞的出現次數&lt;/li&gt;
      &lt;li&gt;結尾名詞的期望值為：結尾名詞出現次數除以相同開頭名詞 + 動詞的出現次數&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;總共訓練 $6$ 次
    &lt;ul&gt;
      &lt;li&gt;每次訓練的結尾會接續訓練的開始&lt;/li&gt;
      &lt;li&gt;評估方法為方均根差（Root Mean Square Error，RMSE）&lt;/li&gt;
      &lt;li&gt;RMSE 計算對象可以是 one-hot vector 或期望值&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;elman-net--3&quot;&gt;Elman Net 架構&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;數值（或範圍）&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;輸入層維度&lt;/td&gt;
      &lt;td&gt;$31$&lt;/td&gt;
      &lt;td&gt;一個單字有 $31$ 個 bits&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;隱藏層維度&lt;/td&gt;
      &lt;td&gt;$150$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;輸出層維度&lt;/td&gt;
      &lt;td&gt;$31$&lt;/td&gt;
      &lt;td&gt;一個單字有 $31$ 個 bits&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-10&quot;&gt;實驗結果&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;訓練後模型輸出與真實答案（one-hot vector）之間的 RMSE 為 $0.88$
    &lt;ul&gt;
      &lt;li&gt;作者發現由於輸出數字絕大部份為 $0$（sparse），因此模型很快就學會將輸出降為 $0$&lt;/li&gt;
      &lt;li&gt;模型的起始誤差為 $15.5$，最終誤差接近 $1$，在所有輸出都接近 $0$ 的狀況下作者認為 RMSE 為 $0.88$ 並不是什麼了不起的事&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;訓練後模型輸出與期望值之間的 RMSE 為 $0.053$，標準差為 $0.1$&lt;/li&gt;
  &lt;li&gt;由於模型的輸出加總並不為 $1$（不像現代（2021）都有使用 softmax），因此使用 cosine similarity 評估
    &lt;ul&gt;
      &lt;li&gt;與期望值之間的 cosine similarity 為 $1$ 時仍然有可能 RMSE 不為 $0$&lt;/li&gt;
      &lt;li&gt;與期望值之間的平均 cosine similarity 為 $0.916$，標準差為 $0.123$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;不論是 RMSE 或 cosine similarity，模型表現都不錯
    &lt;ul&gt;
      &lt;li&gt;由於輸入彼此正交，模型只能透過共同出現統計次數（co-occurrence statistics）進行學習&lt;/li&gt;
      &lt;li&gt;模型有可能透過 co-occurrence 學會單字種類這種泛化能力（generalization），因此作者接下來對單字種類進行分析&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-11&quot;&gt;分析 4-1：隱藏單元的群集&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-10&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 10：群集分析結果。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/7WnWsXM.png&quot; alt=&quot;圖 10&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;將每個時間點產生所得的隱藏單元儲存起來
    &lt;ul&gt;
      &lt;li&gt;總共儲存 $27354 \times 150$ bits&lt;/li&gt;
      &lt;li&gt;注意是隱藏單元不是輸出單元&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;將&lt;strong&gt;相同單字&lt;/strong&gt;產生的隱藏單元加起來取平均，並將結果進行群集（clustering）分析
    &lt;ul&gt;
      &lt;li&gt;作者沒說群集分析採用什麼演算法與距離函數（metric）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;主要的群集以詞性區分，見&lt;a href=&quot;#paper-fig-10&quot;&gt;圖 10&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;動詞的主要群集以及物動詞（transitive）、不及物動詞（intransitive）或兩者皆可進行區分&lt;/li&gt;
      &lt;li&gt;名詞的主要群集以可動物（animates）與不可動物（inanimate）進行區分
        &lt;ul&gt;
          &lt;li&gt;可動物的主要群集以人類（human）與非人類（nonhuman）進行區分&lt;/li&gt;
          &lt;li&gt;不可動物的主要群集以可破壞（breakable）、可實用（edibles）與非人類主詞（subjects of agentless ative verbs）進行區分&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;根據群集觀察結果可以推論模型學會以類別進行區分的泛化能力
    &lt;ul&gt;
      &lt;li&gt;即使模型無法準確預測單字（任務本身的特性），但仍能學會泛化能力&lt;/li&gt;
      &lt;li&gt;由於輸入並沒有給予類別資訊，並非如人類學習語言的過程，作者認為實驗結果非常有趣&lt;/li&gt;
      &lt;li&gt;作者強調群集結果不是絕對正確，因為部份群集可以同時在距離空間（metric space）上被分成不同群集，但同時共享相同特性&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-12&quot;&gt;分析 4-2：模型泛化能力&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-11&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 11：模型泛化能力測試結果。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/Fs5W81y.png&quot; alt=&quot;圖 11&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;加入完全無意義的新字 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zog&lt;/code&gt;，並以第 $30$ 個 bit 為 $1$ 的 one-hot vector 進行表達
    &lt;ul&gt;
      &lt;li&gt;將所有 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;man&lt;/code&gt; 出現的位置都替換成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zog&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;新的序列（$27354 \times 31$ 個 bits）直接輸入給模型，不做任何的訓練，並紀錄隱藏單元後進行與分析 4-1 相同的群集分析&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;根據實驗結果發現 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zog&lt;/code&gt; 與 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;man&lt;/code&gt; 的表現接近，說明模型擁有泛化&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-13&quot;&gt;分析 4-3：上下文感知能力&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-12&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 12：模型泛化能力測試結果。
圖片來源：&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/RNwrZKv.png&quot; alt=&quot;圖 12-1&quot; /&gt;
&lt;img src=&quot;https://i.imgur.com/RIV0mL3.png&quot; alt=&quot;圖 12-2&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;由於分析 4-1 的結果是以隱藏層&lt;strong&gt;平均&lt;/strong&gt;向量進行分析，而平均運算將前文的概念去除，因此無法判斷模型是否擁有上下文感知（context sensitive）能力&lt;/li&gt;
  &lt;li&gt;使用平均進行實驗在計算上是比較實際的，但真的要分析前後文內容只能依靠單筆資料的計算結果進行分析
    &lt;ul&gt;
      &lt;li&gt;總共有 $27454$ 個 $150$ bits 的向量（包含 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zog&lt;/code&gt;）&lt;/li&gt;
      &lt;li&gt;由於種類過多無法視覺化，但概念與&lt;a href=&quot;#paper-fig-10&quot;&gt;圖 10&lt;/a&gt; 相同，只是群集同時考量前文&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#paper-fig-10&quot;&gt;圖 10&lt;/a&gt; 證實模型學會了從 $29$ 種單字中找出類別資訊，&lt;a href=&quot;#paper-fig-12&quot;&gt;圖 12&lt;/a&gt; 證實模型能夠區分類似的前後文
    &lt;ul&gt;
      &lt;li&gt;相同的單字在不同的前後文中會被分成不同的群集
        &lt;ul&gt;
          &lt;li&gt;模型能夠區分單字出現在開頭與結尾&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;意義相同的單字會出現類似的群集&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;根據實驗結果，作者認為以 &lt;strong&gt;distributed representation&lt;/strong&gt; 表達&lt;strong&gt;語言&lt;/strong&gt;能夠同時學會&lt;strong&gt;字符&lt;/strong&gt;（&lt;strong&gt;token&lt;/strong&gt;）與&lt;strong&gt;種類&lt;/strong&gt;（&lt;strong&gt;type&lt;/strong&gt;）的知識
    &lt;ul&gt;
      &lt;li&gt;傳統 symbolic systems 需要手動定義字符與種類的差異&lt;/li&gt;
      &lt;li&gt;Distributed representation 比 symbolic systems 還要強大&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>[&quot;Jeffrey L. Elman&quot;]</name></author><category term="Text Modeling" /><category term="RNN" /><category term="Elman Net" /><category term="model architecture" /><category term="neural network" /><summary type="html">目標 提出 Elman Net 作者 Jeffrey L. Elman 期刊/會議名稱 Cognitive Science 發表時間 1990 論文連結 https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1</summary></entry><entry><title type="html">Learning to Forget: Continual Prediction with LSTM</title><link href="/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html" rel="alternate" type="text/html" title="Learning to Forget: Continual Prediction with LSTM" /><published>2021-12-13T14:09:00+08:00</published><updated>2021-12-13T14:09:00+08:00</updated><id>/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm</id><content type="html" xml:base="/general%20sequence%20modeling/2021/12/13/learning-to-forget-continual-prediction-with-lstm.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;提出在 LSTM 上增加 forget gate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;Felix A. Gers, Jürgen Schmidhuber, Fred Cummins&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;Neural Computation&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;2000&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--
  Define LaTeX command which will be used through out the writing.

  Each command must be wrapped with $ signs.
  We use &quot;display: none;&quot; to avoid redudant whitespaces.
 --&gt;

&lt;p style=&quot;display: none;&quot;&gt;

  &lt;!-- Operator in. --&gt;
  $\providecommand{\opnet}{}$
  $\renewcommand{\opnet}{\operatorname{net}}$
  &lt;!-- Operator in. --&gt;
  $\providecommand{\opin}{}$
  $\renewcommand{\opin}{\operatorname{in}}$
  &lt;!-- Operator out. --&gt;
  $\providecommand{\opout}{}$
  $\renewcommand{\opout}{\operatorname{out}}$
  &lt;!-- Operator cell block. --&gt;
  $\providecommand{\opblk}{}$
  $\renewcommand{\opblk}{\operatorname{block}}$
  &lt;!-- Operator cell multiplicative forget gate. --&gt;
  $\providecommand{\opfg}{}$
  $\renewcommand{\opfg}{\operatorname{fg}}$
  &lt;!-- Operator cell multiplicative input gate. --&gt;
  $\providecommand{\opig}{}$
  $\renewcommand{\opig}{\operatorname{ig}}$
  &lt;!-- Operator cell multiplicative output gate. --&gt;
  $\providecommand{\opog}{}$
  $\renewcommand{\opog}{\operatorname{og}}$
  &lt;!-- Operator sequence. --&gt;
  $\providecommand{\opseq}{}$
  $\renewcommand{\opseq}{\operatorname{seq}}$
  &lt;!-- Operator loss. --&gt;
  $\providecommand{\oploss}{}$
  $\renewcommand{\oploss}{\operatorname{loss}}$

  &lt;!-- Net input. --&gt;
  $\providecommand{\net}{}$
  $\renewcommand{\net}[2]{\opnet_{#1}(#2)}$
  &lt;!-- Net input with activatiton f. --&gt;
  $\providecommand{\fnet}{}$
  $\renewcommand{\fnet}[2]{f_{#1}\big(\net{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input. --&gt;
  $\providecommand{\dfnet}{}$
  $\renewcommand{\dfnet}[2]{f_{#1}'\big(\net{#1}{#2}\big)}$

  &lt;!-- Input dimension. --&gt;
  $\providecommand{\din}{}$
  $\renewcommand{\din}{d_{\opin}}$
  &lt;!-- Output dimension. --&gt;
  $\providecommand{\dout}{}$
  $\renewcommand{\dout}{d_{\opout}}$
  &lt;!-- Cell block dimension. --&gt;
  $\providecommand{\dblk}{}$
  $\renewcommand{\dblk}{d_{\opblk}}$

  &lt;!-- Number of cell blocks. --&gt;
  $\providecommand{\nblk}{}$
  $\renewcommand{\nblk}{n_{\opblk}}$

  &lt;!-- Cell block k. --&gt;
  $\providecommand{\blk}{}$
  $\renewcommand{\blk}[1]{\opblk^{#1}}$

  &lt;!-- Weight of multiplicative forget gate. --&gt;
  $\providecommand{\wfg}{}$
  $\renewcommand{\wfg}{w^{\opfg}}$
  &lt;!-- Weight of multiplicative input gate. --&gt;
  $\providecommand{\wig}{}$
  $\renewcommand{\wig}{w^{\opig}}$
  &lt;!-- Weight of multiplicative output gate. --&gt;
  $\providecommand{\wog}{}$
  $\renewcommand{\wog}{w^{\opog}}$
  &lt;!-- Weight of cell units. --&gt;
  $\providecommand{\wblk}{}$
  $\renewcommand{\wblk}[1]{w^{\blk{#1}}}$
  &lt;!-- Weight of output units. --&gt;
  $\providecommand{\wout}{}$
  $\renewcommand{\wout}{w^{\opout}}$

  &lt;!-- Net input of multiplicative forget gate. --&gt;
  $\providecommand{\netfg}{}$
  $\renewcommand{\netfg}[2]{\opnet_{#1}^{\opfg}(#2)}$
  &lt;!-- Net input of multiplicative forget gate with activatiton f. --&gt;
  $\providecommand{\fnetfg}{}$
  $\renewcommand{\fnetfg}[2]{f_{#1}^{\opfg}\big(\netfg{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input of forget gate. --&gt;
  $\providecommand{\dfnetfg}{}$
  $\renewcommand{\dfnetfg}[2]{f_{#1}^{\opfg}{'}\big(\netfg{#1}{#2}\big)}$
  &lt;!-- Net input of multiplicative input gate. --&gt;
  $\providecommand{\netig}{}$
  $\renewcommand{\netig}[2]{\opnet_{#1}^{\opig}(#2)}$
  &lt;!-- Net input of multiplicative input gate with activatiton f. --&gt;
  $\providecommand{\fnetig}{}$
  $\renewcommand{\fnetig}[2]{f_{#1}^{\opig}\big(\netig{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input of input gate. --&gt;
  $\providecommand{\dfnetig}{}$
  $\renewcommand{\dfnetig}[2]{f_{#1}^{\opig}{'}\big(\netig{#1}{#2}\big)}$
  &lt;!-- Net input of multiplicative output gate. --&gt;
  $\providecommand{\netog}{}$
  $\renewcommand{\netog}[2]{\opnet_{#1}^{\opog}(#2)}$
  &lt;!-- Net input of multiplicative output gate with activatiton f. --&gt;
  $\providecommand{\fnetog}{}$
  $\renewcommand{\fnetog}[2]{f_{#1}^{\opog}\big(\netog{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input of output gate. --&gt;
  $\providecommand{\dfnetog}{}$
  $\renewcommand{\dfnetog}[2]{f_{#1}^{\opog}{'}\big(\netog{#1}{#2}\big)}$
  &lt;!-- Net input of output units. --&gt;
  $\providecommand{\netout}{}$
  $\renewcommand{\netout}[2]{\opnet_{#1}^{\opout}(#2)}$
  &lt;!-- Net input of output units with activatiton f. --&gt;
  $\providecommand{\fnetout}{}$
  $\renewcommand{\fnetout}[2]{f_{#1}^{\opout}\big(\netout{#1}{#2}\big)}$
  &lt;!-- Derivative of f with respect to net input of output units. --&gt;
  $\providecommand{\dfnetout}{}$
  $\renewcommand{\dfnetout}[2]{f_{#1}^{\opout}{'}\big(\netout{#1}{#2}\big)}$

  &lt;!-- Net input of cell unit. --&gt;
  $\providecommand{\netblk}{}$
  $\renewcommand{\netblk}[3]{\opnet_{#1}^{\blk{#2}}(#3)}$
  &lt;!-- Net input of cell unit with activatiton g. --&gt;
  $\providecommand{\gnetblk}{}$
  $\renewcommand{\gnetblk}[3]{g_{#1}\big(\netblk{#1}{#2}{#3}\big)}$
  &lt;!-- Derivative of g with respect to net input of cell unit. --&gt;
  $\providecommand{\dgnetblk}{}$
  $\renewcommand{\dgnetblk}[3]{g_{#1}'\big(\netblk{#1}{#2}{#3}\big)}$
  &lt;!-- Cell unit with activatiton h. --&gt;
  $\providecommand{\hblk}{}$
  $\renewcommand{\hblk}[3]{h_{#1}\big(s_{#1}^{\blk{#2}}(#3)\big)}$
  &lt;!-- Derivative of h with respect to cell unit. --&gt;
  $\providecommand{\dhblk}{}$
  $\renewcommand{\dhblk}[3]{h_{#1}'\big(s_{#1}^{\blk{#2}}(#3)\big)}$

  &lt;!-- Gradient approximation by truncating gradient. --&gt;
  $\providecommand{\aptr}{}$
  $\renewcommand{\aptr}{\approx_{\operatorname{tr}}}$
&lt;/p&gt;

&lt;!-- End LaTeX command define section. --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;此篇論文&lt;/a&gt;與&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 都寫錯自己的數學公式，但我的筆記內容主要以正確版本為主，原版 LSTM 可以參考&lt;a href=&quot;/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html&quot;&gt;我的筆記&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 沒有遺忘閘門，現今常用的 LSTM 都有遺忘閘門，概念由這篇論文提出&lt;/li&gt;
  &lt;li&gt;包含多個子序列的&lt;strong&gt;連續輸入&lt;/strong&gt;會讓 LSTM 的記憶單元內部狀態沒有上下界
    &lt;ul&gt;
      &lt;li&gt;現實中的大多數資料並不存在好的分割序列演算法，導致輸入給模型的資料通常都包含多個子序列&lt;/li&gt;
      &lt;li&gt;根據實驗 1 的分析發現記憶單元內部狀態的累積導致預測結果完全錯誤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;使用遺忘閘門讓模型學會適當的忘記已經處理過的子序列資訊
    &lt;ul&gt;
      &lt;li&gt;當遺忘閘門的&lt;strong&gt;偏差項&lt;/strong&gt;初始化為&lt;strong&gt;正數&lt;/strong&gt;時會保持記憶單元內部狀態，等同於使用原版的 LSTM&lt;/li&gt;
      &lt;li&gt;因此使用遺忘閘門的 LSTM 能夠達成原版 LSTM 的功能，並額外擁有自動重設記憶單元的機制&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;這篇模型的理論背景較少，實驗為主的描述居多&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lstm&quot;&gt;原始 LSTM&lt;/h2&gt;

&lt;h3 id=&quot;section&quot;&gt;模型架構&lt;/h3&gt;

&lt;p&gt;根據&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始論文&lt;/a&gt;提出的架構如下（這篇論文不使用額外的&lt;strong&gt;隱藏單元&lt;/strong&gt;，因此我們也完全不列出隱藏單元相關的公式）（細節可以參考&lt;a href=&quot;/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html&quot;&gt;我的筆記&lt;/a&gt;）&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;符號&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\din$&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;輸入層&lt;/strong&gt;的維度&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;記憶單元區塊&lt;/strong&gt;的維度&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;記憶單元區塊&lt;/strong&gt;的個數&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dout$&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;輸出層&lt;/strong&gt;的維度&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$T$&lt;/td&gt;
      &lt;td&gt;輸入序列的長度&lt;/td&gt;
      &lt;td&gt;數值範圍為 $\Z^+$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;以下所有符號的時間 $t$ 範圍為 $t \in \set{0, \dots, T - 1}$&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;符號&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;維度&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$x(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸入&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\din$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y^{\opig}(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸入閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$y^{\opig}(0) = 0$，同一個記憶單元區塊&lt;strong&gt;共享輸入閘門&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y^{\opog}(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的&lt;strong&gt;輸出閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$y^{\opog}(0) = 0$，同一個記憶單元區塊&lt;strong&gt;共享輸出閘門&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$s^{\blk{k}}(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的第 $k$ 個&lt;strong&gt;記憶單元區塊內部狀態&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$s^{\blk{k}}(0) = 0$ 且 $k \in \set{1, \dots, \nblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y^{\blk{k}}(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的第 $k$ 個&lt;strong&gt;記憶單元區塊輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$y^{\blk{k}}(0) = 0$ 且 $k \in \set{1, \dots, \nblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y(t + 1)$&lt;/td&gt;
      &lt;td&gt;第 $t + 1$ 個時間點的&lt;strong&gt;輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\dout$&lt;/td&gt;
      &lt;td&gt;由 $t$ 時間點的&lt;strong&gt;輸入&lt;/strong&gt;與&lt;strong&gt;記憶單元輸出&lt;/strong&gt;透過&lt;strong&gt;全連接&lt;/strong&gt;產生，因此沒有 $y(0)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\hat{y}(t + 1)$&lt;/td&gt;
      &lt;td&gt;第 $t + 1$ 個時間點的&lt;strong&gt;預測目標&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$\dout$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;符號&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;下標範圍&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$x_j(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的第 $j$ 個&lt;strong&gt;輸入&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$j \in \set{1, \dots, \din}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_k^{\opig}(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點第 $k$ 個記憶單元區塊的&lt;strong&gt;輸入閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$k \in \set{1, \dots, \nblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_k^{\opog}(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點第 $k$ 個記憶單元區塊的&lt;strong&gt;輸出閘門&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$k \in \set{1, \dots, \nblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$s_i^{\blk{k}}(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的第 $k$ 個&lt;strong&gt;記憶單元區塊&lt;/strong&gt;的第 $i$ 個&lt;strong&gt;記憶單元內部狀態&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$i \in \set{1, \dots, \dblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_i^{\blk{k}}(t)$&lt;/td&gt;
      &lt;td&gt;第 $t$ 個時間點的第 $k$ 個&lt;strong&gt;記憶單元區塊&lt;/strong&gt;的第 $i$ 個&lt;strong&gt;記憶單元輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$i \in \set{1, \dots, \dblk}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$y_i(t + 1)$&lt;/td&gt;
      &lt;td&gt;第 $t + 1$ 個時間點的第 $i$ 個&lt;strong&gt;輸出&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$i \in \set{1, \dots, \dout}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\hat{y}_i(t + 1)$&lt;/td&gt;
      &lt;td&gt;第 $t + 1$ 個時間點的第 $i$ 個&lt;strong&gt;預測目標&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;$i \in \set{1, \dots, \dout}$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;輸出維度&lt;/th&gt;
      &lt;th&gt;輸入維度&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\wig$&lt;/td&gt;
      &lt;td&gt;產生&lt;strong&gt;輸入閘門&lt;/strong&gt;的全連接參數&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$\din + \nblk \cdot (2 + \dblk)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\wog$&lt;/td&gt;
      &lt;td&gt;產生&lt;strong&gt;輸出閘門&lt;/strong&gt;的全連接參數&lt;/td&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$\din + \nblk \cdot (2 + \dblk)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\wblk{k}$&lt;/td&gt;
      &lt;td&gt;產生第 $k$ 個&lt;strong&gt;記憶單元區塊淨輸入&lt;/strong&gt;的全連接參數&lt;/td&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$\din + \nblk \cdot (2 + \dblk)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\wout$&lt;/td&gt;
      &lt;td&gt;產生&lt;strong&gt;輸出&lt;/strong&gt;的全連接參數&lt;/td&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$\din + \nblk \cdot \dblk$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;定義 $\sigma$ 為 sigmoid 函數 $\sigma(x) = \frac{1}{1 + e^{-x}}$&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;函數&lt;/th&gt;
      &lt;th&gt;意義&lt;/th&gt;
      &lt;th&gt;公式&lt;/th&gt;
      &lt;th&gt;range&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$f_k^{\opig}$&lt;/td&gt;
      &lt;td&gt;第 $k$ 個&lt;strong&gt;輸入閘門&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;$[0, 1]$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$f_k^{\opog}$&lt;/td&gt;
      &lt;td&gt;第 $k$ 個&lt;strong&gt;輸出閘門&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;$[0, 1]$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$g_i^{\blk{k}}$&lt;/td&gt;
      &lt;td&gt;第 $k$ 個&lt;strong&gt;記憶單元區塊&lt;/strong&gt;中第 $i$ 個&lt;strong&gt;記憶單元內部狀態&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$4\sigma - 2$&lt;/td&gt;
      &lt;td&gt;$[-2, 2]$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$h_i^{\blk{k}}$&lt;/td&gt;
      &lt;td&gt;第 $k$ 個&lt;strong&gt;記憶單元區塊&lt;/strong&gt;中第 $i$ 個&lt;strong&gt;記憶單元輸出&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$2\sigma - 1$&lt;/td&gt;
      &lt;td&gt;$[-1, 1]$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$f_i^{\opout}$&lt;/td&gt;
      &lt;td&gt;第 $i$ 個&lt;strong&gt;輸出&lt;/strong&gt;的啟發函數&lt;/td&gt;
      &lt;td&gt;$\sigma$&lt;/td&gt;
      &lt;td&gt;$[0, 1]$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在 $t$ 時間點時得到&lt;strong&gt;輸入&lt;/strong&gt; $x(t)$，產生 $t + 1$ 時間點&lt;strong&gt;輸入閘門&lt;/strong&gt; $y^{\opig}(t + 1)$ 與&lt;strong&gt;輸出閘門&lt;/strong&gt; $y^{\opog}(t + 1)$ 的方法如下&lt;/p&gt;

\[\begin{align*}
\tilde{x}(t) &amp;amp; = \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix} \\
y^{\opig}(t + 1) &amp;amp; = f^{\opig}\pa{\opnet^{\opig}(t + 1)} = f^{\opig}\pa{\wig \cdot \tilde{x}(t)} \\
y^{\opog}(t + 1) &amp;amp; = f^{\opog}\pa{\opnet^{\opog}(t + 1)} = f^{\opog}\pa{\wog \cdot \tilde{x}(t)}
\end{align*} \tag{1}\label{1}\]

&lt;p&gt;利用 $\eqref{1}$ 產生 $t + 1$ 時間點的&lt;strong&gt;記憶單元內部狀態&lt;/strong&gt; $s^{\blk{k}}(t + 1)$ 方法如下&lt;/p&gt;

\[\begin{align*}
\tilde{x}(t) &amp;amp; = \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix} \\
k &amp;amp; \in \set{1, \dots, \nblk} \\
\opnet^{\blk{k}}(t + 1) &amp;amp; = \wblk{k} \cdot \tilde{x}(t) \\
s^{\blk{k}}(t + 1) &amp;amp; = s^{\blk{k}}(t) + y_k^{\opig}(t + 1) \cdot g(\opnet^{\blk{k}}(t + 1))
\end{align*} \tag{2}\label{2}\]

&lt;p&gt;注意第 $k$ 個記憶單元區塊內部狀態&lt;strong&gt;共享&lt;/strong&gt;輸入閘門 $y_k^{\opig}(t + 1)$。&lt;/p&gt;

&lt;p&gt;利用 $\eqref{1}\eqref{2}$ 產生 $t + 1$ 時間點的&lt;strong&gt;記憶單元輸出&lt;/strong&gt; $y^{\blk{k}}(t + 1)$ 方法如下&lt;/p&gt;

\[\begin{align*}
k &amp;amp; \in \set{1, \dots, \nblk} \\
y^{\blk{k}}(t + 1) &amp;amp; = y_k^{\opog}(t + 1) \cdot h\pa{s^{\blk{k}}(t + 1)}
\end{align*} \tag{3}\label{3}\]

&lt;p&gt;注意第 $k$ 個記憶單元區塊輸出&lt;strong&gt;共享輸出閘門&lt;/strong&gt; $y_k^{\opog}(t + 1)$。&lt;/p&gt;

&lt;p&gt;產生 $t + 1$ 時間點的&lt;strong&gt;輸出&lt;/strong&gt;是透過 $t$ 時間點的&lt;strong&gt;輸入&lt;/strong&gt;與 $t + 1$ 時間點的&lt;strong&gt;記憶單元輸出&lt;/strong&gt;（見 $\eqref{3}$）而得&lt;/p&gt;

\[\begin{align*}
\tilde{x}(t + 1) &amp;amp; = \begin{pmatrix}
x(t) \\
y^{\blk{1}}(t + 1) \\
\vdots \\
y^{\blk{\nblk}}(t + 1)
\end{pmatrix} \\
y(t + 1) &amp;amp; = f^{\opout}(\opnet^{\opout}(t + 1)) = f^{\opout}\pa{\wout \cdot \tilde{x}(t + 1)}
\end{align*} \tag{4}\label{4}\]

&lt;p&gt;&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;這篇論文&lt;/a&gt;與&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM 的論文&lt;/a&gt; 都不小心寫成 $t$ 時間點的記憶單元輸出，在 &lt;a href=&quot;https://www.jmlr.org/papers/v3/gers02a.html&quot;&gt;LSTM-2002&lt;/a&gt; 才終於寫對。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;最佳化&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM&lt;/a&gt; 提出與 truncated BPTT 相似的概念，透過 RTRL 進行參數更新，並故意&lt;strong&gt;丟棄流出記憶單元的所有梯度&lt;/strong&gt;，避免梯度爆炸或梯度消失的問題，同時節省更新所需的空間與時間（local in time and space）。（細節可見&lt;a href=&quot;/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html&quot;&gt;我的筆記&lt;/a&gt;）&lt;/p&gt;

&lt;p&gt;令 $t \in \set{0, \dots, T - 1}$，最佳化的目標為每個時間點 $t + 1$ 所產生的&lt;strong&gt;平方誤差總和最小化&lt;/strong&gt;&lt;/p&gt;

\[\begin{align*}
\oploss(t + 1) &amp;amp; = \sum_{i = 1}^{\dout} \oploss_i(t + 1) \\
&amp;amp; = \sum_{i = 1}^{\dout} \frac{1}{2} \big(y_i(t + 1) - \hat{y}_i(t + 1)\big)^2
\end{align*} \tag{5}\label{5}\]

&lt;p&gt;以下我們使用 $\aptr$ 代表&lt;strong&gt;丟棄部份梯度後的剩餘梯度&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;輸出參數的剩餘梯度為&lt;/p&gt;

\[\begin{align*}
\pd{\oploss(t + 1)}{\wout_{i, j}} &amp;amp; = \pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \pd{\netout{i}{t + 1}}{\wout_{i, j}} \\
&amp;amp; = \big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\blk{1}}(t + 1) \\
\vdots \\
y^{\blk{\nblk}}(t + 1)
\end{pmatrix}_j
\end{align*} \tag{6}\label{6}\]

&lt;p&gt;其中 $1 \leq i \leq \dout$ 且 $1 \leq j \leq \din + \nblk \cdot \dblk$。&lt;/p&gt;

&lt;p&gt;輸出閘門參數的剩餘梯度為&lt;/p&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t + 1)}{\wog_{k, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \\
&amp;amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{y_k^{\opog}(t + 1)}} \cdot \pd{y_k^{\opog}(t + 1)}{\netog{k}{t + 1}} \cdot \pd{\netog{k}{t + 1}}{\wog_{k, q}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \\
&amp;amp; \quad \pa{\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot \hblk{j}{k}{t + 1}} \cdot \dfnetog{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q\Bigg]
\end{align*} \tag{7}\label{7}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (2 + \dblk)$。&lt;/p&gt;

&lt;p&gt;輸入閘門參數的剩餘梯度為&lt;/p&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t + 1)}{\wig_{k, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \\
&amp;amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{s_j^{\blk{k}}(t + 1)} \cdot \pd{s_j^{\blk{k}}(t + 1)}{\wig_{k, q}}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{s_j^{\blk{k}}(t + 1)} \cdot \\
&amp;amp; \quad \quad \br{\pd{s_j^{\blk{k}}(t)}{\wig_{k, q}} + \gnetblk{j}{k}{t + 1} \cdot \pd{y_k^{\opig}(t + 1)}{\netig{k}{t + 1}} \cdot \pd{\netig{k}{t + 1}}{\wig_{k, q}}}\Bigg)\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \\
&amp;amp; \quad \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp;amp; \quad \quad \br{\pd{s_j^{\blk{k}}(t)}{\wig_{k, q}} + \gnetblk{j}{k}{t + 1} \cdot \dfnetig{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg)\Bigg]
\end{align*} \tag{8}\label{8}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (2 + \dblk)$。&lt;/p&gt;

&lt;p&gt;記憶單元淨輸入參數的剩餘梯度為&lt;/p&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t + 1)}{\wblk{k}_{p, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \br{\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \pd{\netout{i}{t + 1}}{y_p^{\blk{k}}(t + 1)} \cdot \pd{y_p^{\blk{k}}(t + 1)}{s_p^{\blk{k}}(t + 1)} \cdot \pd{s_p^{\blk{k}}(t + 1)}{\wblk{k}_{p, q}}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \pd{\netout{i}{t + 1}}{y_p^{\blk{k}}(t + 1)} \cdot \pd{y_p^{\blk{k}}(t + 1)}{s_p^{\blk{k}}(t + 1)} \cdot \\
&amp;amp; \quad \quad \pa{\pd{s_p^{\blk{k}}(t)}{\wblk{k}_{p, q}} + y_k^{\opig}(t + 1) \cdot \pd{\gnetblk{j}{k}{t + 1}}{\netblk{j}{k}{t + 1}} \cdot \pd{\netblk{j}{k}{t + 1}}{\wblk{k}_{p, q}}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot \\
&amp;amp; \quad y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp;amp; \quad \br{\pd{s_p^{\blk{k}}(t)}{\wblk{k}_{p, q}} + y_k^{\opig}(t + 1) \cdot \dgnetblk{p}{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg]
\end{align*} \tag{9}\label{9}\]

&lt;p&gt;其中 $1 \leq k \leq \nblk$， $1 \leq p \leq \dblk$ 且 $1 \leq q \leq \din + \nblk \cdot (2 + \dblk)$。&lt;/p&gt;

&lt;p&gt;計算完上述所有參數後使用&lt;strong&gt;梯度下降&lt;/strong&gt;（gradient descent）進行參數更新&lt;/p&gt;

\[\begin{align*}
\wout_{i, j} &amp;amp; \leftarrow \wout_{i, j} - \alpha \cdot \pd{\oploss(t + 1)}{\wout_{i, j}} \\
\wog_{k, q} &amp;amp; \leftarrow \wog_{k, q} - \alpha \cdot \pd{\oploss(t + 1)}{\wog_{k, q}} \\
\wig_{k, q} &amp;amp; \leftarrow \wig_{k, q} - \alpha \cdot \pd{\oploss(t + 1)}{\wig_{k, q}} \\
\wblk{k}_{p, q} &amp;amp; \leftarrow \wblk{k}_{p, q} - \alpha \cdot \pd{\oploss(t + 1)}{\wblk{k}_{p, q}}
\end{align*} \tag{10}\label{10}\]

&lt;p&gt;其中 $\alpha$ 為&lt;strong&gt;學習率&lt;/strong&gt;（&lt;strong&gt;learning rate&lt;/strong&gt;）。&lt;/p&gt;

&lt;p&gt;由於使用基於 RTRL 的最佳化演算法，因此每個時間點 $t + 1$ 計算完誤差後就可以更新參數。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;問題&lt;/h3&gt;

&lt;p&gt;當一個輸入序列中包含多個獨立的子序列（例如一個文章段落有多個句子），則模型無法知道不同獨立子序列的起始點在哪裡（除非有明確的切斷序列演算法，但實際上不一定存在）。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM&lt;/a&gt; 架構假設任意輸入序列都是由單一獨立序列組成，不會包含多個獨立的序列，因此會在每次序列&lt;strong&gt;輸入時重設模型的計算狀態&lt;/strong&gt; $y^{\opig}(0), y^{\opog}(0), s^{\blk{k}}(0), y^{\blk{k}}(0)$，沒有&lt;strong&gt;需要在計算過程中重設計算狀態的需求&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;但當輸入包含多個獨立的子序列時，且沒有明確的方法辨識不同獨立子序列的起始點時，LSTM 模型就必須要擁有能夠在任意時間點 $t$ &lt;strong&gt;重設計算狀態&lt;/strong&gt; $y^{\opig}(t), y^{\opog}(t), s^{\blk{k}}(t), y^{\blk{k}}(t)$ 的功能。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;遺忘閘門&lt;/h2&gt;

&lt;h3 id=&quot;section-4&quot;&gt;模型架構&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 1：在原始 LSTM 架構上增加遺忘閘門。
圖片來源：&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/ILRsaEU.png&quot; alt=&quot;圖 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;作者提出在模型中加入&lt;strong&gt;遺忘閘門&lt;/strong&gt;（&lt;strong&gt;forget gate&lt;/strong&gt;），概念是讓&lt;strong&gt;記憶單元內部狀態&lt;/strong&gt;能夠進行重設。&lt;/p&gt;

&lt;p&gt;首先需要計算&lt;strong&gt;遺忘閘門&lt;/strong&gt; $y^{\opfg}(t)$，定義如下&lt;/p&gt;

\[\begin{align*}
\tilde{x}(t) &amp;amp; = \begin{pmatrix}
x(t) \\
y^{\opfg}(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix} \\
y^{\opfg}(0) &amp;amp; = 0 \\
y^{\opfg}(t + 1) &amp;amp; = f^{\opfg}\pa{\opnet^{\opfg}(t + 1) = f^{\opfg}\pa{\wfg \cdot \tilde{x}(t)}}
\end{align*} \tag{11}\label{11}\]

&lt;p&gt;計算方法與輸入閘門和輸出閘門相同。&lt;/p&gt;

&lt;p&gt;而計算過程需要做以下修改&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\eqref{1}\eqref{2}$ 中的淨輸入需要加上 $y^{\opfg}(t)$&lt;/li&gt;
  &lt;li&gt;參數 $\wig, \wog, \wblk{k}$ 的輸入維度都改成 $\din + \nblk \cdot (3 + \dblk)$&lt;/li&gt;
  &lt;li&gt;$\wfg$ 的維度與 $\wig$ 完全相同&lt;/li&gt;
  &lt;li&gt;$f^{\opfg}$ 與 $f^{\opig}$ 的定義完全相同&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所謂的遺忘並不是直接設定成 $0$，而是以乘法閘門的形式進行數值重設，因此 $\eqref{2}$ 的計算改成&lt;/p&gt;

\[s^{\blk{k}}(t + 1) = y_k^{\opfg}(t + 1) \cdot s^{\blk{k}}(t) + y_k^{\opig}(t + 1) \cdot g(\opnet^{\blk{k}}(t + 1)) \tag{12}\label{12}\]

&lt;h3 id=&quot;section-5&quot;&gt;偏差項&lt;/h3&gt;

&lt;p&gt;如同&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM&lt;/a&gt;，&lt;strong&gt;輸入閘門&lt;/strong&gt;與&lt;strong&gt;輸出閘門&lt;/strong&gt;可以使用&lt;strong&gt;偏差項&lt;/strong&gt;（bias term），將偏差項初始化成&lt;strong&gt;負數&lt;/strong&gt;可以讓輸入閘門與輸出閘門在需要的時候才被啟用（細節可以看&lt;a href=&quot;/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html&quot;&gt;我的筆記&lt;/a&gt;）。&lt;/p&gt;

&lt;p&gt;而&lt;strong&gt;遺忘閘門&lt;/strong&gt;也可以使用偏差項，但初始化的數值應該為&lt;strong&gt;正數&lt;/strong&gt;，理由是在模型計算前期應該要讓遺忘閘門開啟（$y^{\opfg} \approx 1$），讓記憶單元內部狀態的數值能夠進行改變。&lt;/p&gt;

&lt;p&gt;注意遺忘閘門只有在&lt;strong&gt;關閉&lt;/strong&gt;（$y^{\opfg} \approx 0$）時才能進行遺忘，這個名字取得不是很好。&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;最佳化&lt;/h3&gt;

&lt;p&gt;基於&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM&lt;/a&gt; 的最佳化演算法，將流出遺忘閘門的梯度也一起&lt;strong&gt;丟棄&lt;/strong&gt;&lt;/p&gt;

\[\begin{align*}
\pd{\netfg{k}{t + 1}}{y_{k^{\star}}^{\opfg}(t)} &amp;amp; \aptr 0 &amp;amp;&amp;amp; k = 1, \dots, \nblk \\
\pd{\netfg{k}{t + 1}}{y_{k^{\star}}^{\opig}(t)} &amp;amp; \aptr 0 &amp;amp;&amp;amp; k^{\star} = 1, \dots, \nblk \\
\pd{\netfg{k}{t + 1}}{y_{k^{\star}}^{\opog}(t)} &amp;amp; \aptr 0 \\
\pd{\netfg{k}{t + 1}}{y_i^{\blk{k^{\star}}}(t)} &amp;amp; \aptr 0 &amp;amp;&amp;amp; i = 1, \dots, \dblk
\end{align*} \tag{13}\label{13}\]

&lt;p&gt;因此&lt;strong&gt;遺忘閘門的參數&lt;/strong&gt;剩餘梯度為&lt;/p&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t + 1)}{\wfg_{k, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \\
&amp;amp; \quad \pa{\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{s_j^{\blk{k}}(t + 1)} \cdot \pd{s_j^{\blk{k}}(t + 1)}{\wfg_{k, q}}}\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\pd{\oploss(t + 1)}{y_i(t + 1)} \cdot \pd{y_i(t + 1)}{\netout{i}{t + 1}} \cdot \Bigg(\sum_{j = 1}^{\dblk} \pd{\netout{i}{t + 1}}{y_j^{\blk{k}}(t + 1)} \cdot \pd{y_j^{\blk{k}}(t + 1)}{s_j^{\blk{k}}(t + 1)} \cdot \\
&amp;amp; \quad \quad \br{y_k^{\opfg}(t + 1) \cdot \pd{s_j^{\blk{k}}(t)}{\wfg_{k, q}} + s_j^{\blk{k}}(t) \cdot \pd{y_k^{\opfg}(t + 1)}{\netfg{k}{t + 1}} \cdot \pd{\netfg{k}{t + 1}}{\wfg_{k, q}}}\Bigg)\Bigg] \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \\
&amp;amp; \quad \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp;amp; \quad \quad \br{y_k^{\opfg}(t + 1) \cdot \pd{s_j^{\blk{k}}(t)}{\wfg_{k, q}}  + s_j^{\blk{k}}(t) \cdot \dfnetog{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg)\Bigg]
\end{align*} \tag{14}\label{14}\]

&lt;p&gt;$\eqref{14}$ 式就是論文的 3.12 式，其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。&lt;/p&gt;

&lt;p&gt;由於 $\eqref{12}$ 的修改，$\eqref{9} \eqref{10}$ 最佳化的過程也需要跟著修改。&lt;/p&gt;

&lt;p&gt;輸入閘門的參數剩餘梯度改為&lt;/p&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t + 1)}{\wig_{k, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \\
&amp;amp; \quad \Bigg(\sum_{j = 1}^{\dblk} \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp;amp; \quad \quad \br{y_k^{\opfg}(t + 1) \cdot \pd{s_j^{\blk{k}}(t)}{\wig_{k, q}} + \gnetblk{j}{k}{t + 1} \cdot \dfnetig{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg)\Bigg]
\end{align*} \tag{15}\label{15}\]

&lt;p&gt;$\eqref{14}$ 式就是論文的 3.11 式，其中 $1 \leq k \leq \nblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。&lt;/p&gt;

&lt;p&gt;記憶單元淨輸入參數的剩餘梯度改為&lt;/p&gt;

\[\begin{align*}
&amp;amp; \pd{\oploss(t + 1)}{\wblk{k}_{p, q}} \\
&amp;amp; \aptr \sum_{i = 1}^{\dout} \Bigg[\big(y_i(t + 1) - \hat{y}_i(t + 1)\big) \cdot \dfnetout{i}{t + 1} \cdot \wout_{i, \din + (k - 1) \cdot \dblk + j} \cdot \\
&amp;amp; \quad y_k^{\opog}(t + 1) \cdot \dhblk{j}{k}{t + 1} \cdot \\
&amp;amp; \quad \br{y_k^{\opfg}(t + 1) \cdot \pd{s_p^{\blk{k}}(t)}{\wblk{k}_{p, q}} + y_k^{\opig}(t + 1) \cdot \dgnetblk{p}{k}{t + 1} \cdot \begin{pmatrix}
x(t) \\
y^{\opfg}(t) \\
y^{\opig}(t) \\
y^{\opog}(t) \\
y^{\blk{1}}(t) \\
\vdots \\
y^{\blk{\nblk}}(t)
\end{pmatrix}_q}\Bigg]
\end{align*} \tag{16}\label{16}\]

&lt;p&gt;$\eqref{14}$ 式就是論文的 3.10 式，其中 $1 \leq k \leq \nblk$， $1 \leq p \leq \dblk$ 且 $1 \leq q \leq \din + \nblk \cdot (3 + \dblk)$。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注意錯誤&lt;/strong&gt;：根據論文中的 3.4 式，論文 2.5 式的 $t - 1$ 應該改成 $t$。&lt;/p&gt;

&lt;p&gt;根據 $\eqref{14}\eqref{15}\eqref{16}$，當遺忘閘門 $y_k^{\opfg}(t + 1) \approx 0$ （關閉）時，不只記憶單元 $s^{\blk{k}}(t + 1)$ 會重設，與其相關的梯度也會重設，因此更新時需要額外紀錄以下的項次&lt;/p&gt;

\[\pd{s_i^{\blk{k}}(t + 1)}{\wfg_{k, q}}, \pd{s_i^{\blk{k}}(t + 1)}{\wig_{k, q}}, \pd{s_i^{\blk{k}}(t + 1)}{\wblk{k}_{p, q}}\]

&lt;p&gt;同樣的概念在&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM&lt;/a&gt; 中也有出現，細節可以看&lt;a href=&quot;/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html&quot;&gt;我的筆記&lt;/a&gt;。&lt;/p&gt;

&lt;h2 id=&quot;continual-embedded-reber-grammar&quot;&gt;實驗 1：Continual Embedded Reber Grammar&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 2：Continual Embedded Reber Grammar。
圖片來源：&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/rhHtVRN.png&quot; alt=&quot;圖 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;任務定義&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;根據&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM 論文&lt;/a&gt;中的實驗 1（Embedded Reber Grammar）進行修改，輸入為連續序列，連續序列的定義是由多個 Embedded Reber Grammar 產生的序列組合而成（細節可以看&lt;a href=&quot;/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html&quot;&gt;我的筆記&lt;/a&gt;）&lt;/li&gt;
  &lt;li&gt;每個分支的生成機率值為 $0.5$&lt;/li&gt;
  &lt;li&gt;當所有輸出單元的平方誤差低於 $0.49$ 時就當成預測正確&lt;/li&gt;
  &lt;li&gt;在一次的訓練過程中，給予模型的輸入只會在以下兩種狀況之一發生時停止
    &lt;ul&gt;
      &lt;li&gt;當模型產生一次的預測錯誤&lt;/li&gt;
      &lt;li&gt;模型連續接收 $10^6$ 個輸入&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;每次訓練停止就進行一次測試
    &lt;ul&gt;
      &lt;li&gt;一次測試會執行 $10$ 次的連續輸入&lt;/li&gt;
      &lt;li&gt;評估結果是 $10$ 次連續輸入的平均值&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;每輸入一個訊號就進行更新（RTRL）&lt;/li&gt;
  &lt;li&gt;訓練最多執行 $30000$ 次，實驗結果由 $100$ 個訓練模型實驗進行平均&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lstm-&quot;&gt;LSTM 架構&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 3：LSTM 架構。
圖片來源：&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/uUJjmSz.png&quot; alt=&quot;圖 3&quot; /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;參數&lt;/th&gt;
      &lt;th&gt;數值（或範圍）&lt;/th&gt;
      &lt;th&gt;備註&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\din$&lt;/td&gt;
      &lt;td&gt;$7$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\nblk$&lt;/td&gt;
      &lt;td&gt;$4$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dblk$&lt;/td&gt;
      &lt;td&gt;$2$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dout$&lt;/td&gt;
      &lt;td&gt;$7$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wblk{k})$&lt;/td&gt;
      &lt;td&gt;$\dblk \times [\din + \nblk \cdot \dblk]$&lt;/td&gt;
      &lt;td&gt;訊號來源為外部輸入與記憶單元&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wfg)$&lt;/td&gt;
      &lt;td&gt;$\nblk \times [\din + \nblk \cdot \dblk + 1]$&lt;/td&gt;
      &lt;td&gt;訊號來源為外部輸入與記憶單元，有額外使用偏差項&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wig)$&lt;/td&gt;
      &lt;td&gt;$\nblk \times [\din + \nblk \cdot \dblk + 1]$&lt;/td&gt;
      &lt;td&gt;訊號來源為外部輸入與記憶單元，有額外使用偏差項&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wog)$&lt;/td&gt;
      &lt;td&gt;$\nblk \times [\din + \nblk \cdot \dblk + 1]$&lt;/td&gt;
      &lt;td&gt;訊號來源為外部輸入與記憶單元，有額外使用偏差項&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;$\dim(\wout)$&lt;/td&gt;
      &lt;td&gt;$\dout \times [\din + \nblk \cdot \dblk + 1]$&lt;/td&gt;
      &lt;td&gt;訊號來源為外部輸入與記憶單元，有額外使用偏差項&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;總參數量&lt;/td&gt;
      &lt;td&gt;$424$&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;參數初始化&lt;/td&gt;
      &lt;td&gt;$[-0.2, 0.2]$&lt;/td&gt;
      &lt;td&gt;平均分佈&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;輸入閘門偏差項初始化&lt;/td&gt;
      &lt;td&gt;$\set{-0.5, -1.0, -1.5, -2.0}$&lt;/td&gt;
      &lt;td&gt;依序初始化成不同數值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;輸出閘門偏差項初始化&lt;/td&gt;
      &lt;td&gt;$\set{-0.5, -1.0, -1.5, -2.0}$&lt;/td&gt;
      &lt;td&gt;依序初始化成不同數值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;遺忘閘門偏差項初始化&lt;/td&gt;
      &lt;td&gt;$\set{0.5, 1.0, 1.5, 2.0}$&lt;/td&gt;
      &lt;td&gt;依序初始化成不同數值&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Learning rate $\alpha$&lt;/td&gt;
      &lt;td&gt;$0.5$&lt;/td&gt;
      &lt;td&gt;訓練過程可以固定 $\alpha$，或是以 $0.99$ 的 decay factor 在每次更新後進行衰減&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;section-8&quot;&gt;實驗結果&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 4：Continual Embedded Reber Grammar 實驗結果。
圖片來源：&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/uu9Nccj.png&quot; alt=&quot;圖 4&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM&lt;/a&gt; 在有手動進行計算狀態的重置時表現非常好，但當沒有手動重置時完全無法執行任務
    &lt;ul&gt;
      &lt;li&gt;就算讓記憶單元內部狀態進行 decay 也無濟於事&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;使用遺忘閘門的 LSTM 不需要手動重置計算狀態也能達成完美預測
    &lt;ul&gt;
      &lt;li&gt;完美預測指的是連續 $10^6$ 輸入都預測正確&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;有嘗試使用 $\alpha / t$ 或 $\alpha / \sqrt{T}$ 作為 learning rate，實驗發現不論是哪種最佳化的方法使用遺忘閘門的 LSTM 都表現的不錯
    &lt;ul&gt;
      &lt;li&gt;在其他模型架構上（包含原版 LSTM）就算使用這些最佳化演算法也無法解決任務&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;額外實驗在將 Embedded Reber Grammar 開頭的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B&lt;/code&gt; 與結尾的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;E&lt;/code&gt; 去除的狀態下，使用遺忘閘門的 LSTM 仍然表現不錯&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-9&quot;&gt;分析&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-5&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 5：&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 記憶單元內部狀態的累加值。
圖片來源：&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/qwU4pnG.png&quot; alt=&quot;圖 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-6&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 6：LSTM 加上遺忘閘門後第三個記憶單元內部狀態。
圖片來源：&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/jtLnfu2.png&quot; alt=&quot;圖 6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-7&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 7：LSTM 加上遺忘閘門後第一個記憶單元內部狀態。
圖片來源：&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/K1mp9rg.png&quot; alt=&quot;圖 7&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;觀察&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原版 LSTM&lt;/a&gt; 的記憶單元內部狀態，可以發現在不進行手動重設的狀態下，記憶單元內部狀態的數值只會不斷的累加（朝向極正或極負前進）&lt;/li&gt;
  &lt;li&gt;觀察架上遺忘閘門後 LSTM 的記憶單元內部狀態，可以發現模型學會自動重設
    &lt;ul&gt;
      &lt;li&gt;在第三個記憶單元中展現了長期記憶重設的能力&lt;/li&gt;
      &lt;li&gt;在第一個記憶單元中展現了短期記憶重設的能力&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;noisy-temporal-order-problem&quot;&gt;實驗 2：Noisy Temporal Order Problem&lt;/h2&gt;

&lt;h3 id=&quot;section-10&quot;&gt;任務定義&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;就是&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM 論文&lt;/a&gt;中的實驗 6b，細節可以看&lt;a href=&quot;/general%20sequence%20modeling/optimization/2021/11/14/long-short-term-memory.html&quot;&gt;我的筆記&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;由於此任務需要讓記憶維持一段不短的時間，因此遺忘資訊對於這個任務可能有害，透過這個任務想要驗證是否有任務是只能使用原版 LSTM 可以解決但增加遺忘閘門後不能解決&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lstm--1&quot;&gt;LSTM 架構&lt;/h3&gt;

&lt;p&gt;與實驗 1 大致相同，只做以下修改&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\din = \dout = 8$&lt;/li&gt;
  &lt;li&gt;將遺忘閘門的偏差項初始化成較大的正數（論文使用 $5$），讓遺忘閘門很難被關閉，藉此達到跟原本 LSTM 幾乎相同的計算能力&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-11&quot;&gt;實驗結果&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;使用遺忘閘門的 LSTM 仍然能夠解決 Noisy Temporal Order Problem
    &lt;ul&gt;
      &lt;li&gt;當偏差項初始化成較大的正數（例如 $5$）時，收斂速度與原版 LSTM 一樣快&lt;/li&gt;
      &lt;li&gt;當偏差項初始化成較小的正數（例如 $1$）時，收斂速度約為原版 LSTM 的 $3$ 倍&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;因此根據實驗沒有什麼任務是原版 LSTM 可以解決但加上遺忘閘門後不能解決的&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;continual-noisy-temporal-order-problem&quot;&gt;實驗 3：Continual Noisy Temporal Order Problem&lt;/h2&gt;

&lt;h3 id=&quot;section-12&quot;&gt;任務定義&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;根據&lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/6795963&quot;&gt;原始 LSTM 論文&lt;/a&gt;中的實驗 6b 進行修改，輸入為連續序列，連續序列的定義是由 $100$ 筆 Noisy Temporal Order 序列所組成&lt;/li&gt;
  &lt;li&gt;在一次的訓練過程中，給予模型的輸入只會在以下兩種狀況之一發生時停止
    &lt;ul&gt;
      &lt;li&gt;當模型產生一次的預測錯誤&lt;/li&gt;
      &lt;li&gt;模型連續接收 $100$ 個 Noisy Temporal Order 序列&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;每次訓練停止就進行一次測試
    &lt;ul&gt;
      &lt;li&gt;一次測試會執行 $10$ 次的連續輸入&lt;/li&gt;
      &lt;li&gt;評估結果是 $10$ 次連續輸入中預測正確的序列個數平均值&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;論文沒有講怎麼計算誤差與更新，我猜變成每個非預測時間點必須輸出 $0$，預測時間點時輸出預測結果&lt;/li&gt;
  &lt;li&gt;訓練最多執行 $10^5$ 次，實驗結果由 $100$ 個訓練模型實驗進行平均&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lstm--2&quot;&gt;LSTM 架構&lt;/h3&gt;

&lt;p&gt;與實驗 2 相同。&lt;/p&gt;

&lt;h3 id=&quot;section-13&quot;&gt;實驗結果&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;paper-fig-8&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;圖 8：Continual Noisy Temporal Order Problem 實驗結果。
圖片來源：&lt;a href=&quot;https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM&quot;&gt;論文&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/VV5wQVG.png&quot; alt=&quot;圖 8&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#paper-fig-8&quot;&gt;圖 8&lt;/a&gt; 中的註解 a 應該寫錯了，應該改為 correct classification of 100 successive NTO sequences&lt;/li&gt;
  &lt;li&gt;實驗再次驗證原版 LSTM 無法解決連續輸入，但使用輸入閘門後就能夠解決問題&lt;/li&gt;
  &lt;li&gt;將 learning rate 使用 decay factor $0.9$ 逐漸下降可以讓模型表現變更好，但作者認為這不重要&lt;/li&gt;
&lt;/ul&gt;</content><author><name>[&quot;Felix A. Gers&quot;, &quot;Jürgen Schmidhuber&quot;, &quot;Fred Cummins&quot;]</name></author><category term="General Sequence Modeling" /><category term="RNN" /><category term="LSTM" /><category term="model architecture" /><category term="neural network" /><summary type="html">目標 提出在 LSTM 上增加 forget gate 作者 Felix A. Gers, Jürgen Schmidhuber, Fred Cummins 期刊/會議名稱 Neural Computation 發表時間 2000 論文連結 https://direct.mit.edu/neco/article-abstract/12/10/2451/6415/Learning-to-Forget-Continual-Prediction-with-LSTM</summary></entry><entry><title type="html">Learning representations by back-propagating errors</title><link href="/optimization/2021/12/07/learning-representations-by-backpropagating-errors.html" rel="alternate" type="text/html" title="Learning representations by back-propagating errors" /><published>2021-12-07T15:15:00+08:00</published><updated>2021-12-07T15:15:00+08:00</updated><id>/optimization/2021/12/07/learning-representations-by-backpropagating-errors</id><content type="html" xml:base="/optimization/2021/12/07/learning-representations-by-backpropagating-errors.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;目標&lt;/td&gt;
      &lt;td&gt;提出 back-propagation&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;作者&lt;/td&gt;
      &lt;td&gt;David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;期刊/會議名稱&lt;/td&gt;
      &lt;td&gt;Nature&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;發表時間&lt;/td&gt;
      &lt;td&gt;1986&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;論文連結&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://www.nature.com/articles/323533a0&quot;&gt;https://www.nature.com/articles/323533a0&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;!--
  Define LaTeX command which will be used through out the writing.

  Each command must be wrapped with $ signs.
  We use &quot;display: none;&quot; to avoid redudant whitespaces.
 --&gt;

&lt;p style=&quot;display: none;&quot;&gt;

  $\newcommand{\opin}{\operatorname{in}}$
  $\newcommand{\opout}{\operatorname{out}}$
  $\newcommand{\din}{d_{\opin}}$
  $\newcommand{\dout}{d_{\opout}}$

&lt;/p&gt;

&lt;!-- End LaTeX command define section. --&gt;

&lt;h2 id=&quot;section&quot;&gt;重點&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;提出 back-propagation，只需要 $1$ 階微分的最佳化演算法
    &lt;ul&gt;
      &lt;li&gt;相較於 $2$ 階微分計算複雜度低&lt;/li&gt;
      &lt;li&gt;不使用 $2$ 階微分無法保證收斂，但根據實驗大部份模型最佳化時都會透過 $1$ 階微分找到 global minimum&lt;/li&gt;
      &lt;li&gt;作者說，當你遇到模型卡在 local minimum 時，多加一點參數他就會繞過去了（幹話王）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;可以套用到 RNN 模型上，現在稱為 BPTT&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;模型架構&lt;/h2&gt;

&lt;p&gt;令模型有 $L$ 層，每一層以 $l$ 表示，因此 $l \in \set{1, \dots L}$。
定義第 $1$ 層為輸入層，第 $L$ 層為輸出層，剩餘的所有層都稱為隱藏層。
由於模型一定包含輸入與輸出，因此 $L \geq 2$。&lt;/p&gt;

&lt;p&gt;定義第 $l$ 層的架構如下&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;輸入定義為 $x[l]$，維度為 $\din[l]$
    &lt;ul&gt;
      &lt;li&gt;以下標 $j$ 表示第 $j$ 個輸入&lt;/li&gt;
      &lt;li&gt;$j$ 的範圍為 $j = 1, \dots \din[l]$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;淨輸入定義為 $a[l]$，維度為 $\dout[l]$
    &lt;ul&gt;
      &lt;li&gt;以下標 $i$ 表示第 $i$ 個淨輸入&lt;/li&gt;
      &lt;li&gt;$i$ 的範圍為 $i = 1, \dots \dout[l]$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;輸出定義為 $y[l]$，維度為 $\dout[l]$
    &lt;ul&gt;
      &lt;li&gt;以下標 $i$ 表示第 $i$ 個輸出&lt;/li&gt;
      &lt;li&gt;$i$ 的範圍為 $i = 1, \dots \dout[l]$&lt;/li&gt;
      &lt;li&gt;第 $l$ 層的輸出會作為第 $l + 1$ 層的輸入，因此 $\din[l + 1] = \dout[l]$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;參數為 $w[l]$，維度為 $\dout[l] \times \din[l]$
    &lt;ul&gt;
      &lt;li&gt;輸入維度為 $\din[l]$&lt;/li&gt;
      &lt;li&gt;輸出維度為 $\dout[l]$&lt;/li&gt;
      &lt;li&gt;以下標 $w_{i, j}[l]$ 代表第 $j$ 個輸入 $x_j[l]$ 與第 $i$ 個淨輸入 $a_i[l]$ 相接的參數&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;計算方法如下&lt;/p&gt;

\[\begin{align*}
a_i[l] &amp;amp; = \sum_{j = 1}^{\din[l]} w_{i, j}[l] \cdot x_j[l] &amp;amp;&amp;amp; \forall i = 1, \dots, \dout[l] \\
a[l] &amp;amp; = w[l] \cdot x[l] \\
y_i[l] &amp;amp; = f\big(a_i[l]\big) = \frac{1}{1 + \exp(-a_i[l])} &amp;amp;&amp;amp; \forall i = 1, \dots, \dout[l] \\
x[l + 1] &amp;amp; = y[l]
\end{align*} \tag{1}\label{1}\]

&lt;ul&gt;
  &lt;li&gt;$f$ 定義成 sigmoid 函數 $\sigma$
    &lt;ul&gt;
      &lt;li&gt;論文說可以不用是 sigmoid，只要是任何可微分且微分值有上下界即可&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;使用線性組合再配合非線性轉換讓學習的過程簡單許多&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;目標函數&lt;/h2&gt;

&lt;p&gt;若資料集有 $N$ 筆資料，則目標函數定義為&lt;/p&gt;

\[\begin{align*}
E &amp;amp; = \frac{1}{2} \sum_{n = 1}^N E^n\big(x^n, \hat{y}^{n}\big) \\
&amp;amp; = \frac{1}{2} \sum_{n = 1}^N \sum_{i = 1}^{\dout[L]} \big(y_i^n[L] - \hat{y}_i^n\big)^2
\end{align*} \tag{2}\label{2}\]

&lt;ul&gt;
  &lt;li&gt;以 $E^n$ 代表第 $n$ 筆資料的誤差&lt;/li&gt;
  &lt;li&gt;以 $y^n[L]$ 代表第 $n$ 筆資料在第 $L$ 層的輸出&lt;/li&gt;
  &lt;li&gt;以 $\hat{y}^n$ 代表第 $n$ 筆資料的預測目標&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目標為透過最佳化 $w$ 的過常達到 $E$ 的數值最小化。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;最佳化&lt;/h2&gt;

&lt;p&gt;為了方便描述，假設 $N = 1$，唯一的資料為 $x = x[1]$，且模型在輸入 $x[1]$ 後得到的輸出為 $y[L]$。&lt;/p&gt;

&lt;p&gt;首先計算模型輸出 $y[L]$ 對於 $\eqref{2}$ 中的 $E$ 的梯度&lt;/p&gt;

\[\pd{E}{y_i[L]} = y_i[L] - \hat{y}_i \quad \forall i = 1, \dots, \dout[L] \tag{3}\label{3}\]

&lt;p&gt;令 $y[L], \hat{y}$ 為行向量（column vector），則全微分的寫法為&lt;/p&gt;

\[\begin{align*}
\pd{E}{y[L]} &amp;amp; = \nabla E \big(y[L]\big) \\
&amp;amp; = \begin{pmatrix}
\pd{E}{y_1[L]} &amp;amp; \pd{E}{y_2[L]} &amp;amp; \cdots &amp;amp; \pd{E}{y_{\dout[L]}[L]}
\end{pmatrix} \\
&amp;amp; = \begin{pmatrix}
y_1[L] - \hat{y}_1 &amp;amp; y_2[L] - \hat{y}_2 &amp;amp; \cdots &amp;amp; y_{\dout[L]}[L] - \hat{y}_{\dout[L]}
\end{pmatrix} \\
&amp;amp; = \big(y[L] - \hat{y}\big)^T
\end{align*} \tag{4}\label{4}\]

&lt;p&gt;這裡我們採用 nominator-layout notation 與梯度為列向量（row vector）的習慣。&lt;/p&gt;

&lt;p&gt;假設模型使用的啟發函數（activation function）為 sigmoid 函數 $\sigma$，則根據 chain rule 我們可以利用 $\eqref{1}\eqref{3}\eqref{4}$ 求得淨輸入 $a[L]$ 對於 $E$ 的梯度&lt;/p&gt;

\[\begin{align*}
\pd{E}{a_i[L]} &amp;amp; = \pd{E}{y_i[L]} \cdot \pd{y_i[L]}{a_i[L]} &amp;amp;&amp;amp; \forall i = 1, \dots, \dout[L] \\
&amp;amp; = \pd{E}{y_i[L]} \cdot \sigma'(a_i[L]) \\
&amp;amp; = \pd{E}{y_i[L]} \cdot \sigma(a_i[L]) \cdot \big(1 - \sigma(a_i[L])\big) \\
&amp;amp; = \pd{E}{y_i[L]} \cdot y_i[L] \cdot \big(1 - y_i[L]\big) \\
&amp;amp; = \big(y_i[L] - \hat{y}_i\big) \cdot y_i[L] \cdot \big(1 - y_i[L]\big) \\
\end{align*} \tag{5}\label{5}\]

\[\begin{align*}
\pd{E}{a[L]} &amp;amp; = \pd{E}{y[L]} \cdot \pd{y[L]}{a[L]} \\
&amp;amp; = \big(y[L] - \hat{y}\big)^T \cdot \begin{pmatrix}
\pd{y_1[L]}{a_1[L]} &amp;amp; \pd{y_1[L]}{a_2[L]} &amp;amp; \cdots &amp;amp; \pd{y_1[L]}{a_{\dout[L]}[L]} \\
\pd{y_2[L]}{a_1[L]} &amp;amp; \pd{y_2[L]}{a_2[L]} &amp;amp; \cdots &amp;amp; \pd{y_2[L]}{a_{\dout[L]}[L]} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\pd{y_{\dout[L]}[L]}{a_1[L]} &amp;amp; \pd{y_{\dout[L]}[L]}{a_2[L]} &amp;amp; \cdots &amp;amp; \pd{y_{\dout[L]}[L]}{a_{\dout[L]}[L]}
\end{pmatrix} \\
&amp;amp; = \big(y[L] - \hat{y}\big)^T \cdot \begin{pmatrix}
\pd{y_1[L]}{a_1[L]} &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
0 &amp;amp; \pd{y_2[L]}{a_2[L]} &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; \pd{y_{\dout[L]}[L]}{a_{\dout[L]}[L]}
\end{pmatrix} \\
&amp;amp; = \big(y[L] - \hat{y}\big)^T \odot \begin{pmatrix}
\pd{y_1[L]}{a_1[L]} &amp;amp; \pd{y_2[L]}{a_2[L]} &amp;amp; \cdots &amp;amp; \pd{y_{\dout[L]}[L]}{a_{\dout[L]}[L]} \\
\end{pmatrix} \\
&amp;amp; = \big(y[L] - \hat{y}\big)^T \odot \begin{pmatrix}
y_1[L] \cdot \big(1 - y_1[L]\big) \\
y_2[L] \cdot \big(1 - y_2[L]\big) \\
\vdots \\
y_{\dout[L]}[L] \cdot \big(1 - y_{\dout[L]}[L]\big)
\end{pmatrix}^T \\
&amp;amp; = \big(y[L] - \hat{y}\big)^T \odot \Big(y[L] \odot \big(1 - y[L]\big)\Big)^T \\
&amp;amp; = \Big(\big(y[L] - \hat{y}\big) \odot y[L] \odot \big(1 - y[L]\big)\Big)^T
\end{align*} \tag{6}\label{6}\]

&lt;p&gt;其中 $\odot$ 代表 elementwise product。&lt;/p&gt;

&lt;p&gt;接著利用 $\eqref{1}\eqref{5}\eqref{6}$ 我們可以算出第 $L$ 層的參數 $w[L]$ 對於 $E$ 的梯度&lt;/p&gt;

\[\begin{align*}
\pd{E}{w_{i, j}[L]} &amp;amp; = \pd{E}{a_i[L]} \cdot \pd{a_i[L]}{w_{i, j}[L]} &amp;amp;&amp;amp; \forall i = 1, \dots, \dout[L] \\
&amp;amp; = \pd{E}{a_i[L]} \cdot x_j[L] &amp;amp;&amp;amp; \forall j = 1, \dots, \din[L] \\
&amp;amp; = \pd{E}{a_i[L]} \cdot y_j[L - 1] \\
&amp;amp; = \big(y_i[L] - \hat{y}_i\big) \cdot y_i[L] \cdot \big(1 - y_i[L]\big) \cdot y_j[L - 1]
\end{align*} \tag{7}\label{7}\]

\[\begin{align*}
\pd{E}{w_i[L]} &amp;amp; = \pd{E}{a[L]} \cdot \pd{a[L]}{w_i[L]} &amp;amp;&amp;amp; \forall i = 1, \dots, \dout[L] \\
&amp;amp; = \pd{E}{a[L]} \cdot \begin{pmatrix}
\pd{a_1[L]}{w_{i, 1}[L]} &amp;amp; \pd{a_1[L]}{w_{i, 2}[L]} &amp;amp; \cdots &amp;amp; \pd{a_1[L]}{w_{i, \dout[L]}[L]} \\
\pd{a_2[L]}{w_{i, 1}[L]} &amp;amp; \pd{a_2[L]}{w_{i, 2}[L]} &amp;amp; \cdots &amp;amp; \pd{a_2[L]}{w_{i, \dout[L]}[L]} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\pd{a_{\dout[L]}[L]}{w_{i, 1}[L]} &amp;amp; \pd{a_{\dout[L]}[L]}{w_{i, 2}[L]} &amp;amp; \cdots &amp;amp; \pd{a_{\dout[L]}[L]}{w_{i, \din[L]}[L]}
\end{pmatrix} \\
&amp;amp; = \pd{E}{a[L]} \cdot \begin{pmatrix}
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
x_1[L] &amp;amp; x_2[L] &amp;amp; \cdots &amp;amp; x_{\din[L]} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0
\end{pmatrix} \\
&amp;amp; = \pd{E}{a_i[L]} \cdot x[L]^T
\end{align*} \tag{8}\label{8}\]

&lt;p&gt;其中 $w_i$ 代表參數 $w$ 的第 $i$ 列（$i$th row）。
由於 $w$ 的微分展開需要三個維度，無法使用二維空間表達，因此我按照每個列展開，最後再將展開的列合併成矩陣。&lt;/p&gt;

\[\pd{E}{w} = \begin{pmatrix}
\pd{E}{w_1} \\
\pd{E}{w_2} \\
\vdots \\
\pd{E}{w_{\dout[L]}}
\end{pmatrix} = \begin{pmatrix}
\pd{E}{a_1[L]} \cdot x[L]^T \\
\pd{E}{a_2[L]} \cdot x[L]^T \\
\vdots \\
\pd{E}{a_{\dout[L]}[L]} \cdot x[L]^T
\end{pmatrix} \tag{9}\label{9}\]

&lt;p&gt;以 $\eqref{1}\eqref{5}\eqref{6}$ 我們也可以算出第 $L$ 層的輸入 $x[L]$ 對於 $E$ 的梯度&lt;/p&gt;

\[\begin{align*}
\pd{E}{x_j[L]} &amp;amp; = \sum_{i = 1}^{\dout[L]} \pd{E}{a_i[L]} \cdot \pd{a_i[L]}{x_j[L]} &amp;amp;&amp;amp; \forall j = 1, \dots, \din[L] \\
&amp;amp; = \sum_{i = 1}^{\dout[L]} \pd{E}{a_i[L]} \cdot w_{i, j}[L] \\
&amp;amp; = \sum_{i = 1}^{\dout[L]} \big(y_i[L] - \hat{y}_i\big) \cdot y_i[L] \cdot \big(1 - y_i[L]\big) \cdot w_{i, j}[L]
\end{align*} \tag{10}\label{10}\]

\[\begin{align*}
\pd{E}{x[L]} &amp;amp; = \pd{E}{a[L]} \cdot \pd{a[L]}{x[L]} \\
&amp;amp; = \pd{E}{a[L]} \cdot \begin{pmatrix}
\pd{a_1[L]}{x_1[L]} &amp;amp; \pd{a_1[L]}{x_2[L]} &amp;amp; \cdots &amp;amp; \pd{a_1[L]}{x_{\din[L]}[L]} \\
\pd{a_2[L]}{x_1[L]} &amp;amp; \pd{a_2[L]}{x_2[L]} &amp;amp; \cdots &amp;amp; \pd{a_2[L]}{x_{\din[L]}[L]} \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
\pd{a_{\dout[L]}[L]}{x_1[L]} &amp;amp; \pd{a_{\dout[L]}[L]}{x_2[L]} &amp;amp; \cdots &amp;amp; \pd{a_{\dout[L]}[L]}{x_{\din[L]}[L]}
\end{pmatrix} \\
&amp;amp; = \pd{E}{a[L]} \cdot \begin{pmatrix}
w_{1, 1}[L] &amp;amp; w_{1, 2}[L] &amp;amp; \cdots &amp;amp; w_{1, \din[L]}[L] \\
w_{2, 1}[L] &amp;amp; w_{2, 2}[L] &amp;amp; \cdots &amp;amp; w_{2, \din[L]}[L] \\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
w_{\dout[L], 1}[L] &amp;amp; w_{\dout[L], 2}[L] &amp;amp; \cdots &amp;amp; w_{\dout[L], \din[L]}[L]
\end{pmatrix} \\
&amp;amp; = \pd{E}{a[L]} \cdot w[L] \\
&amp;amp; = \Big(\big(y[L] - \hat{y}\big) \odot y[L] \odot \big(1 - y[L]\big)\Big)^T \cdot w[L]
\end{align*} \tag{11}\label{11}\]

&lt;p&gt;由於 $x[L] = y[L - 1]$，我們根據 $\eqref{11}$ 可以推得&lt;/p&gt;

\[\pd{E}{x[L]} = \pd{E}{y[L - 1]} = \Big(\big(y[L] - \hat{y}\big) \odot y[L] \odot \big(1 - y[L]\big)\Big)^T \cdot w[L] \tag{12}\label{12}\]

&lt;p&gt;依照 $\eqref{5} \eqref{7} \eqref{10}$ 的結果我們可以推得&lt;/p&gt;

\[\begin{align*}
&amp;amp; \pd{E}{a_i[L - 1]} &amp;amp;&amp;amp; \forall i = 1, \dots, \dout[L - 1] \\
&amp;amp; = \pd{E}{y_i[L - 1]} \cdot \pd{y_i[L - 1]}{a_i[L - 1]} \\
&amp;amp; = \pd{E}{x_i[L]} \cdot y_i[L - 1] \cdot \big(1 - y_i[L - 1]\big) \\
&amp;amp; = \bigg(\sum_{k = 1}^{\dout[L]} \big(y_k[L] - \hat{y}_k[L]\big) \cdot y_k[L] \cdot \big(1 - y_k[L]\big) \cdot w_{k, i}[L]\bigg) \\
&amp;amp; \quad \cdot y_i[L - 1] \cdot \big(1 - y_i[L - 1]\big)
\end{align*} \tag{13}\label{13}\]

\[\begin{align*}
&amp;amp; \pd{E}{w_{i, j}[L - 1]} &amp;amp;&amp;amp; \forall i = 1, \dots, \dout[L - 1] \\
&amp;amp; = \pd{E}{a_i[L - 1]} \cdot \pd{a_i[L - 1]}{w_{i, j}} &amp;amp;&amp;amp; \forall j = 1, \dots, \din[L - 1] \\
&amp;amp; = \bigg(\sum_{k = 1}^{\dout[L]} \big(y_k[L] - \hat{y}_k[L]\big) \cdot y_k[L] \cdot \big(1 - y_k[L]\big) \cdot w_{k, i}[L]\bigg) \\
&amp;amp; \quad \cdot y_i[L - 1] \cdot \big(1 - y_i[L - 1]\big) \cdot y_j[L - 2]
\end{align*} \tag{14}\label{14}\]

\[\begin{align*}
&amp;amp; \pd{E}{x_j[L - 1]} &amp;amp;&amp;amp; \forall j = 1, \dots, \din[L - 1] \\
&amp;amp; = \sum_{i = 1}^{\dout[L - 1]} \pd{E}{a_i[L - 1]} \cdot \pd{a_i[L - 1]}{x_j[L - 1]} \\
&amp;amp; = \sum_{i = 1}^{\dout[L - 1]} \Bigg[\bigg(\sum_{k = 1}^{\dout[L]} \big(y_k[L] - \hat{y}_k[L]\big) \cdot y_k[L] \cdot \big(1 - y_k[L]\big) \cdot w_{k, i}[L]\bigg) \\
&amp;amp; \quad \cdot y_i[L - 1] \cdot \big(1 - y_i[L - 1]\big)\Bigg] \cdot w_{i j}[L - 1] \\
\end{align*} \tag{15}\label{15}\]

&lt;p&gt;依照 $\eqref{6} \eqref{9} \eqref{11} \eqref{12}$ 的結果我們可以推得&lt;/p&gt;

\[\begin{align*}
&amp;amp; \pd{E}{a[L - 1]} \\
&amp;amp; = \pd{E}{y[L - 1]} \cdot \pd{y[L - 1]}{a[L - 1]} \\
&amp;amp; = \bigg[\Big(\big(y[L] - \hat{y}\big) \odot y[L] \odot \big(1 - y[L]\big)\Big)^T \cdot w\bigg] \odot \Big(y[L - 1] \odot \big(1 - y[L - 1]\big)\Big)^T
\end{align*} \tag{16}\label{16}\]

\[\begin{align*}
\pd{E}{w[L - 1]} &amp;amp; = \begin{pmatrix}
\pd{E}{a_1[L - 1]} \cdot x[L - 1]^T \\
\pd{E}{a_2[L - 1]} \cdot x[L - 1]^T \\
\vdots \\
\pd{E}{a_{\dout[L - 1]}[L - 1]} \cdot x[L - 1]^T
\end{pmatrix}
\end{align*} \tag{17}\label{17}\]

\[\begin{align*}
&amp;amp; \pd{E}{x[L - 1]} \\
&amp;amp; = \pd{E}{a[L - 1]} \cdot \pd{a[L - 1]}{x[L - 1]} \\
&amp;amp; = \Bigg[\bigg[\Big(\big(y[L] - \hat{y}\big) \odot y[L] \odot \big(1 - y[L]\big)\Big)^T \cdot w\bigg] \odot \Big(y[L - 1] \odot \big(1 - y[L - 1]\big)\Big)^T\Bigg] \\
&amp;amp; \quad \cdot w[L - 1]
\end{align*} \tag{18}\label{18}\]

&lt;p&gt;從 $\eqref{13}\eqref{14}\eqref{15}\eqref{16}\eqref{17}\eqref{18}$ 我們可以繼續往後推到任意隱藏層或輸入層的每個節點相對於誤差的微分&lt;/p&gt;

\[\begin{align*}
\pd{E}{a_i[l]} &amp;amp; = \pd{E}{y_i[l]} \cdot \pd{y_i[l]}{a_i[l]} &amp;amp;&amp;amp; \forall i = 1, \dots, \dout[l] \\
&amp;amp; = \pd{E}{y_i[l]} \cdot y_i[l] \cdot \big(1 - y_i[l]\big) \\
\pd{E}{w_{i j}[l]} &amp;amp; = \pd{E}{a_i[l]} \cdot \pd{a_i[l]}{w_{i j}[l]} &amp;amp;&amp;amp; \forall i = 1, \dots, \dout[l] \\
&amp;amp; = \pd{E}{a_i[l]} \cdot y_j[l - 1] &amp;amp;&amp;amp; \forall j = 1, \dots, \din[l] \\
&amp;amp; = \pd{E}{y_i[l]} \cdot y_i[l] \cdot \big(1 - y_i[l]\big) \cdot y_j[l - 1] \\
\pd{E}{x_j[l]} &amp;amp; = \sum_{i = 1}^{\dout[l]} \pd{E}{a_i[l]} \cdot \pd{a_i[l]}{x_j[l]} &amp;amp;&amp;amp; \forall j = 1, \dots, \din[l] \\
&amp;amp; = \sum_{i = 1}^{\dout[l]} \pd{E}{y_i[l]} \cdot y_i[l] \cdot \big(1 - y_i[l]\big) \cdot w_{i j}[l] \\
\pd{E}{a[l]} &amp;amp; = \pd{E}{y[l]} \cdot \pd{y[l]}{a[l]} \\
&amp;amp; = \pd{E}{y[l]} \odot \Big(y[l] \odot \big(1 - y[l]\big)\Big)^T \\
\pd{E}{w[l]} &amp;amp; = \begin{pmatrix}
\pd{E}{a_1[l]} \cdot x[l]^T \\
\pd{E}{a_2[l]} \cdot x[l]^T \\
\vdots \\
\pd{E}{a_{\dout[l]}[l]} \cdot x[l]^T
\end{pmatrix} \\
\pd{E}{x[l]} &amp;amp; = \pd{E}{a[l]} \cdot \pd{a[l]}{x[l]} \\
&amp;amp; = \pd{E}{a[l]} \cdot w[l]
\end{align*} \tag{19}\label{19}\]

&lt;p&gt;接著使用梯度進行參數更新，論文中使用的方法是對每一筆資料（共 $N$ 筆）進行一次 $\eqref{19}$ 的計算，並將所有誤差所得梯度加總作為梯度更新的方向&lt;/p&gt;

\[\pd{E}{w[l]} = \sum_{n = 1}^N \pd{E^n(x^n, \hat{y}^n)}{w[l]} \quad \forall l = 1, \dots, L \tag{20}\label{20}\]

&lt;p&gt;接著利用 $\eqref{20}$ 進行梯度下降（gradient descent）&lt;/p&gt;

\[\begin{align*}
\triangle w[l]\pa{0} &amp;amp; = O \\
\triangle w[l]\pa{t} &amp;amp; = -\varepsilon \cdot \pd{E}{w[l]\pa{t}} + \alpha \cdot \triangle w[l]\pa{t - 1} \\
w[l]\pa{t + 1} &amp;amp; = w[l]\pa{t} + \triangle w[l]\pa{t}
\end{align*}\tag{21}\label{21}\]

&lt;ul&gt;
  &lt;li&gt;我們使用 $w[l]\pa{t}$ 代表第 $t$ 個 epochs 時第 $l$ 層的參數 $w[l]$&lt;/li&gt;
  &lt;li&gt;我們使用 $\triangle w[l]\pa{t}$ 代表第 $t$ 個 epochs 時第 $l$ 層的參數 $w[l]$ 更新的方向
    &lt;ul&gt;
      &lt;li&gt;更新方向與 $t$ 時間點計算所得梯度方向相反&lt;/li&gt;
      &lt;li&gt;$\varepsilon$ 為 learning rate&lt;/li&gt;
      &lt;li&gt;更新方向與 $t - 1$ 時間點計算所得更新方向相同&lt;/li&gt;
      &lt;li&gt;$\alpha$ 為 exponential decay factor，數值介於 $(0, 1)$&lt;/li&gt;
      &lt;li&gt;參數減去 $t$ 時間點的誤差梯度，並加上 $t - 1$ 時間點的梯度所得&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$t + 1$ 時間點的參數是由 $t$ 時間點的參數往 $\triangle w[l]\pa{t}$ 更新
    &lt;ul&gt;
      &lt;li&gt;總共訓練 $T$ 個 epochs&lt;/li&gt;
      &lt;li&gt;$t$ 的範圍為 $t = 1, \dots T$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;收斂速度會比使用 $2$ 階微分的最佳化方法還要慢
    &lt;ul&gt;
      &lt;li&gt;計算成本比 $2$ 階微分還要低許多&lt;/li&gt;
      &lt;li&gt;計算概念簡單且容易實作&lt;/li&gt;
      &lt;li&gt;但不像 $2$ 階微分保證收斂&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;所有參數隨機初始化成較小的數值&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;實驗 1：對稱性偵測&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;任務為給予 $6$ 個輸入數值，每 $3$ 個數值為一組，模型需要偵測兩組輸入數值是否對稱
    &lt;ul&gt;
      &lt;li&gt;所有數值只能是 $0$ 或 $1$，因此共有 $2^6 = 64$ 種不同的輸入&lt;/li&gt;
      &lt;li&gt;總共只有 $2^3 = 8$ 種對稱輸入&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;如果沒有使用隱藏層，則單純的把輸入相加得到輸出是無法偵測輸入的對稱性&lt;/li&gt;
  &lt;li&gt;根據實驗結果，模型成功學會讓參數數值對稱，確保對稱輸入能夠被偵測
    &lt;ul&gt;
      &lt;li&gt;只使用一層隱藏層，只有兩個隱藏單元&lt;/li&gt;
      &lt;li&gt;訓練花了 $1425$ 個 epochs&lt;/li&gt;
      &lt;li&gt;$\varepsilon = 0.1$&lt;/li&gt;
      &lt;li&gt;$\alpha = 0.9$&lt;/li&gt;
      &lt;li&gt;參數使用平均分佈初始化，區間為 $[-0.3, 0.3]$&lt;/li&gt;
      &lt;li&gt;連接第一組輸入和隱藏單元的參數與連接第二組輸入和隱藏單元的參數的數值正負號相反
        &lt;ul&gt;
          &lt;li&gt;如果輸入對稱，則隱藏單元收到的淨輸入為 $0$&lt;/li&gt;
          &lt;li&gt;數值呈現 $1 : 2 : 4$ 的比例，確保數值能夠正確對應&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;隱藏單元的 bias 為負數
        &lt;ul&gt;
          &lt;li&gt;如果輸入對稱，淨輸入加上 bias 得到負數，sigmoid 啟發值就會接近 $0$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;連接輸入與兩個隱藏單元的參數正負號相反
        &lt;ul&gt;
          &lt;li&gt;當輸入不對稱時，會有一個隱藏單元輸出接近 $1$，另一個接近 $0$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;輸出單元參數
        &lt;ul&gt;
          &lt;li&gt;連接隱藏單元的參數為負數&lt;/li&gt;
          &lt;li&gt;bias 為正數&lt;/li&gt;
          &lt;li&gt;在兩個隱藏單元輸出接近 $0$ 時輸出接近 $1$&lt;/li&gt;
          &lt;li&gt;在其中一個隱藏單元輸出接近 $1$ 時輸出接近 $0$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;實驗 2：家族關係圖&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;每個家族關係定義為一個三元組（3-tuple），概念為：人-關係-人
    &lt;ul&gt;
      &lt;li&gt;總共有 $24$ 個人，分屬兩個不同的家族，每個家族有 $12$ 個人&lt;/li&gt;
      &lt;li&gt;關係共有 $12$ 個，分別為 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;father&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mother&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;husband&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wife&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;son&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;daughter&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uncle&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;aunt&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;brother&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sister&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nephew&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;niece&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;給予 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;人-人&lt;/code&gt; 時需要預測關係
    &lt;ul&gt;
      &lt;li&gt;輸入為一個 $12$ 維的向量，其中兩個數值為 $1$，剩餘為 $0$&lt;/li&gt;
      &lt;li&gt;輸出為 $12$ 維的向量，其中一個數值為 $1$（代表關係），剩餘為 $0$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;給予 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;人-關係&lt;/code&gt; 時需要預測人
    &lt;ul&gt;
      &lt;li&gt;其中一個輸入為 $12$ 維的向量，只有一個數值為 $1$，剩餘為 $0$&lt;/li&gt;
      &lt;li&gt;另外一個輸入為 $12$ 維的向量，只有一個數值為 $1$，剩餘為 $0$&lt;/li&gt;
      &lt;li&gt;輸出為 $24$ 維的向量，可以有多個數值為 $1$（可能有多個人符合關係），剩餘為 $0$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;架構為 $5$ 層，都是全連接
    &lt;ul&gt;
      &lt;li&gt;第一層維度為 $12 \times 2 + 12 = 36$&lt;/li&gt;
      &lt;li&gt;第二層維度為 $6 \times 2 = 12$，每個家族各自全連接到自己專屬的 $6$ 個隱藏單元&lt;/li&gt;
      &lt;li&gt;第三層維度為 $12$&lt;/li&gt;
      &lt;li&gt;第四層維度為 $6$&lt;/li&gt;
      &lt;li&gt;第五層維度為 $12 \times 2 + 12 = 36$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;總資料數有 $104$ 筆，訓練資料共有 $100$ 筆
    &lt;ul&gt;
      &lt;li&gt;總共訓練 $1500$ 個 epochs&lt;/li&gt;
      &lt;li&gt;前 $20$ 個 epochs 使用不同的參數
        &lt;ul&gt;
          &lt;li&gt;$\varepsilon = 0.005$&lt;/li&gt;
          &lt;li&gt;$\alpha = 0.5$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;剩餘的 epochs 使用相同的參數
        &lt;ul&gt;
          &lt;li&gt;$\varepsilon = 0.01$&lt;/li&gt;
          &lt;li&gt;$\alpha = 0.9$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;每次更新參數後，手動進行 $0.2\%$ 的 weight decay（將參數乘上 $0.998$）&lt;/li&gt;
      &lt;li&gt;為了避免模型一定要輸出 $0$ 或 $1$，當模型輸出 $0.8$ 就當成 $1$，輸出 $0.2$ 就當成 $0$
        &lt;ul&gt;
          &lt;li&gt;如果預測正確就不計算誤差（讓誤差為 $0$）&lt;/li&gt;
          &lt;li&gt;如果預測失敗就按照原本方法計算誤差&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;實驗結果顯示在兩組家族上學到一樣的結構（isomorphism）
    &lt;ul&gt;
      &lt;li&gt;這是廢話，因為本來就是用數值代表&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>[&quot;David E. Rumelhart&quot;, &quot;Geoffrey E. Hinton&quot;, &quot;Ronald J. Williams&quot;]</name></author><category term="Optimization" /><category term="back-propagation" /><category term="gradient descent" /><category term="neural network" /><summary type="html">目標 提出 back-propagation 作者 David E. Rumelhart, Geoffrey E. Hinton, Ronald J. Williams 期刊/會議名稱 Nature 發表時間 1986 論文連結 https://www.nature.com/articles/323533a0</summary></entry></feed>